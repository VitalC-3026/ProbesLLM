2024-04-24 00:38:44.963337: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-04-24 00:38:46.028655: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
================layer -5================
Epoch 0/10: Training Loss: 0.001907768574627963
Epoch 1/10: Training Loss: 0.001819283395380407
Epoch 2/10: Training Loss: 0.0016088029304584424
Epoch 3/10: Training Loss: 0.002117398943934407
Epoch 4/10: Training Loss: 0.0026502338322726164
Epoch 5/10: Training Loss: 0.0019005253598406598
Epoch 6/10: Training Loss: 0.0019801388253698816
Epoch 7/10: Training Loss: 0.001822570195564857
Epoch 8/10: Training Loss: 0.001340292342059262
Epoch 9/10: Training Loss: 0.001702735995079254
Epoch 0/10: Training Loss: 0.001961954198517166
Epoch 1/10: Training Loss: 0.0025316118360399365
Epoch 2/10: Training Loss: 0.0015882445382071541
Epoch 3/10: Training Loss: 0.0015459392037425008
Epoch 4/10: Training Loss: 0.0011650548948274626
Epoch 5/10: Training Loss: 0.002398884171372527
Epoch 6/10: Training Loss: 0.002413889328082958
Epoch 7/10: Training Loss: 0.001976970192435738
Epoch 8/10: Training Loss: 0.001515179768308893
Epoch 9/10: Training Loss: 0.0009834783060567362
Epoch 0/10: Training Loss: 0.001873400661495182
Epoch 1/10: Training Loss: 0.00225127180973133
Epoch 2/10: Training Loss: 0.0014906719431176886
Epoch 3/10: Training Loss: 0.0018611406529700006
Epoch 4/10: Training Loss: 0.001043469443187847
Epoch 5/10: Training Loss: 0.0012237548411309302
Epoch 6/10: Training Loss: 0.0015054729643401567
Epoch 7/10: Training Loss: 0.0010546799097861444
Epoch 8/10: Training Loss: 0.0015862850876121254
Epoch 9/10: Training Loss: 0.0011100084423185227
Epoch 0/10: Training Loss: 0.0016970654572445922
Epoch 1/10: Training Loss: 0.0007580084943332555
Epoch 2/10: Training Loss: 0.0010300650735574266
Epoch 3/10: Training Loss: 0.0018249126665431299
Epoch 4/10: Training Loss: 0.0010788558633780918
Epoch 5/10: Training Loss: 0.0012041878846525416
Epoch 6/10: Training Loss: 0.0006532634328479416
Epoch 7/10: Training Loss: 0.00044919810960629235
Epoch 8/10: Training Loss: 0.00045309156362264435
Epoch 9/10: Training Loss: 0.0009984772629533078
Epoch 0/10: Training Loss: 0.0019203806581672715
Epoch 1/10: Training Loss: 0.0008984920254514261
Epoch 2/10: Training Loss: 0.0007594601234044034
Epoch 3/10: Training Loss: 0.0020782923771559825
Epoch 4/10: Training Loss: 0.0013054473085637472
Epoch 5/10: Training Loss: 0.0012658552889443615
Epoch 6/10: Training Loss: 0.0008932194826793085
Epoch 7/10: Training Loss: 0.0006056914300274995
Epoch 8/10: Training Loss: 0.0004627606345831982
Epoch 9/10: Training Loss: 0.001233263524032078
Epoch 0/10: Training Loss: 0.002136085479537402
Epoch 1/10: Training Loss: 0.0007519057831881236
Epoch 2/10: Training Loss: 0.0012962488126169684
Epoch 3/10: Training Loss: 0.0013634822119964413
Epoch 4/10: Training Loss: 0.0011507290089788613
Epoch 5/10: Training Loss: 0.000769005322748898
Epoch 6/10: Training Loss: 0.0005334038401673907
Epoch 7/10: Training Loss: 0.0005027790742417786
Epoch 8/10: Training Loss: 0.00013484111044304503
Epoch 9/10: Training Loss: 0.0006674323345254535
Epoch 0/10: Training Loss: 0.0014778407290577888
Epoch 1/10: Training Loss: 0.0012140924111008645
Epoch 2/10: Training Loss: 0.0009029114618897438
Epoch 3/10: Training Loss: 0.001427960954606533
Epoch 4/10: Training Loss: 0.0009437767788767814
Epoch 5/10: Training Loss: 0.0019037026911973953
Epoch 6/10: Training Loss: 0.0006048585288226605
Epoch 7/10: Training Loss: 0.0009686521254479886
Epoch 8/10: Training Loss: 0.0007823968306183815
Epoch 9/10: Training Loss: 0.0005276211071759462
Epoch 0/10: Training Loss: 0.0010356229729950428
Epoch 1/10: Training Loss: 0.0008850238285958767
Epoch 2/10: Training Loss: 0.001227643620222807
Epoch 3/10: Training Loss: 0.0015157577581703663
Epoch 4/10: Training Loss: 0.001022125780582428
Epoch 5/10: Training Loss: 0.0013185671530663967
Epoch 6/10: Training Loss: 0.0012487699277698994
Epoch 7/10: Training Loss: 0.0007038559298962355
Epoch 8/10: Training Loss: 0.0006898588966578245
Epoch 9/10: Training Loss: 0.0007561407051980496
Epoch 0/10: Training Loss: 0.0010355274192988873
Epoch 1/10: Training Loss: 0.0013253967277705669
Epoch 2/10: Training Loss: 0.0009078646078705787
Epoch 3/10: Training Loss: 0.0015666104853153228
Epoch 4/10: Training Loss: 0.001770423911511898
Epoch 5/10: Training Loss: 0.0020843464881181717
Epoch 6/10: Training Loss: 0.0015564627014100552
Epoch 7/10: Training Loss: 0.0006810524500906467
Epoch 8/10: Training Loss: 0.0013329990208148957
Epoch 9/10: Training Loss: 0.0003761058673262596
Epoch 0/10: Training Loss: 0.0018528420834025001
Epoch 1/10: Training Loss: 0.0008455382980358828
Epoch 2/10: Training Loss: 0.0007773755936865594
Epoch 3/10: Training Loss: 0.0014911108905342734
Epoch 4/10: Training Loss: 0.000899341550602275
Epoch 5/10: Training Loss: 0.0014336159464659964
Epoch 6/10: Training Loss: 0.0015987161618129463
Epoch 7/10: Training Loss: 0.001413866592820283
Epoch 8/10: Training Loss: 0.001094530247578955
Epoch 9/10: Training Loss: 0.0012429794118662548
Epoch 0/10: Training Loss: 0.0017587248307124824
Epoch 1/10: Training Loss: 0.0012792091650567997
Epoch 2/10: Training Loss: 0.0005689146602229707
Epoch 3/10: Training Loss: 0.0011316253121491451
Epoch 4/10: Training Loss: 0.002428711409781389
Epoch 5/10: Training Loss: 0.001629600289520944
Epoch 6/10: Training Loss: 0.0014266141091182733
Epoch 7/10: Training Loss: 0.0026195751633613733
Epoch 8/10: Training Loss: 0.0015567619899275957
Epoch 9/10: Training Loss: 0.0014545296787456343
Epoch 0/10: Training Loss: 0.0018135349082339341
Epoch 1/10: Training Loss: 0.0015218895711716573
Epoch 2/10: Training Loss: 0.001242960524407162
Epoch 3/10: Training Loss: 0.0012699202367454578
Epoch 4/10: Training Loss: 0.0019622682386143194
Epoch 5/10: Training Loss: 0.002148411076539641
Epoch 6/10: Training Loss: 0.0015955711625943519
Epoch 7/10: Training Loss: 0.001754810286175673
Epoch 8/10: Training Loss: 0.0017216668766774949
Epoch 9/10: Training Loss: 0.0011328635322060555
Epoch 0/10: Training Loss: 0.001994727857854982
Epoch 1/10: Training Loss: 0.0018899677605028974
Epoch 2/10: Training Loss: 0.0015963659381234882
Epoch 3/10: Training Loss: 0.0016988527695864241
Epoch 4/10: Training Loss: 0.0017744438932431454
Epoch 5/10: Training Loss: 0.0019881646364729925
Epoch 6/10: Training Loss: 0.0011971672836518445
Epoch 7/10: Training Loss: 0.0014626700554462458
Epoch 8/10: Training Loss: 0.001462503182177512
Epoch 9/10: Training Loss: 0.0011958534157039313
Epoch 0/10: Training Loss: 0.0016651599612457074
Epoch 1/10: Training Loss: 0.0015115500088559082
Epoch 2/10: Training Loss: 0.0015314994071493086
Epoch 3/10: Training Loss: 0.0014355960271216386
Epoch 4/10: Training Loss: 0.0012477314630091585
Epoch 5/10: Training Loss: 0.0016640941828291937
Epoch 6/10: Training Loss: 0.0013677948555409513
Epoch 7/10: Training Loss: 0.002027198178878683
Epoch 8/10: Training Loss: 0.0019134069910112595
Epoch 9/10: Training Loss: 0.0016449581313606918
Epoch 0/10: Training Loss: 0.0016616531555226307
Epoch 1/10: Training Loss: 0.0018926031936872874
Epoch 2/10: Training Loss: 0.0014797875423305082
Epoch 3/10: Training Loss: 0.0017973352346988703
Epoch 4/10: Training Loss: 0.0016560866343264548
Epoch 5/10: Training Loss: 0.0011346865173996678
Epoch 6/10: Training Loss: 0.0009547318054350796
Epoch 7/10: Training Loss: 0.002216461675846024
Epoch 8/10: Training Loss: 0.0017042085034957785
Epoch 9/10: Training Loss: 0.0015936672490164145
Epoch 0/10: Training Loss: 0.0030871584134943347
Epoch 1/10: Training Loss: 0.001135177822674022
Epoch 2/10: Training Loss: 0.0014750158085542566
Epoch 3/10: Training Loss: 0.0010663772330564611
Epoch 4/10: Training Loss: 0.0011534961707451764
Epoch 5/10: Training Loss: 0.0011266007142908432
Epoch 6/10: Training Loss: 0.0005180520169875201
Epoch 7/10: Training Loss: 0.0010108862729633556
Epoch 8/10: Training Loss: 0.0007857183323186986
Epoch 9/10: Training Loss: 0.0008067836656289942
Epoch 0/10: Training Loss: 0.002218482774846694
Epoch 1/10: Training Loss: 0.0014240502434618332
Epoch 2/10: Training Loss: 0.0019158089862150304
Epoch 3/10: Training Loss: 0.0012759555788601146
Epoch 4/10: Training Loss: 0.0017396122217178344
Epoch 5/10: Training Loss: 0.00081090988481746
Epoch 6/10: Training Loss: 0.0009060686125474818
Epoch 7/10: Training Loss: 0.000539470902260612
Epoch 8/10: Training Loss: 0.0009706052787163678
Epoch 9/10: Training Loss: 0.0009210815324502833
Epoch 0/10: Training Loss: 0.002168877685771269
Epoch 1/10: Training Loss: 0.0015757629100014183
Epoch 2/10: Training Loss: 0.000936494767665863
Epoch 3/10: Training Loss: 0.001130170068320106
Epoch 4/10: Training Loss: 0.0007587332059355343
Epoch 5/10: Training Loss: 0.0013765797019004821
Epoch 6/10: Training Loss: 0.0010516840745421017
Epoch 7/10: Training Loss: 0.0012692328761605655
Epoch 8/10: Training Loss: 0.001176780549918904
Epoch 9/10: Training Loss: 0.0012587914572042577
dataset: capitals layer_num_from_end:-5 Avg_acc:tensor(0.7451) Avg_AUC:0.9212233029317146 Avg_threshold:0.5541968444983164
dataset: inventions layer_num_from_end:-5 Avg_acc:tensor(0.6522) Avg_AUC:0.7525841144961499 Avg_threshold:0.04559610776292781
dataset: elements layer_num_from_end:-5 Avg_acc:tensor(0.5810) Avg_AUC:0.6474143446255829 Avg_threshold:0.5080610712369283
dataset: animals layer_num_from_end:-5 Avg_acc:tensor(0.5271) Avg_AUC:0.6766568510120097 Avg_threshold:0.4932195246219635
dataset: companies layer_num_from_end:-5 Avg_acc:tensor(0.5775) Avg_AUC:0.8690425925925926 Avg_threshold:0.7688366174697876
dataset: facts layer_num_from_end:-5 Avg_acc:tensor(0.6618) Avg_AUC:0.7367679097782904 Avg_threshold:0.5977926651636759


