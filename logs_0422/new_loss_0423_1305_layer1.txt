2024-04-23 17:05:58.165828: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-04-23 17:06:00.290603: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Epoch 0/100: Training Loss: 0.005039418077135419
Epoch 1/100: Training Loss: 0.005058947143021164
Epoch 2/100: Training Loss: 0.004205597447348641
Epoch 3/100: Training Loss: 0.003750705635631001
Epoch 4/100: Training Loss: 0.004078742507454399
Epoch 5/100: Training Loss: 0.0036970276932616333
Epoch 6/100: Training Loss: 0.0045354224585153
Epoch 7/100: Training Loss: 0.004219918817906947
Epoch 8/100: Training Loss: 0.003968817787570553
Epoch 9/100: Training Loss: 0.003776872908318793
Epoch 10/100: Training Loss: 0.004347329789941961
Epoch 11/100: Training Loss: 0.003619845096881573
Epoch 12/100: Training Loss: 0.004042663774290285
Epoch 13/100: Training Loss: 0.004243876550581072
Epoch 14/100: Training Loss: 0.00386762744063264
Epoch 15/100: Training Loss: 0.0026825687268397192
Epoch 16/100: Training Loss: 0.0034173939611528304
Epoch 17/100: Training Loss: 0.0028508609825080924
Epoch 18/100: Training Loss: 0.0045859196802952905
Epoch 19/100: Training Loss: 0.0023692669151546236
Epoch 20/100: Training Loss: 0.00394220452208619
Epoch 21/100: Training Loss: 0.0028605838338811914
Epoch 22/100: Training Loss: 0.0036223047263138778
Epoch 23/100: Training Loss: 0.0024242590774189343
Epoch 24/100: Training Loss: 0.0032646480973783907
Epoch 25/100: Training Loss: 0.004051846640926974
Epoch 26/100: Training Loss: 0.0025671064020036816
Epoch 27/100: Training Loss: 0.0037342514191474113
Epoch 28/100: Training Loss: 0.0023109383516378337
Epoch 29/100: Training Loss: 0.0032655284121319962
Epoch 30/100: Training Loss: 0.0020584459905024174
Epoch 31/100: Training Loss: 0.0012806464325297964
Epoch 32/100: Training Loss: 0.001818792386488481
Epoch 33/100: Training Loss: 0.001783081493177614
Epoch 34/100: Training Loss: 0.0034647296358655383
Epoch 35/100: Training Loss: 0.0016978847813772988
Epoch 36/100: Training Loss: 0.0018220829380142106
Epoch 37/100: Training Loss: 0.0020502624395010356
Epoch 38/100: Training Loss: 0.0009938274230156744
Epoch 39/100: Training Loss: 0.0010075082520504932
Epoch 40/100: Training Loss: 0.003115008344183435
Epoch 41/100: Training Loss: 0.002105677461290693
Epoch 42/100: Training Loss: 0.0021410676149221566
Epoch 43/100: Training Loss: 0.0018629125365010508
Epoch 44/100: Training Loss: 0.0015734649829931193
Epoch 45/100: Training Loss: 0.0016193635813839785
Epoch 46/100: Training Loss: 0.0017144636257545097
Epoch 47/100: Training Loss: 0.0034847399154743115
Epoch 48/100: Training Loss: 0.0011236982328908427
Epoch 49/100: Training Loss: 0.0019163881148491705
Epoch 50/100: Training Loss: 0.001900955930456415
Epoch 51/100: Training Loss: 0.0024270286093225012
Epoch 52/100: Training Loss: 0.003167785964645706
Epoch 53/100: Training Loss: 0.0027291831019875053
Epoch 54/100: Training Loss: 0.0017863991377236959
Epoch 55/100: Training Loss: 0.002846724086708122
Epoch 56/100: Training Loss: 0.0017886757850646973
Epoch 57/100: Training Loss: 0.0016594206119750763
Epoch 58/100: Training Loss: 0.006099423745295385
Epoch 59/100: Training Loss: 0.001598588757581644
Epoch 60/100: Training Loss: 0.001961155579640315
Epoch 61/100: Training Loss: 0.0012031639372552191
Epoch 62/100: Training Loss: 0.00136078373118714
Epoch 63/100: Training Loss: 0.0014452103759858993
Epoch 64/100: Training Loss: 0.0013398159217167568
Epoch 65/100: Training Loss: 0.0011843993113591121
Epoch 66/100: Training Loss: 0.0011162855825224123
Epoch 67/100: Training Loss: 0.0010753908565828017
Epoch 68/100: Training Loss: 0.0012803289231720504
Epoch 69/100: Training Loss: 0.0005640244775718742
Epoch 70/100: Training Loss: 0.000792939762969117
Epoch 71/100: Training Loss: 0.0004812092943625017
Epoch 72/100: Training Loss: 0.0008106236899649347
Epoch 73/100: Training Loss: 0.0005610493513254019
Epoch 74/100: Training Loss: 0.000607374352175039
Epoch 75/100: Training Loss: 0.0005625889122069298
Epoch 76/100: Training Loss: 0.0005716587175856104
Epoch 77/100: Training Loss: 0.001071060662503009
Epoch 78/100: Training Loss: 0.0013289311965862353
Epoch 79/100: Training Loss: 0.0007819713620872764
Epoch 80/100: Training Loss: 0.002656965614198805
Epoch 81/100: Training Loss: 0.0007877889629844185
Epoch 82/100: Training Loss: 0.0003589147089661418
Epoch 83/100: Training Loss: 0.0005520789564906301
Epoch 84/100: Training Loss: 0.0011241180705023812
Epoch 85/100: Training Loss: 0.00047367889355946253
Epoch 86/100: Training Loss: 0.0019359532352927681
Epoch 87/100: Training Loss: 0.003435200536167705
Epoch 88/100: Training Loss: 0.0007088476127677865
Epoch 89/100: Training Loss: 0.0020950632495479985
Epoch 90/100: Training Loss: 0.002133374864404852
Epoch 91/100: Training Loss: 0.0008963888341730291
Epoch 92/100: Training Loss: 0.0008680233796993336
Epoch 93/100: Training Loss: 0.0006022711212818439
Epoch 94/100: Training Loss: 0.0010804751744637122
Epoch 95/100: Training Loss: 0.0013958853441518504
Epoch 96/100: Training Loss: 0.001195374277088192
Epoch 97/100: Training Loss: 0.002587859030370112
Epoch 98/100: Training Loss: 0.002967925963701902
Epoch 99/100: Training Loss: 0.0025049047870235844
Epoch 0/100: Training Loss: 0.005373637993018944
Epoch 1/100: Training Loss: 0.005326533651018476
Epoch 2/100: Training Loss: 0.00480909114117389
Epoch 3/100: Training Loss: 0.004084324086462701
Epoch 4/100: Training Loss: 0.004724595930192854
Epoch 5/100: Training Loss: 0.005022303207770928
Epoch 6/100: Training Loss: 0.0033752497259553495
Epoch 7/100: Training Loss: 0.0030156328127934383
Epoch 8/100: Training Loss: 0.0032288386271550106
Epoch 9/100: Training Loss: 0.003405351530421864
Epoch 10/100: Training Loss: 0.0037662124300336503
Epoch 11/100: Training Loss: 0.0028307515007632597
Epoch 12/100: Training Loss: 0.005216986149341077
Epoch 13/100: Training Loss: 0.004613073138923912
Epoch 14/100: Training Loss: 0.004207392255743066
Epoch 15/100: Training Loss: 0.003912822349921807
Epoch 16/100: Training Loss: 0.0045725990842272355
Epoch 17/100: Training Loss: 0.002531687071273377
Epoch 18/100: Training Loss: 0.002351179406359479
Epoch 19/100: Training Loss: 0.0028170830719954484
Epoch 20/100: Training Loss: 0.00245868305226306
Epoch 21/100: Training Loss: 0.002633860686442235
Epoch 22/100: Training Loss: 0.0030928591748217603
Epoch 23/100: Training Loss: 0.0030892153719922046
Epoch 24/100: Training Loss: 0.004412026672096519
Epoch 25/100: Training Loss: 0.002251799498404656
Epoch 26/100: Training Loss: 0.004262391503874238
Epoch 27/100: Training Loss: 0.003919919053991357
Epoch 28/100: Training Loss: 0.0033239888978171183
Epoch 29/100: Training Loss: 0.003693289273268693
Epoch 30/100: Training Loss: 0.003166547813615599
Epoch 31/100: Training Loss: 0.003931446925743477
Epoch 32/100: Training Loss: 0.0022447517701795886
Epoch 33/100: Training Loss: 0.003721628572557356
Epoch 34/100: Training Loss: 0.0018139735802070243
Epoch 35/100: Training Loss: 0.002743880648713012
Epoch 36/100: Training Loss: 0.0028999512429003947
Epoch 37/100: Training Loss: 0.0024149570014926937
Epoch 38/100: Training Loss: 0.0025176945683005806
Epoch 39/100: Training Loss: 0.0020889081321396196
Epoch 40/100: Training Loss: 0.002000159316963249
Epoch 41/100: Training Loss: 0.0016823492266915061
Epoch 42/100: Training Loss: 0.002857392067675824
Epoch 43/100: Training Loss: 0.0020915553286359025
Epoch 44/100: Training Loss: 0.0021492699226299365
Epoch 45/100: Training Loss: 0.0029035614920662834
Epoch 46/100: Training Loss: 0.0018685576799032572
Epoch 47/100: Training Loss: 0.002332796166826795
Epoch 48/100: Training Loss: 0.0017609748390171077
Epoch 49/100: Training Loss: 0.0019260242685571417
Epoch 50/100: Training Loss: 0.0014370884095038568
Epoch 51/100: Training Loss: 0.001850978894667192
Epoch 52/100: Training Loss: 0.0011752441629663214
Epoch 53/100: Training Loss: 0.0016968846321105957
Epoch 54/100: Training Loss: 0.002170268889073725
Epoch 55/100: Training Loss: 0.0012568714735391256
Epoch 56/100: Training Loss: 0.002096947464909587
Epoch 57/100: Training Loss: 0.0013903173116537242
Epoch 58/100: Training Loss: 0.001472917886880728
Epoch 59/100: Training Loss: 0.0016591803177253349
Epoch 60/100: Training Loss: 0.002249701664997981
Epoch 61/100: Training Loss: 0.0014808901123233608
Epoch 62/100: Training Loss: 0.001360366602877637
Epoch 63/100: Training Loss: 0.0016108430348909819
Epoch 64/100: Training Loss: 0.001413199659827706
Epoch 65/100: Training Loss: 0.0011454155811896692
Epoch 66/100: Training Loss: 0.0017871331501673985
Epoch 67/100: Training Loss: 0.003000980073755438
Epoch 68/100: Training Loss: 0.0013109733174730847
Epoch 69/100: Training Loss: 0.0017389928127502228
Epoch 70/100: Training Loss: 0.0010704671169494415
Epoch 71/100: Training Loss: 0.0010978844199147257
Epoch 72/100: Training Loss: 0.00156265976545694
Epoch 73/100: Training Loss: 0.0016263128369004576
Epoch 74/100: Training Loss: 0.00164841375984512
Epoch 75/100: Training Loss: 0.00120338964295554
Epoch 76/100: Training Loss: 0.0006618125663770662
Epoch 77/100: Training Loss: 0.0012178212612658948
Epoch 78/100: Training Loss: 0.0011335325824630843
Epoch 79/100: Training Loss: 0.0013778040875921716
Epoch 80/100: Training Loss: 0.0013537734121709436
Epoch 81/100: Training Loss: 0.0010384238891668254
Epoch 82/100: Training Loss: 0.0016560772290596594
Epoch 83/100: Training Loss: 0.0016748602990503912
Epoch 84/100: Training Loss: 0.001388747375328224
Epoch 85/100: Training Loss: 0.0009147658214702473
Epoch 86/100: Training Loss: 0.002028081800554182
Epoch 87/100: Training Loss: 0.001225779106566956
Epoch 88/100: Training Loss: 0.0021009220109952913
Epoch 89/100: Training Loss: 0.0016026827213647481
Epoch 90/100: Training Loss: 0.001116339976970966
Epoch 91/100: Training Loss: 0.0015928493096278263
Epoch 92/100: Training Loss: 0.0012992566282098944
Epoch 93/100: Training Loss: 0.0006588202464830625
Epoch 94/100: Training Loss: 0.0006351898183355798
Epoch 95/100: Training Loss: 0.0012859535384011436
Epoch 96/100: Training Loss: 0.0008815719739540473
Epoch 97/100: Training Loss: 0.0005738438738809599
Epoch 98/100: Training Loss: 0.0006606141691441302
Epoch 99/100: Training Loss: 0.0010687631743771214
Epoch 0/100: Training Loss: 0.005302385433570488
Epoch 1/100: Training Loss: 0.005676109057206374
Epoch 2/100: Training Loss: 0.004347515272927451
Epoch 3/100: Training Loss: 0.005112753047809734
Epoch 4/100: Training Loss: 0.0050027712241752995
Epoch 5/100: Training Loss: 0.00464928316903281
Epoch 6/100: Training Loss: 0.0033289546316320247
Epoch 7/100: Training Loss: 0.004577206148134245
Epoch 8/100: Training Loss: 0.004476710632964447
Epoch 9/100: Training Loss: 0.00434768074875945
Epoch 10/100: Training Loss: 0.0024041552226860205
Epoch 11/100: Training Loss: 0.003967960397680322
Epoch 12/100: Training Loss: 0.004010883244601163
Epoch 13/100: Training Loss: 0.0038138657183080286
Epoch 14/100: Training Loss: 0.002923386080281718
Epoch 15/100: Training Loss: 0.0028796867057160063
Epoch 16/100: Training Loss: 0.0037486028004359534
Epoch 17/100: Training Loss: 0.004155531629815802
Epoch 18/100: Training Loss: 0.0028998268234146224
Epoch 19/100: Training Loss: 0.0024065929692941948
Epoch 20/100: Training Loss: 0.002203430120761578
Epoch 21/100: Training Loss: 0.003519705542317637
Epoch 22/100: Training Loss: 0.003014749788737797
Epoch 23/100: Training Loss: 0.005259066194921107
Epoch 24/100: Training Loss: 0.0037822756733927694
Epoch 25/100: Training Loss: 0.004488046352679913
Epoch 26/100: Training Loss: 0.0042461494465807935
Epoch 27/100: Training Loss: 0.00313754306806551
Epoch 28/100: Training Loss: 0.002669394433081567
Epoch 29/100: Training Loss: 0.002419781851601767
Epoch 30/100: Training Loss: 0.003994141842101838
Epoch 31/100: Training Loss: 0.004548571743331589
Epoch 32/100: Training Loss: 0.0051177202404795825
Epoch 33/100: Training Loss: 0.003955006182610572
Epoch 34/100: Training Loss: 0.003577813818738177
Epoch 35/100: Training Loss: 0.004132433787926094
Epoch 36/100: Training Loss: 0.0024907326364850663
Epoch 37/100: Training Loss: 0.0025104146737318775
Epoch 38/100: Training Loss: 0.0032625087908097913
Epoch 39/100: Training Loss: 0.0023782065698316882
Epoch 40/100: Training Loss: 0.002399419987951959
Epoch 41/100: Training Loss: 0.001975256454694521
Epoch 42/100: Training Loss: 0.0029002140451978137
Epoch 43/100: Training Loss: 0.0016921133428186804
Epoch 44/100: Training Loss: 0.0019864698390027026
Epoch 45/100: Training Loss: 0.0017814361132108248
Epoch 46/100: Training Loss: 0.003001461287478467
Epoch 47/100: Training Loss: 0.0027405911392265267
Epoch 48/100: Training Loss: 0.0016623539524478512
Epoch 49/100: Training Loss: 0.001758870336559269
Epoch 50/100: Training Loss: 0.0019476221991585685
Epoch 51/100: Training Loss: 0.0014550410784207857
Epoch 52/100: Training Loss: 0.001626265215706992
Epoch 53/100: Training Loss: 0.0013672171772776784
Epoch 54/100: Training Loss: 0.0015480197392977201
Epoch 55/100: Training Loss: 0.0022439564858283195
Epoch 56/100: Training Loss: 0.0018433624214225716
Epoch 57/100: Training Loss: 0.0017143204495623395
Epoch 58/100: Training Loss: 0.0017626410597687834
Epoch 59/100: Training Loss: 0.0018870763845377034
Epoch 60/100: Training Loss: 0.0013272941529334009
Epoch 61/100: Training Loss: 0.0015479925420734432
Epoch 62/100: Training Loss: 0.0015947235214126693
Epoch 63/100: Training Loss: 0.0016689234888636983
Epoch 64/100: Training Loss: 0.0017444741475832212
Epoch 65/100: Training Loss: 0.0014303685068250535
Epoch 66/100: Training Loss: 0.0006570252296807883
Epoch 67/100: Training Loss: 0.0020560680569468677
Epoch 68/100: Training Loss: 0.0008047255915361684
Epoch 69/100: Training Loss: 0.0004971274337568484
Epoch 70/100: Training Loss: 0.002408319628322041
Epoch 71/100: Training Loss: 0.0021024931977678845
Epoch 72/100: Training Loss: 0.0009167724764430439
Epoch 73/100: Training Loss: 0.0012488773652723619
Epoch 74/100: Training Loss: 0.0010499746857823191
Epoch 75/100: Training Loss: 0.000973922597778427
Epoch 76/100: Training Loss: 0.001116205970724146
Epoch 77/100: Training Loss: 0.0006174553524364125
Epoch 78/100: Training Loss: 0.0011362057257365513
Epoch 79/100: Training Loss: 0.0030074363405054264
Epoch 80/100: Training Loss: 0.0022272805233935376
Epoch 81/100: Training Loss: 0.0016253086236807017
Epoch 82/100: Training Loss: 0.0015108291502599115
Epoch 83/100: Training Loss: 0.0025896688441296558
Epoch 84/100: Training Loss: 0.0030105993464276505
Epoch 85/100: Training Loss: 0.001990282660597688
Epoch 86/100: Training Loss: 0.0013986903053897243
Epoch 87/100: Training Loss: 0.0011690834185460232
Epoch 88/100: Training Loss: 0.0022469750651112805
Epoch 89/100: Training Loss: 0.0009845375389485925
Epoch 90/100: Training Loss: 0.0010757998569861991
Epoch 91/100: Training Loss: 0.0014484697705382235
Epoch 92/100: Training Loss: 0.0010192492208280764
Epoch 93/100: Training Loss: 0.000671121676068206
Epoch 94/100: Training Loss: 0.0015816347790764762
Epoch 95/100: Training Loss: 0.0010234723974774768
Epoch 96/100: Training Loss: 0.0011886458296875855
Epoch 97/100: Training Loss: 0.0007136767352377618
Epoch 98/100: Training Loss: 0.002247760137477955
Epoch 99/100: Training Loss: 0.0012026750124417818
Epoch 0/100: Training Loss: 0.004093258293128453
Epoch 1/100: Training Loss: 0.0029222032775176815
Epoch 2/100: Training Loss: 0.0027429139687239756
Epoch 3/100: Training Loss: 0.0017719890442362593
Epoch 4/100: Training Loss: 0.0016713157021926224
Epoch 5/100: Training Loss: 0.0006425804887081216
Epoch 6/100: Training Loss: 0.00048481736994959826
Epoch 7/100: Training Loss: 0.0008808943940086599
Epoch 8/100: Training Loss: 0.0017196163078027269
Epoch 9/100: Training Loss: 0.00151709819132565
Epoch 10/100: Training Loss: 0.0013847228573875194
Epoch 11/100: Training Loss: 0.0016647805831183685
Epoch 12/100: Training Loss: 0.0021218741598304797
Epoch 13/100: Training Loss: 0.00239516017627131
Epoch 14/100: Training Loss: 0.0023438802526041043
Epoch 15/100: Training Loss: 0.0007013382355859674
Epoch 16/100: Training Loss: 0.0006469506427554266
Epoch 17/100: Training Loss: 0.0005663880632698901
Epoch 18/100: Training Loss: 0.0005643191330271996
Epoch 19/100: Training Loss: 0.00046146670184983796
Epoch 20/100: Training Loss: 0.0004204687165336375
Epoch 21/100: Training Loss: 0.0004401744509035824
Epoch 22/100: Training Loss: 0.0005991942494924814
Epoch 23/100: Training Loss: 0.00041945882004462866
Epoch 24/100: Training Loss: 0.0005255735999236077
Epoch 25/100: Training Loss: 0.0006797052234228403
Epoch 26/100: Training Loss: 0.0004737706272148647
Epoch 27/100: Training Loss: 0.0007763562575439734
Epoch 28/100: Training Loss: 0.0015677531804043823
Epoch 29/100: Training Loss: 0.0005774663452721812
Epoch 30/100: Training Loss: 0.0006171948522146494
Epoch 31/100: Training Loss: 0.0009445580968096213
Epoch 32/100: Training Loss: 0.0005199361615385745
Epoch 33/100: Training Loss: 0.000543895400374945
Epoch 34/100: Training Loss: 0.0003564550649900378
Epoch 35/100: Training Loss: 0.0011877338944768613
Epoch 36/100: Training Loss: 0.00044032803342386256
Epoch 37/100: Training Loss: 0.0015246162385296968
Epoch 38/100: Training Loss: 0.00033038694617207066
Epoch 39/100: Training Loss: 0.00015253191985235623
Epoch 40/100: Training Loss: 2.794932185506528e-05
Epoch 41/100: Training Loss: 0.00014330776968250977
Epoch 42/100: Training Loss: 0.00032514379434059
Epoch 43/100: Training Loss: 0.00045953270482139354
Epoch 44/100: Training Loss: 0.0004760840553447513
Epoch 45/100: Training Loss: 0.0004238131587490714
Epoch 46/100: Training Loss: 0.0003511162211924243
Epoch 47/100: Training Loss: 0.00032780456579536017
Epoch 48/100: Training Loss: 0.00019438578490099293
Epoch 49/100: Training Loss: 8.981261242386753e-05
Epoch 50/100: Training Loss: 9.584228616733493e-05
Epoch 51/100: Training Loss: 0.0003305314782938343
Epoch 52/100: Training Loss: 4.2314592611753135e-05
Epoch 53/100: Training Loss: 0.0004350765168301167
Epoch 54/100: Training Loss: 0.00016568390869655492
Epoch 55/100: Training Loss: 0.0011096024440110095
Epoch 56/100: Training Loss: 0.00013104221709301134
Epoch 57/100: Training Loss: 0.00013839948259613996
Epoch 58/100: Training Loss: 0.00026564793718373116
Epoch 59/100: Training Loss: 0.00012563728938804814
Epoch 60/100: Training Loss: 0.00014367964735791726
Epoch 61/100: Training Loss: 1.048051789288689e-05
Epoch 62/100: Training Loss: 6.502089690577033e-05
Epoch 63/100: Training Loss: 0.0013910389385340405
Epoch 64/100: Training Loss: 0.0002132324399026625
Epoch 65/100: Training Loss: 0.00013735041342264305
Epoch 66/100: Training Loss: 4.54949006506819e-06
Epoch 67/100: Training Loss: 8.363659533620611e-06
Epoch 68/100: Training Loss: 0.00033867052910518063
Epoch 69/100: Training Loss: 1.747387552782436e-05
Epoch 70/100: Training Loss: 1.766378534214994e-05
Epoch 71/100: Training Loss: 1.5570072562896583e-06
Epoch 72/100: Training Loss: 4.53101231093787e-05
Epoch 73/100: Training Loss: 6.5076443440983635e-06
Epoch 74/100: Training Loss: 3.3759323457267386e-05
Epoch 75/100: Training Loss: 9.542171831922663e-06
Epoch 76/100: Training Loss: 7.068976885610563e-06
Epoch 77/100: Training Loss: 1.557767848295668e-05
Epoch 78/100: Training Loss: 1.235837202542033e-05
Epoch 79/100: Training Loss: 1.2792935423507281e-05
Epoch 80/100: Training Loss: 3.093494490886027e-05
Epoch 81/100: Training Loss: 0.00042355604515485236
Epoch 82/100: Training Loss: 8.064765694545457e-06
Epoch 83/100: Training Loss: 0.00013830597323874022
Epoch 84/100: Training Loss: 1.4525435384980009e-05
Epoch 85/100: Training Loss: 1.0583634728257276e-05
Epoch 86/100: Training Loss: 4.5866226246066253e-07
Epoch 87/100: Training Loss: 7.4180971511119715e-06
Epoch 88/100: Training Loss: 3.765002635816124e-05
Epoch 89/100: Training Loss: 6.9830838042351366e-06
Epoch 90/100: Training Loss: 1.1244883576545155e-06
Epoch 91/100: Training Loss: 1.4050280892410161e-05
Epoch 92/100: Training Loss: 1.2826470535339936e-06
Epoch 93/100: Training Loss: 6.356177297335095e-06
Epoch 94/100: Training Loss: 0.00035314707576862873
Epoch 95/100: Training Loss: 3.2976714671343747e-06
Epoch 96/100: Training Loss: 8.609815848137086e-08
Epoch 97/100: Training Loss: 7.394088455047344e-06
Epoch 98/100: Training Loss: 0.0006989384172884233
Epoch 99/100: Training Loss: 2.5869649977771782e-05
Epoch 0/100: Training Loss: 0.004112183682026307
Epoch 1/100: Training Loss: 0.0026603644610913983
Epoch 2/100: Training Loss: 0.0035696314887766457
Epoch 3/100: Training Loss: 0.002884154305136277
Epoch 4/100: Training Loss: 0.0016300848290964139
Epoch 5/100: Training Loss: 0.0012143608982577647
Epoch 6/100: Training Loss: 0.000522001663600009
Epoch 7/100: Training Loss: 0.0005545575925908937
Epoch 8/100: Training Loss: 0.0025021623980048245
Epoch 9/100: Training Loss: 0.00031435343385474083
Epoch 10/100: Training Loss: 0.0005063777503791762
Epoch 11/100: Training Loss: 0.00047554624226927027
Epoch 12/100: Training Loss: 0.0018885536062205495
Epoch 13/100: Training Loss: 0.0019508486876458479
Epoch 14/100: Training Loss: 0.002572950409965281
Epoch 15/100: Training Loss: 0.0013413198886473485
Epoch 16/100: Training Loss: 0.0008743062523976425
Epoch 17/100: Training Loss: 0.0004741864427467065
Epoch 18/100: Training Loss: 0.0005600702451781992
Epoch 19/100: Training Loss: 0.0007457858397185437
Epoch 20/100: Training Loss: 0.000932265187333698
Epoch 21/100: Training Loss: 0.00039494631846258245
Epoch 22/100: Training Loss: 0.00044142020626302146
Epoch 23/100: Training Loss: 0.0004916293489421072
Epoch 24/100: Training Loss: 0.000411208284778829
Epoch 25/100: Training Loss: 0.0007574855824189683
Epoch 26/100: Training Loss: 0.0009981244802474976
Epoch 27/100: Training Loss: 0.0004770636467114548
Epoch 28/100: Training Loss: 0.0004413311649685257
Epoch 29/100: Training Loss: 0.00047073833233008354
Epoch 30/100: Training Loss: 0.0005614680746581657
Epoch 31/100: Training Loss: 0.0007462807776737799
Epoch 32/100: Training Loss: 0.0004982205987707969
Epoch 33/100: Training Loss: 0.00047247856855392456
Epoch 34/100: Training Loss: 0.0004710940861263158
Epoch 35/100: Training Loss: 0.00044170140854420104
Epoch 36/100: Training Loss: 0.0005032925700848819
Epoch 37/100: Training Loss: 0.0004736609254146646
Epoch 38/100: Training Loss: 0.0009348310210222115
Epoch 39/100: Training Loss: 0.00014697401527246815
Epoch 40/100: Training Loss: 0.00022358470167850423
Epoch 41/100: Training Loss: 0.00010850648481421675
Epoch 42/100: Training Loss: 0.0003587602197758259
Epoch 43/100: Training Loss: 0.00035519913935953856
Epoch 44/100: Training Loss: 0.00047961233949368715
Epoch 45/100: Training Loss: 0.0003574126702876179
Epoch 46/100: Training Loss: 0.0004887844612993346
Epoch 47/100: Training Loss: 0.0003367091751903113
Epoch 48/100: Training Loss: 0.0003673229023722783
Epoch 49/100: Training Loss: 0.0003328167496283362
Epoch 50/100: Training Loss: 2.0857134753941028e-05
Epoch 51/100: Training Loss: 0.0004506170109737139
Epoch 52/100: Training Loss: 0.00019886420091237028
Epoch 53/100: Training Loss: 0.0002519491991382435
Epoch 54/100: Training Loss: 0.00033990737119335337
Epoch 55/100: Training Loss: 8.26525130520569e-05
Epoch 56/100: Training Loss: 3.992029939966699e-05
Epoch 57/100: Training Loss: 0.00033947482613698107
Epoch 58/100: Training Loss: 0.0005846658001648137
Epoch 59/100: Training Loss: 0.0003003383660974678
Epoch 60/100: Training Loss: 0.0003688156010548761
Epoch 61/100: Training Loss: 0.00020075607702044621
Epoch 62/100: Training Loss: 0.0002313389734256487
Epoch 63/100: Training Loss: 4.835922179595093e-05
Epoch 64/100: Training Loss: 6.769933777809875e-06
Epoch 65/100: Training Loss: 2.3325585240235357e-05
Epoch 66/100: Training Loss: 2.884717109653116e-05
Epoch 67/100: Training Loss: 0.00015914924671313514
Epoch 68/100: Training Loss: 8.9887933851294e-06
Epoch 69/100: Training Loss: 0.0002113562877192819
Epoch 70/100: Training Loss: 0.00014842659280709693
Epoch 71/100: Training Loss: 6.008510437845453e-05
Epoch 72/100: Training Loss: 2.2979530340140582e-05
Epoch 73/100: Training Loss: 1.1170417230370585e-05
Epoch 74/100: Training Loss: 1.0472625933953224e-05
Epoch 75/100: Training Loss: 1.3127921583370928e-05
Epoch 76/100: Training Loss: 2.7316245336108414e-05
Epoch 77/100: Training Loss: 2.3505667601626344e-05
Epoch 78/100: Training Loss: 0.00016348734726569404
Epoch 79/100: Training Loss: 9.860508600992659e-05
Epoch 80/100: Training Loss: 3.283197757290916e-05
Epoch 81/100: Training Loss: 6.886905464886522e-06
Epoch 82/100: Training Loss: 0.00011621856707736758
Epoch 83/100: Training Loss: 5.5929771595937343e-05
Epoch 84/100: Training Loss: 0.0003755390415528069
Epoch 85/100: Training Loss: 2.511760414195207e-05
Epoch 86/100: Training Loss: 0.0009823510983238922
Epoch 87/100: Training Loss: 6.269595722890705e-06
Epoch 88/100: Training Loss: 4.788657273251586e-05
Epoch 89/100: Training Loss: 5.022477304300099e-06
Epoch 90/100: Training Loss: 0.00013615609220931867
Epoch 91/100: Training Loss: 4.546444268862894e-05
Epoch 92/100: Training Loss: 7.192664236927325e-06
Epoch 93/100: Training Loss: 7.171091263645266e-05
Epoch 94/100: Training Loss: 1.2624054644371103e-05
Epoch 95/100: Training Loss: 2.4430802309074285e-05
Epoch 96/100: Training Loss: 1.1118930233128231e-05
Epoch 97/100: Training Loss: 2.943532130012483e-05
Epoch 98/100: Training Loss: 2.9368428913362188e-05
Epoch 99/100: Training Loss: 5.8205506872911396e-05
Epoch 0/100: Training Loss: 0.004011282891583589
Epoch 1/100: Training Loss: 0.003318546374151312
Epoch 2/100: Training Loss: 0.004320543967873041
Epoch 3/100: Training Loss: 0.002200333984351597
Epoch 4/100: Training Loss: 0.002071777370078432
Epoch 5/100: Training Loss: 0.001107349351871233
Epoch 6/100: Training Loss: 0.0012239248840355435
Epoch 7/100: Training Loss: 0.0004847507718150601
Epoch 8/100: Training Loss: 0.0011318408272749075
Epoch 9/100: Training Loss: 0.0023708317908772663
Epoch 10/100: Training Loss: 0.0018111577063250396
Epoch 11/100: Training Loss: 0.001693119117818727
Epoch 12/100: Training Loss: 0.0018027673469730682
Epoch 13/100: Training Loss: 0.0008606369510018752
Epoch 14/100: Training Loss: 0.0006275253336122431
Epoch 15/100: Training Loss: 0.0007185777820692472
Epoch 16/100: Training Loss: 0.0003994301510003447
Epoch 17/100: Training Loss: 0.0004322286123878385
Epoch 18/100: Training Loss: 0.000505248461764283
Epoch 19/100: Training Loss: 0.0005890792864231975
Epoch 20/100: Training Loss: 0.00040020258880100367
Epoch 21/100: Training Loss: 0.00037277677307831
Epoch 22/100: Training Loss: 0.00028858644647832297
Epoch 23/100: Training Loss: 0.0005835107316268733
Epoch 24/100: Training Loss: 0.0008136937223329134
Epoch 25/100: Training Loss: 0.0003748532224652226
Epoch 26/100: Training Loss: 0.0010165875674756758
Epoch 27/100: Training Loss: 0.001038762689368125
Epoch 28/100: Training Loss: 0.0012014613378267347
Epoch 29/100: Training Loss: 0.0005158999580547122
Epoch 30/100: Training Loss: 0.00044770387966940007
Epoch 31/100: Training Loss: 0.0005281916333853833
Epoch 32/100: Training Loss: 0.0004838599017792684
Epoch 33/100: Training Loss: 0.00046097418647602295
Epoch 34/100: Training Loss: 0.00027418042893058686
Epoch 35/100: Training Loss: 0.00020220510067383936
Epoch 36/100: Training Loss: 6.479686526067418e-05
Epoch 37/100: Training Loss: 6.911932759306914e-05
Epoch 38/100: Training Loss: 0.0001750309477371672
Epoch 39/100: Training Loss: 0.00014101391646759642
Epoch 40/100: Training Loss: 0.00036434148169733996
Epoch 41/100: Training Loss: 0.0009954052468750375
Epoch 42/100: Training Loss: 0.000159069952880678
Epoch 43/100: Training Loss: 9.070782482258381e-05
Epoch 44/100: Training Loss: 0.00010257206308695437
Epoch 45/100: Training Loss: 0.00032256932163531065
Epoch 46/100: Training Loss: 0.0004510288955243819
Epoch 47/100: Training Loss: 9.755818801789196e-05
Epoch 48/100: Training Loss: 0.0011490144612599004
Epoch 49/100: Training Loss: 0.00019168157838970605
Epoch 50/100: Training Loss: 1.7286061189291667e-05
Epoch 51/100: Training Loss: 4.0892471610768444e-05
Epoch 52/100: Training Loss: 2.3205563186045073e-05
Epoch 53/100: Training Loss: 0.00023698233113698432
Epoch 54/100: Training Loss: 0.00010152915321244784
Epoch 55/100: Training Loss: 0.00030031020930208313
Epoch 56/100: Training Loss: 0.0005820932015319543
Epoch 57/100: Training Loss: 2.4259193269021673e-05
Epoch 58/100: Training Loss: 5.4584130461961944e-05
Epoch 59/100: Training Loss: 3.2772084389544707e-05
Epoch 60/100: Training Loss: 0.00010294250092623424
Epoch 61/100: Training Loss: 5.9377150667225655e-05
Epoch 62/100: Training Loss: 5.3596402878410245e-05
Epoch 63/100: Training Loss: 7.029868479520997e-05
Epoch 64/100: Training Loss: 0.000589621761825187
Epoch 65/100: Training Loss: 2.3502785072553377e-05
Epoch 66/100: Training Loss: 7.143416699090618e-05
Epoch 67/100: Training Loss: 0.00015591665462482196
Epoch 68/100: Training Loss: 8.793016381424629e-06
Epoch 69/100: Training Loss: 4.664891216606816e-06
Epoch 70/100: Training Loss: 4.32276458370905e-05
Epoch 71/100: Training Loss: 4.558877425234011e-05
Epoch 72/100: Training Loss: 1.3436239349878638e-05
Epoch 73/100: Training Loss: 2.955997854105534e-05
Epoch 74/100: Training Loss: 1.9738826317929784e-05
Epoch 75/100: Training Loss: 5.750289826762457e-06
Epoch 76/100: Training Loss: 0.004101199606444938
Epoch 77/100: Training Loss: 0.0029485861947931395
Epoch 78/100: Training Loss: 1.835048290118118e-05
Epoch 79/100: Training Loss: 2.3001502125175453e-05
Epoch 80/100: Training Loss: 5.250678403428727e-05
Epoch 81/100: Training Loss: 0.00030070639278259744
Epoch 82/100: Training Loss: 0.00010185840716756926
Epoch 83/100: Training Loss: 8.455402663468948e-06
Epoch 84/100: Training Loss: 3.3052359633376263e-06
Epoch 85/100: Training Loss: 2.2175960011346024e-06
Epoch 86/100: Training Loss: 8.677554343958482e-07
Epoch 87/100: Training Loss: 9.323886247134647e-05
Epoch 88/100: Training Loss: 1.7739277464434778e-06
Epoch 89/100: Training Loss: 0.00026293599441007604
Epoch 90/100: Training Loss: 5.717054680789723e-06
Epoch 91/100: Training Loss: 1.3713130693128504e-05
Epoch 92/100: Training Loss: 9.749966217890564e-07
Epoch 93/100: Training Loss: 5.041106969712336e-06
Epoch 94/100: Training Loss: 3.87569728730235e-07
Epoch 95/100: Training Loss: 2.0007281835573765e-06
Epoch 96/100: Training Loss: 5.503436136259441e-06
Epoch 97/100: Training Loss: 4.167196432277103e-06
Epoch 98/100: Training Loss: 3.840858930938997e-06
Epoch 99/100: Training Loss: 1.3654164725663198e-06
Epoch 0/100: Training Loss: 0.0026694849133491517
Epoch 1/100: Training Loss: 0.0018942363560199738
Epoch 2/100: Training Loss: 0.0035280950367450716
Epoch 3/100: Training Loss: 0.002785302698612213
Epoch 4/100: Training Loss: 0.002654777839779854
Epoch 5/100: Training Loss: 0.0015267635695636272
Epoch 6/100: Training Loss: 0.0021158166229724882
Epoch 7/100: Training Loss: 0.0016620470210909843
Epoch 8/100: Training Loss: 0.001924016699194908
Epoch 9/100: Training Loss: 0.0023637792095541955
Epoch 10/100: Training Loss: 0.0022217344492673876
Epoch 11/100: Training Loss: 0.0015260769985616207
Epoch 12/100: Training Loss: 0.0010580755770206451
Epoch 13/100: Training Loss: 0.0025783155113458635
Epoch 14/100: Training Loss: 0.0020818963646888735
Epoch 15/100: Training Loss: 0.0016382919624447823
Epoch 16/100: Training Loss: 0.0023597575724124908
Epoch 17/100: Training Loss: 0.0016383146867156029
Epoch 18/100: Training Loss: 0.0016030320897698403
Epoch 19/100: Training Loss: 0.0015302790328860284
Epoch 20/100: Training Loss: 0.0011649852618575095
Epoch 21/100: Training Loss: 0.0017936231568455696
Epoch 22/100: Training Loss: 0.0016135392710566522
Epoch 23/100: Training Loss: 0.0014423824846744536
Epoch 24/100: Training Loss: 0.0009557751007378101
Epoch 25/100: Training Loss: 0.0010216528549790382
Epoch 26/100: Training Loss: 0.0016983237117528915
Epoch 27/100: Training Loss: 0.0012980697676539422
Epoch 28/100: Training Loss: 0.0013261030428111554
Epoch 29/100: Training Loss: 0.0008780837059020996
Epoch 30/100: Training Loss: 0.0007823934778571129
Epoch 31/100: Training Loss: 0.0006323899608105421
Epoch 32/100: Training Loss: 0.0015177770517766475
Epoch 33/100: Training Loss: 0.0010412257164716721
Epoch 34/100: Training Loss: 0.0008631050586700439
Epoch 35/100: Training Loss: 0.0006295546423643828
Epoch 36/100: Training Loss: 0.0011524681001901626
Epoch 37/100: Training Loss: 0.0009503066539764404
Epoch 38/100: Training Loss: 0.0005592440720647573
Epoch 39/100: Training Loss: 0.0005766444839537144
Epoch 40/100: Training Loss: 0.00032773965504020455
Epoch 41/100: Training Loss: 0.0008731080219149589
Epoch 42/100: Training Loss: 0.0005937625654041767
Epoch 43/100: Training Loss: 0.0005173761397600174
Epoch 44/100: Training Loss: 0.0005064480006694794
Epoch 45/100: Training Loss: 0.000493383314460516
Epoch 46/100: Training Loss: 0.0006415811367332935
Epoch 47/100: Training Loss: 0.0005694924388080835
Epoch 48/100: Training Loss: 0.0005810024216771126
Epoch 49/100: Training Loss: 0.0006778615526854992
Epoch 50/100: Training Loss: 0.00048111886717379094
Epoch 51/100: Training Loss: 0.0007000112906098366
Epoch 52/100: Training Loss: 0.0007838536985218525
Epoch 53/100: Training Loss: 0.0005419109016656876
Epoch 54/100: Training Loss: 0.00047558438964188097
Epoch 55/100: Training Loss: 0.0006927927024662495
Epoch 56/100: Training Loss: 0.0008322501555085183
Epoch 57/100: Training Loss: 0.0007561827078461647
Epoch 58/100: Training Loss: 0.0004720384255051613
Epoch 59/100: Training Loss: 0.0004596177488565445
Epoch 60/100: Training Loss: 0.0003698963671922684
Epoch 61/100: Training Loss: 0.0005375855602324009
Epoch 62/100: Training Loss: 0.0007321306504309178
Epoch 63/100: Training Loss: 0.0004663600120693445
Epoch 64/100: Training Loss: 0.0004658866673707962
Epoch 65/100: Training Loss: 0.0004650666378438473
Epoch 66/100: Training Loss: 0.0004787215031683445
Epoch 67/100: Training Loss: 0.00046727294102311134
Epoch 68/100: Training Loss: 0.00048372028395533563
Epoch 69/100: Training Loss: 0.0005920283496379852
Epoch 70/100: Training Loss: 0.0006007829681038857
Epoch 71/100: Training Loss: 0.0007293364033102989
Epoch 72/100: Training Loss: 0.0003879355732351542
Epoch 73/100: Training Loss: 0.0001051474828273058
Epoch 74/100: Training Loss: 0.0001704713562503457
Epoch 75/100: Training Loss: 0.00029431888833642006
Epoch 76/100: Training Loss: 0.00011734075378626585
Epoch 77/100: Training Loss: 0.00020377435721457003
Epoch 78/100: Training Loss: 0.00032549463212490083
Epoch 79/100: Training Loss: 0.00032383066136389973
Epoch 80/100: Training Loss: 0.0002022024942561984
Epoch 81/100: Training Loss: 0.0003405365627259016
Epoch 82/100: Training Loss: 0.00017482135444879532
Epoch 83/100: Training Loss: 0.00029869629070162774
Epoch 84/100: Training Loss: 0.00019149205181747674
Epoch 85/100: Training Loss: 0.0001422587432898581
Epoch 86/100: Training Loss: 0.00010441368212923407
Epoch 87/100: Training Loss: 6.439221324399114e-05
Epoch 88/100: Training Loss: 0.00019626214634627104
Epoch 89/100: Training Loss: 0.0001954297535121441
Epoch 90/100: Training Loss: 5.8411649661138654e-05
Epoch 91/100: Training Loss: 2.8498211759142577e-05
Epoch 92/100: Training Loss: 0.00010900447377935052
Epoch 93/100: Training Loss: 1.7645934713073073e-05
Epoch 94/100: Training Loss: 2.3039850930217654e-05
Epoch 95/100: Training Loss: 7.330416701734066e-05
Epoch 96/100: Training Loss: 3.918878210242838e-05
Epoch 97/100: Training Loss: 3.2712053507566455e-05
Epoch 98/100: Training Loss: 6.869123317301273e-05
Epoch 99/100: Training Loss: 5.622873432002962e-05
Epoch 0/100: Training Loss: 0.003212747722864151
Epoch 1/100: Training Loss: 0.002711157128214836
Epoch 2/100: Training Loss: 0.002433926984667778
Epoch 3/100: Training Loss: 0.0024410685524344443
Epoch 4/100: Training Loss: 0.0032489120960235594
Epoch 5/100: Training Loss: 0.0024983761832118034
Epoch 6/100: Training Loss: 0.001291027758270502
Epoch 7/100: Training Loss: 0.0010654598474502564
Epoch 8/100: Training Loss: 0.0014452725648880006
Epoch 9/100: Training Loss: 0.00132936118170619
Epoch 10/100: Training Loss: 0.0014297915622591972
Epoch 11/100: Training Loss: 0.00165789145976305
Epoch 12/100: Training Loss: 0.0016942676156759262
Epoch 13/100: Training Loss: 0.0018080364912748336
Epoch 14/100: Training Loss: 0.0017196185886859893
Epoch 15/100: Training Loss: 0.0016372103244066238
Epoch 16/100: Training Loss: 0.0008069771341979503
Epoch 17/100: Training Loss: 0.0012909790500998497
Epoch 18/100: Training Loss: 0.0006056067533791066
Epoch 19/100: Training Loss: 0.0023115912452340128
Epoch 20/100: Training Loss: 0.0011710726656019687
Epoch 21/100: Training Loss: 0.001783282682299614
Epoch 22/100: Training Loss: 0.001807212084531784
Epoch 23/100: Training Loss: 0.001216648519039154
Epoch 24/100: Training Loss: 0.0013488121330738068
Epoch 25/100: Training Loss: 0.00157972052693367
Epoch 26/100: Training Loss: 0.001047099567949772
Epoch 27/100: Training Loss: 0.0009157879278063775
Epoch 28/100: Training Loss: 0.0011477163061499596
Epoch 29/100: Training Loss: 0.0006898501887917519
Epoch 30/100: Training Loss: 0.0009226852096617221
Epoch 31/100: Training Loss: 0.0011121660470962524
Epoch 32/100: Training Loss: 0.0009726986289024353
Epoch 33/100: Training Loss: 0.0008592328988015652
Epoch 34/100: Training Loss: 0.0006830325350165367
Epoch 35/100: Training Loss: 0.00047457139007747175
Epoch 36/100: Training Loss: 0.0009967084974050523
Epoch 37/100: Training Loss: 0.0004110317211598158
Epoch 38/100: Training Loss: 0.0013221383094787597
Epoch 39/100: Training Loss: 0.0012351433746516705
Epoch 40/100: Training Loss: 0.001157859992235899
Epoch 41/100: Training Loss: 0.0007174103520810604
Epoch 42/100: Training Loss: 0.000677756080403924
Epoch 43/100: Training Loss: 0.00016213578637689353
Epoch 44/100: Training Loss: 0.0004984518978744745
Epoch 45/100: Training Loss: 0.000654562609270215
Epoch 46/100: Training Loss: 0.0005868594162166119
Epoch 47/100: Training Loss: 0.0005215415731072426
Epoch 48/100: Training Loss: 0.001031484454870224
Epoch 49/100: Training Loss: 0.0006894317455589771
Epoch 50/100: Training Loss: 4.088360874447972e-05
Epoch 51/100: Training Loss: 0.0004240641370415688
Epoch 52/100: Training Loss: 0.00045798826031386854
Epoch 53/100: Training Loss: 0.0002959878882393241
Epoch 54/100: Training Loss: 0.0005411939695477485
Epoch 55/100: Training Loss: 0.0005954866297543049
Epoch 56/100: Training Loss: 0.0004862925969064236
Epoch 57/100: Training Loss: 0.00045152837410569193
Epoch 58/100: Training Loss: 0.000496356887742877
Epoch 59/100: Training Loss: 0.0004805325996130705
Epoch 60/100: Training Loss: 0.0004878166131675243
Epoch 61/100: Training Loss: 0.0005403419956564904
Epoch 62/100: Training Loss: 0.0005900966934859753
Epoch 63/100: Training Loss: 0.0004893770907074213
Epoch 64/100: Training Loss: 0.0003352397121489048
Epoch 65/100: Training Loss: 0.0006002176553010941
Epoch 66/100: Training Loss: 0.000459658307954669
Epoch 67/100: Training Loss: 0.00045647104270756244
Epoch 68/100: Training Loss: 0.0004031423944979906
Epoch 69/100: Training Loss: 0.0004424591548740864
Epoch 70/100: Training Loss: 0.0008410082198679447
Epoch 71/100: Training Loss: 0.0003255688585340977
Epoch 72/100: Training Loss: 0.0002936209551990032
Epoch 73/100: Training Loss: 0.00019237969536334276
Epoch 74/100: Training Loss: 7.129018194973469e-05
Epoch 75/100: Training Loss: 0.00022250667680054904
Epoch 76/100: Training Loss: 0.0002405571285635233
Epoch 77/100: Training Loss: 0.00047097955830395223
Epoch 78/100: Training Loss: 0.0002291735028848052
Epoch 79/100: Training Loss: 0.0003947780001908541
Epoch 80/100: Training Loss: 0.0004487750120460987
Epoch 81/100: Training Loss: 0.0004607853014022112
Epoch 82/100: Training Loss: 6.250693695619702e-05
Epoch 83/100: Training Loss: 8.422971004620194e-05
Epoch 84/100: Training Loss: 0.00012700341176241636
Epoch 85/100: Training Loss: 0.0004038103390485048
Epoch 86/100: Training Loss: 0.0003920750692486763
Epoch 87/100: Training Loss: 0.0003882463788613677
Epoch 88/100: Training Loss: 0.0005040296819061041
Epoch 89/100: Training Loss: 0.00038127771113067866
Epoch 90/100: Training Loss: 0.0001911454601213336
Epoch 91/100: Training Loss: 0.0002523570554330945
Epoch 92/100: Training Loss: 0.0012998465448617936
Epoch 93/100: Training Loss: 0.00028316122479736804
Epoch 94/100: Training Loss: 0.000264703924767673
Epoch 95/100: Training Loss: 0.0004390614572912455
Epoch 96/100: Training Loss: 5.247123190201819e-05
Epoch 97/100: Training Loss: 0.00011039796518161893
Epoch 98/100: Training Loss: 0.0004089323803782463
Epoch 99/100: Training Loss: 0.00038313621189445257
Epoch 0/100: Training Loss: 0.0033680010586977006
Epoch 1/100: Training Loss: 0.0027787692844867705
Epoch 2/100: Training Loss: 0.0028510076925158502
Epoch 3/100: Training Loss: 0.0030840817838907243
Epoch 4/100: Training Loss: 0.0028065314516425134
Epoch 5/100: Training Loss: 0.001963919773697853
Epoch 6/100: Training Loss: 0.0013696379959583283
Epoch 7/100: Training Loss: 0.0015820132568478585
Epoch 8/100: Training Loss: 0.001562649756669998
Epoch 9/100: Training Loss: 0.0024032976478338243
Epoch 10/100: Training Loss: 0.0022489313036203385
Epoch 11/100: Training Loss: 0.002051062136888504
Epoch 12/100: Training Loss: 0.002109127677977085
Epoch 13/100: Training Loss: 0.0018196545541286468
Epoch 14/100: Training Loss: 0.0016663102433085442
Epoch 15/100: Training Loss: 0.0019590236246585847
Epoch 16/100: Training Loss: 0.001730787567794323
Epoch 17/100: Training Loss: 0.0017546992748975754
Epoch 18/100: Training Loss: 0.0013918079435825347
Epoch 19/100: Training Loss: 0.0017124000936746598
Epoch 20/100: Training Loss: 0.0013498838059604168
Epoch 21/100: Training Loss: 0.00166583564132452
Epoch 22/100: Training Loss: 0.0010939680971205235
Epoch 23/100: Training Loss: 0.0015638917684555054
Epoch 24/100: Training Loss: 0.0012599153444170952
Epoch 25/100: Training Loss: 0.001065899059176445
Epoch 26/100: Training Loss: 0.0015224138274788856
Epoch 27/100: Training Loss: 0.0007224730215966702
Epoch 28/100: Training Loss: 0.0011223239824175834
Epoch 29/100: Training Loss: 0.0015383487567305564
Epoch 30/100: Training Loss: 0.000827487651258707
Epoch 31/100: Training Loss: 0.0008789930492639541
Epoch 32/100: Training Loss: 0.0011084884405136108
Epoch 33/100: Training Loss: 0.0007227562367916107
Epoch 34/100: Training Loss: 0.0013132268562912942
Epoch 35/100: Training Loss: 0.0012683632783591747
Epoch 36/100: Training Loss: 0.0006529404316097498
Epoch 37/100: Training Loss: 0.0006515903398394585
Epoch 38/100: Training Loss: 0.00070368149317801
Epoch 39/100: Training Loss: 0.0005743347108364105
Epoch 40/100: Training Loss: 0.0007036500610411167
Epoch 41/100: Training Loss: 0.0007661745883524417
Epoch 42/100: Training Loss: 0.0004887660499662161
Epoch 43/100: Training Loss: 0.000843341276049614
Epoch 44/100: Training Loss: 0.0008137829601764679
Epoch 45/100: Training Loss: 0.0005021417513489723
Epoch 46/100: Training Loss: 0.0006186102516949176
Epoch 47/100: Training Loss: 0.0005476068239659071
Epoch 48/100: Training Loss: 0.0004872418940067291
Epoch 49/100: Training Loss: 0.0007569856010377407
Epoch 50/100: Training Loss: 0.0005004939623177052
Epoch 51/100: Training Loss: 0.000279855914413929
Epoch 52/100: Training Loss: 0.0004439129959791899
Epoch 53/100: Training Loss: 0.000246692867949605
Epoch 54/100: Training Loss: 0.0008483100682497025
Epoch 55/100: Training Loss: 0.00032474505715072153
Epoch 56/100: Training Loss: 0.00018464038148522378
Epoch 57/100: Training Loss: 0.000601632334291935
Epoch 58/100: Training Loss: 0.00022489007096737623
Epoch 59/100: Training Loss: 0.00047325817868113516
Epoch 60/100: Training Loss: 0.00040219267830252645
Epoch 61/100: Training Loss: 0.000510329008102417
Epoch 62/100: Training Loss: 0.0004450746811926365
Epoch 63/100: Training Loss: 0.000505494512617588
Epoch 64/100: Training Loss: 0.0004514607135206461
Epoch 65/100: Training Loss: 0.0004700263496488333
Epoch 66/100: Training Loss: 0.0008974786847829819
Epoch 67/100: Training Loss: 0.0004617106635123491
Epoch 68/100: Training Loss: 7.2866294067353e-05
Epoch 69/100: Training Loss: 0.0002856852021068335
Epoch 70/100: Training Loss: 0.0007059653289616108
Epoch 71/100: Training Loss: 0.0014681069180369378
Epoch 72/100: Training Loss: 0.0005865890532732009
Epoch 73/100: Training Loss: 5.670107202604413e-05
Epoch 74/100: Training Loss: 0.0002892094198614359
Epoch 75/100: Training Loss: 1.1597469710977749e-05
Epoch 76/100: Training Loss: 0.0003798224264755845
Epoch 77/100: Training Loss: 0.00016534687019884587
Epoch 78/100: Training Loss: 0.000475961621850729
Epoch 79/100: Training Loss: 0.0005084552336484194
Epoch 80/100: Training Loss: 0.0005023076664656401
Epoch 81/100: Training Loss: 0.00020305514335632323
Epoch 82/100: Training Loss: 0.00010894933948293328
Epoch 83/100: Training Loss: 0.0003240456804633141
Epoch 84/100: Training Loss: 0.00017794384621083736
Epoch 85/100: Training Loss: 0.00013241386041045188
Epoch 86/100: Training Loss: 4.4555592467077076e-05
Epoch 87/100: Training Loss: 0.0004295302089303732
Epoch 88/100: Training Loss: 0.00030509806238114833
Epoch 89/100: Training Loss: 0.0005008914973586798
Epoch 90/100: Training Loss: 0.00011454701889306307
Epoch 91/100: Training Loss: 2.9628837364725768e-05
Epoch 92/100: Training Loss: 0.0002485957695171237
Epoch 93/100: Training Loss: 0.00033703551162034273
Epoch 94/100: Training Loss: 5.210902891121805e-05
Epoch 95/100: Training Loss: 7.041320204734803e-05
Epoch 96/100: Training Loss: 0.00023231077939271926
Epoch 97/100: Training Loss: 0.000470646983012557
Epoch 98/100: Training Loss: 0.0002439452102407813
Epoch 99/100: Training Loss: 0.0001934721483848989
Epoch 0/100: Training Loss: 0.0037885851161495137
Epoch 1/100: Training Loss: 0.0023700224745805097
Epoch 2/100: Training Loss: 0.00252943946297761
Epoch 3/100: Training Loss: 0.00229076291345487
Epoch 4/100: Training Loss: 0.0027699020637828075
Epoch 5/100: Training Loss: 0.001518119862125178
Epoch 6/100: Training Loss: 0.0014932749757341518
Epoch 7/100: Training Loss: 0.002499675674802938
Epoch 8/100: Training Loss: 0.0018487162650770443
Epoch 9/100: Training Loss: 0.0010975866940370791
Epoch 10/100: Training Loss: 0.0026509837739786524
Epoch 11/100: Training Loss: 0.0014505783084091868
Epoch 12/100: Training Loss: 0.0015624858391512732
Epoch 13/100: Training Loss: 0.0016591250896453857
Epoch 14/100: Training Loss: 0.0011532172845427398
Epoch 15/100: Training Loss: 0.0011711710008086673
Epoch 16/100: Training Loss: 0.001061650313389529
Epoch 17/100: Training Loss: 0.0010721065626022922
Epoch 18/100: Training Loss: 0.0010482468612634452
Epoch 19/100: Training Loss: 0.0016854990060162392
Epoch 20/100: Training Loss: 0.001269359307683957
Epoch 21/100: Training Loss: 0.001593027335063667
Epoch 22/100: Training Loss: 0.00223259181733344
Epoch 23/100: Training Loss: 0.0012814276347494428
Epoch 24/100: Training Loss: 0.0016021779768026559
Epoch 25/100: Training Loss: 0.0012070961818573581
Epoch 26/100: Training Loss: 0.0008114335263610645
Epoch 27/100: Training Loss: 0.0010935810341197215
Epoch 28/100: Training Loss: 0.0013906653899296074
Epoch 29/100: Training Loss: 0.0011039872647850377
Epoch 30/100: Training Loss: 0.000963630569968254
Epoch 31/100: Training Loss: 0.0014486091721589399
Epoch 32/100: Training Loss: 0.0005608856393273469
Epoch 33/100: Training Loss: 0.0009360131184766247
Epoch 34/100: Training Loss: 0.0012959999263666238
Epoch 35/100: Training Loss: 0.0012280564209458174
Epoch 36/100: Training Loss: 0.0007314550079357852
Epoch 37/100: Training Loss: 0.0012817840287639836
Epoch 38/100: Training Loss: 0.0015570912391516813
Epoch 39/100: Training Loss: 0.0005783819279093651
Epoch 40/100: Training Loss: 0.0003711130873412843
Epoch 41/100: Training Loss: 0.0010270766771523056
Epoch 42/100: Training Loss: 0.0011776603141408057
Epoch 43/100: Training Loss: 0.0007668218225430531
Epoch 44/100: Training Loss: 0.001614547838830644
Epoch 45/100: Training Loss: 0.0010291877066253856
Epoch 46/100: Training Loss: 0.0009655585144735445
Epoch 47/100: Training Loss: 0.0014257617057508725
Epoch 48/100: Training Loss: 0.0011469179847437864
Epoch 49/100: Training Loss: 0.0012185184439276434
Epoch 50/100: Training Loss: 0.001091432134816601
Epoch 51/100: Training Loss: 0.0005306651827636039
Epoch 52/100: Training Loss: 0.0006527031303211382
Epoch 53/100: Training Loss: 0.000680673084441264
Epoch 54/100: Training Loss: 0.0010731793512963946
Epoch 55/100: Training Loss: 0.000661307364512401
Epoch 56/100: Training Loss: 0.0008092574822674891
Epoch 57/100: Training Loss: 0.000664207775881336
Epoch 58/100: Training Loss: 0.0005941449362001602
Epoch 59/100: Training Loss: 0.001448107942654069
Epoch 60/100: Training Loss: 0.00026590156422299187
Epoch 61/100: Training Loss: 0.0006178155731243692
Epoch 62/100: Training Loss: 0.000502656362238963
Epoch 63/100: Training Loss: 0.00027509145201391473
Epoch 64/100: Training Loss: 0.000292903203873118
Epoch 65/100: Training Loss: 0.0009694144983959806
Epoch 66/100: Training Loss: 0.0009038336337751643
Epoch 67/100: Training Loss: 0.0005802609928094657
Epoch 68/100: Training Loss: 0.0005072868743519874
Epoch 69/100: Training Loss: 0.00047573266894954027
Epoch 70/100: Training Loss: 0.00031076390652140236
Epoch 71/100: Training Loss: 0.000373182403054207
Epoch 72/100: Training Loss: 0.0006477342099900458
Epoch 73/100: Training Loss: 0.000570631995322598
Epoch 74/100: Training Loss: 0.0005107128126606061
Epoch 75/100: Training Loss: 0.0007228427062368697
Epoch 76/100: Training Loss: 0.0006441191123549346
Epoch 77/100: Training Loss: 0.00021821421802423562
Epoch 78/100: Training Loss: 0.0012574267994826006
Epoch 79/100: Training Loss: 0.0005484009814110531
Epoch 80/100: Training Loss: 0.0008256089915135863
Epoch 81/100: Training Loss: 0.0004878914470125915
Epoch 82/100: Training Loss: 0.0001354358473401161
Epoch 83/100: Training Loss: 0.0004916952294149216
Epoch 84/100: Training Loss: 0.000494981172737802
Epoch 85/100: Training Loss: 2.3928562855454768e-05
Epoch 86/100: Training Loss: 5.204693242243141e-05
Epoch 87/100: Training Loss: 0.0004268833880971192
Epoch 88/100: Training Loss: 0.000713571811177928
Epoch 89/100: Training Loss: 0.0005084643982777929
Epoch 90/100: Training Loss: 0.0004414456665136252
Epoch 91/100: Training Loss: 0.00021834258630776862
Epoch 92/100: Training Loss: 0.00012817485317303116
Epoch 93/100: Training Loss: 0.00043090836257691595
Epoch 94/100: Training Loss: 0.0004237593169424944
Epoch 95/100: Training Loss: 0.0010154417176155529
Epoch 96/100: Training Loss: 3.78329805128134e-05
Epoch 97/100: Training Loss: 0.00042699372312825196
Epoch 98/100: Training Loss: 0.00044453039670446117
Epoch 99/100: Training Loss: 0.00040646016028276675
Epoch 0/100: Training Loss: 0.003459329058410256
Epoch 1/100: Training Loss: 0.002413656491382866
Epoch 2/100: Training Loss: 0.0021012321019628247
Epoch 3/100: Training Loss: 0.0014494330070580646
Epoch 4/100: Training Loss: 0.0016805757382872758
Epoch 5/100: Training Loss: 0.0038659314441073472
Epoch 6/100: Training Loss: 0.001157277423864717
Epoch 7/100: Training Loss: 0.0018744880606414405
Epoch 8/100: Training Loss: 0.0021238721859682896
Epoch 9/100: Training Loss: 0.0014713382834841491
Epoch 10/100: Training Loss: 0.0010096545621847651
Epoch 11/100: Training Loss: 0.0005699452131417147
Epoch 12/100: Training Loss: 0.0013016140574862243
Epoch 13/100: Training Loss: 0.0013440015019884535
Epoch 14/100: Training Loss: 0.0019227828189825557
Epoch 15/100: Training Loss: 0.0019237886948190678
Epoch 16/100: Training Loss: 0.0017188128772055267
Epoch 17/100: Training Loss: 0.0015939204556167505
Epoch 18/100: Training Loss: 0.0014202335647716645
Epoch 19/100: Training Loss: 0.0016885175826443228
Epoch 20/100: Training Loss: 0.0012310115015430815
Epoch 21/100: Training Loss: 0.0012110826695800588
Epoch 22/100: Training Loss: 0.0008958693902203992
Epoch 23/100: Training Loss: 0.0018054670209337951
Epoch 24/100: Training Loss: 0.002306621165791894
Epoch 25/100: Training Loss: 0.001717694625733005
Epoch 26/100: Training Loss: 0.0011855765322970736
Epoch 27/100: Training Loss: 0.0008189786391653073
Epoch 28/100: Training Loss: 0.0008267287615757839
Epoch 29/100: Training Loss: 0.0011862119671645437
Epoch 30/100: Training Loss: 0.0010361189295531837
Epoch 31/100: Training Loss: 0.0011770610406899907
Epoch 32/100: Training Loss: 0.0006572651635309693
Epoch 33/100: Training Loss: 0.0008312473251561451
Epoch 34/100: Training Loss: 0.0010730231263834959
Epoch 35/100: Training Loss: 0.001003754176911275
Epoch 36/100: Training Loss: 0.0009286058176854613
Epoch 37/100: Training Loss: 0.0007517772495366966
Epoch 38/100: Training Loss: 0.000915039212081083
Epoch 39/100: Training Loss: 0.000692877752386081
Epoch 40/100: Training Loss: 0.0010226753297125457
Epoch 41/100: Training Loss: 0.0007171944543054909
Epoch 42/100: Training Loss: 0.0012197237295709599
Epoch 43/100: Training Loss: 0.0009789212494139459
Epoch 44/100: Training Loss: 0.0015039692638785975
Epoch 45/100: Training Loss: 0.0011528606057926347
Epoch 46/100: Training Loss: 0.0005339809284088718
Epoch 47/100: Training Loss: 0.0008288732000217316
Epoch 48/100: Training Loss: 0.0009955744834462548
Epoch 49/100: Training Loss: 0.001199244503762312
Epoch 50/100: Training Loss: 0.0006897924052681893
Epoch 51/100: Training Loss: 0.0008020156128391339
Epoch 52/100: Training Loss: 0.000951529972872157
Epoch 53/100: Training Loss: 0.0005305711251155586
Epoch 54/100: Training Loss: 0.000562123432280911
Epoch 55/100: Training Loss: 0.000949411350450698
Epoch 56/100: Training Loss: 0.0005416155905480597
Epoch 57/100: Training Loss: 0.0007165553653316133
Epoch 58/100: Training Loss: 0.0005934684519555159
Epoch 59/100: Training Loss: 0.001549496392535556
Epoch 60/100: Training Loss: 0.0013252046837168895
Epoch 61/100: Training Loss: 0.00040074385655154086
Epoch 62/100: Training Loss: 0.0010991069921262705
Epoch 63/100: Training Loss: 0.001101123299568322
Epoch 64/100: Training Loss: 0.0009424071403066064
Epoch 65/100: Training Loss: 0.0005570983241318137
Epoch 66/100: Training Loss: 0.00014327652752399445
Epoch 67/100: Training Loss: 0.0006347582408577014
Epoch 68/100: Training Loss: 0.0006583799032648657
Epoch 69/100: Training Loss: 0.0006966756977093447
Epoch 70/100: Training Loss: 0.0005483703248819727
Epoch 71/100: Training Loss: 0.0009406841104956949
Epoch 72/100: Training Loss: 0.0009983772305166645
Epoch 73/100: Training Loss: 0.0006674735504350845
Epoch 74/100: Training Loss: 0.0011142202813154572
Epoch 75/100: Training Loss: 0.0009618492642785334
Epoch 76/100: Training Loss: 0.0011463114979920115
Epoch 77/100: Training Loss: 0.0009286425485732449
Epoch 78/100: Training Loss: 0.0006207962799224125
Epoch 79/100: Training Loss: 0.0010030489818305727
Epoch 80/100: Training Loss: 0.0005643947678766433
Epoch 81/100: Training Loss: 0.0001938294168490513
Epoch 82/100: Training Loss: 0.0005544113220682569
Epoch 83/100: Training Loss: 0.0005243971564207867
Epoch 84/100: Training Loss: 0.0005507586393386695
Epoch 85/100: Training Loss: 0.0006630579187611866
Epoch 86/100: Training Loss: 0.0004724169233042723
Epoch 87/100: Training Loss: 0.0005430389361776364
Epoch 88/100: Training Loss: 0.0005810998237816391
Epoch 89/100: Training Loss: 0.0005175666824267928
Epoch 90/100: Training Loss: 0.0005170985771592256
Epoch 91/100: Training Loss: 0.0001797295727167919
Epoch 92/100: Training Loss: 0.0004498968078831958
Epoch 93/100: Training Loss: 0.0005568696814737502
Epoch 94/100: Training Loss: 0.0005081789983305961
Epoch 95/100: Training Loss: 0.0005791052986102499
Epoch 96/100: Training Loss: 0.0005162194086487886
Epoch 97/100: Training Loss: 0.00025554034550478506
Epoch 98/100: Training Loss: 0.0003833522557452985
Epoch 99/100: Training Loss: 0.0005067733063060007
Epoch 0/100: Training Loss: 0.003737423070676767
Epoch 1/100: Training Loss: 0.00229431489470658
Epoch 2/100: Training Loss: 0.0025899649425676673
Epoch 3/100: Training Loss: 0.003478794340874739
Epoch 4/100: Training Loss: 0.003787632960422783
Epoch 5/100: Training Loss: 0.0020894129185160255
Epoch 6/100: Training Loss: 0.001895412517960664
Epoch 7/100: Training Loss: 0.002123882246624892
Epoch 8/100: Training Loss: 0.0036898441375440853
Epoch 9/100: Training Loss: 0.002677836046097385
Epoch 10/100: Training Loss: 0.0032337315522941055
Epoch 11/100: Training Loss: 0.0021238364991109085
Epoch 12/100: Training Loss: 0.0019454132219788374
Epoch 13/100: Training Loss: 0.0009747657236779571
Epoch 14/100: Training Loss: 0.0010502621246750946
Epoch 15/100: Training Loss: 0.0007448670021287955
Epoch 16/100: Training Loss: 0.0019803814067962063
Epoch 17/100: Training Loss: 0.0021048882964310373
Epoch 18/100: Training Loss: 0.0012000394854575964
Epoch 19/100: Training Loss: 0.0014539704580975186
Epoch 20/100: Training Loss: 0.0012362484529519536
Epoch 21/100: Training Loss: 0.0013779973148540328
Epoch 22/100: Training Loss: 0.0006427112848136076
Epoch 23/100: Training Loss: 0.0011943648005746732
Epoch 24/100: Training Loss: 0.0009014128120082199
Epoch 25/100: Training Loss: 0.0013471773475598379
Epoch 26/100: Training Loss: 0.001248174412235333
Epoch 27/100: Training Loss: 0.0013375007043218915
Epoch 28/100: Training Loss: 0.0014171429500458347
Epoch 29/100: Training Loss: 0.001215657990449553
Epoch 30/100: Training Loss: 0.0010871824565207122
Epoch 31/100: Training Loss: 0.0004588560124111783
Epoch 32/100: Training Loss: 0.0008715106423493403
Epoch 33/100: Training Loss: 0.00025663859430392075
Epoch 34/100: Training Loss: 0.0005504031469867487
Epoch 35/100: Training Loss: 0.0010623762941664192
Epoch 36/100: Training Loss: 0.0008600282061631512
Epoch 37/100: Training Loss: 0.0009710703306137377
Epoch 38/100: Training Loss: 0.0013307389939666554
Epoch 39/100: Training Loss: 0.001383905198164047
Epoch 40/100: Training Loss: 0.0011065858564559062
Epoch 41/100: Training Loss: 0.0010176965385485606
Epoch 42/100: Training Loss: 0.0009566993470404558
Epoch 43/100: Training Loss: 0.0007972819789959367
Epoch 44/100: Training Loss: 0.0008409068842602384
Epoch 45/100: Training Loss: 0.0009441259939959094
Epoch 46/100: Training Loss: 0.000787195316545523
Epoch 47/100: Training Loss: 0.0007231699623120059
Epoch 48/100: Training Loss: 0.0006240172560807246
Epoch 49/100: Training Loss: 0.0005350159421847884
Epoch 50/100: Training Loss: 0.0006437379938022346
Epoch 51/100: Training Loss: 0.0003991991186597545
Epoch 52/100: Training Loss: 0.0007908884317252287
Epoch 53/100: Training Loss: 0.0008397076729756252
Epoch 54/100: Training Loss: 0.00043413085732490396
Epoch 55/100: Training Loss: 0.0004791401943583397
Epoch 56/100: Training Loss: 0.000536792881929191
Epoch 57/100: Training Loss: 0.0007611147727176642
Epoch 58/100: Training Loss: 0.00026393814641199296
Epoch 59/100: Training Loss: 0.000525795635144422
Epoch 60/100: Training Loss: 0.0008266525473564294
Epoch 61/100: Training Loss: 0.0007760652880759756
Epoch 62/100: Training Loss: 0.0010017702344116892
Epoch 63/100: Training Loss: 0.0005946661446504532
Epoch 64/100: Training Loss: 0.0006712778548526156
Epoch 65/100: Training Loss: 0.0001602243086335006
Epoch 66/100: Training Loss: 0.0005883583027845735
Epoch 67/100: Training Loss: 0.0004105875446538257
Epoch 68/100: Training Loss: 0.0005583812476723058
Epoch 69/100: Training Loss: 0.0007113276678285781
Epoch 70/100: Training Loss: 0.0008065400609544888
Epoch 71/100: Training Loss: 0.0007824091015348009
Epoch 72/100: Training Loss: 0.000587974289420304
Epoch 73/100: Training Loss: 0.0006985619284544781
Epoch 74/100: Training Loss: 0.0011619976751363961
Epoch 75/100: Training Loss: 0.00047527595310454157
Epoch 76/100: Training Loss: 0.0007059705105556804
Epoch 77/100: Training Loss: 0.00044894759442396225
Epoch 78/100: Training Loss: 0.00034933093531875855
Epoch 79/100: Training Loss: 0.0006251867124988775
Epoch 80/100: Training Loss: 0.0006212911028770884
Epoch 81/100: Training Loss: 0.0006529576839155452
Epoch 82/100: Training Loss: 0.00108624548669074
Epoch 83/100: Training Loss: 0.000519534062807727
Epoch 84/100: Training Loss: 0.0007086560414854888
Epoch 85/100: Training Loss: 0.0002512222120336666
Epoch 86/100: Training Loss: 0.0005298609946184097
Epoch 87/100: Training Loss: 0.0005423381544981793
Epoch 88/100: Training Loss: 0.0002728374852875995
Epoch 89/100: Training Loss: 0.0005635313547340927
Epoch 90/100: Training Loss: 0.0009716249954928258
Epoch 91/100: Training Loss: 0.0005292990215265067
Epoch 92/100: Training Loss: 0.0005610592805655899
Epoch 93/100: Training Loss: 0.0005626346275305293
Epoch 94/100: Training Loss: 0.0005064423012126023
Epoch 95/100: Training Loss: 0.0005392668543347887
Epoch 96/100: Training Loss: 0.0005489695983327878
Epoch 97/100: Training Loss: 0.000605663106699658
Epoch 98/100: Training Loss: 0.0008420141259576105
Epoch 99/100: Training Loss: 0.0005365091903953795
Epoch 0/100: Training Loss: 0.0039889970362581165
Epoch 1/100: Training Loss: 0.0037403213267294777
Epoch 2/100: Training Loss: 0.004845252889671073
Epoch 3/100: Training Loss: 0.003959380238261444
Epoch 4/100: Training Loss: 0.004811308636570608
Epoch 5/100: Training Loss: 0.004105338987135729
Epoch 6/100: Training Loss: 0.004027643740571888
Epoch 7/100: Training Loss: 0.0045057795695121715
Epoch 8/100: Training Loss: 0.004661841108309512
Epoch 9/100: Training Loss: 0.004204718482415408
Epoch 10/100: Training Loss: 0.0030409423326024946
Epoch 11/100: Training Loss: 0.0041430032016425734
Epoch 12/100: Training Loss: 0.004217351509245816
Epoch 13/100: Training Loss: 0.0033158099414497023
Epoch 14/100: Training Loss: 0.004261690654502009
Epoch 15/100: Training Loss: 0.004083293163223772
Epoch 16/100: Training Loss: 0.00431437050269929
Epoch 17/100: Training Loss: 0.004257147675318434
Epoch 18/100: Training Loss: 0.003542137067049544
Epoch 19/100: Training Loss: 0.0034562519054539154
Epoch 20/100: Training Loss: 0.003557146385016031
Epoch 21/100: Training Loss: 0.0039051313273954077
Epoch 22/100: Training Loss: 0.004453242219836506
Epoch 23/100: Training Loss: 0.00413914311011106
Epoch 24/100: Training Loss: 0.0036596936105892357
Epoch 25/100: Training Loss: 0.004007067901409225
Epoch 26/100: Training Loss: 0.0036520081640079322
Epoch 27/100: Training Loss: 0.003323196970074382
Epoch 28/100: Training Loss: 0.0035154637911461836
Epoch 29/100: Training Loss: 0.0035888390825284237
Epoch 30/100: Training Loss: 0.003588771977961458
Epoch 31/100: Training Loss: 0.0032865262978913766
Epoch 32/100: Training Loss: 0.0034345767355912567
Epoch 33/100: Training Loss: 0.003126563417990476
Epoch 34/100: Training Loss: 0.0029255039249824373
Epoch 35/100: Training Loss: 0.0027259789555278044
Epoch 36/100: Training Loss: 0.0026259461775520777
Epoch 37/100: Training Loss: 0.00247004253185348
Epoch 38/100: Training Loss: 0.0027480074111989002
Epoch 39/100: Training Loss: 0.0024775160069497215
Epoch 40/100: Training Loss: 0.0023389265237265073
Epoch 41/100: Training Loss: 0.002708141574796462
Epoch 42/100: Training Loss: 0.002785417812549515
Epoch 43/100: Training Loss: 0.0028129537768711316
Epoch 44/100: Training Loss: 0.0025469194974330877
Epoch 45/100: Training Loss: 0.002843963981464209
Epoch 46/100: Training Loss: 0.0024365443267569636
Epoch 47/100: Training Loss: 0.0023213013118466006
Epoch 48/100: Training Loss: 0.001583784127866985
Epoch 49/100: Training Loss: 0.001864379999653393
Epoch 50/100: Training Loss: 0.0018416001701986554
Epoch 51/100: Training Loss: 0.0021011904375442605
Epoch 52/100: Training Loss: 0.0017342188500410673
Epoch 53/100: Training Loss: 0.0021704952448409125
Epoch 54/100: Training Loss: 0.0022269011728021483
Epoch 55/100: Training Loss: 0.0017320916352682556
Epoch 56/100: Training Loss: 0.001986899912752063
Epoch 57/100: Training Loss: 0.0015917142100681531
Epoch 58/100: Training Loss: 0.0015468976355546358
Epoch 59/100: Training Loss: 0.001242839836126921
Epoch 60/100: Training Loss: 0.0020423032195362825
Epoch 61/100: Training Loss: 0.0012490587518704647
Epoch 62/100: Training Loss: 0.001508560697763961
Epoch 63/100: Training Loss: 0.0021777464854006736
Epoch 64/100: Training Loss: 0.0018255759943399997
Epoch 65/100: Training Loss: 0.0020086532396985995
Epoch 66/100: Training Loss: 0.001880749369299175
Epoch 67/100: Training Loss: 0.0018558506144593094
Epoch 68/100: Training Loss: 0.0010572142948378001
Epoch 69/100: Training Loss: 0.0015275346127566912
Epoch 70/100: Training Loss: 0.0014922010977536637
Epoch 71/100: Training Loss: 0.001328693517785988
Epoch 72/100: Training Loss: 0.0013102148937073764
Epoch 73/100: Training Loss: 0.0014737477365708508
Epoch 74/100: Training Loss: 0.0015365179406096603
Epoch 75/100: Training Loss: 0.0010663597591665406
Epoch 76/100: Training Loss: 0.0015141717250773449
Epoch 77/100: Training Loss: 0.0014644012545907733
Epoch 78/100: Training Loss: 0.0013536651008176487
Epoch 79/100: Training Loss: 0.0015441967754174543
Epoch 80/100: Training Loss: 0.0011544272994363544
Epoch 81/100: Training Loss: 0.0014716400610690085
Epoch 82/100: Training Loss: 0.0012528125616098871
Epoch 83/100: Training Loss: 0.0015470394432939441
Epoch 84/100: Training Loss: 0.0015720992490945273
Epoch 85/100: Training Loss: 0.0015605183824008663
Epoch 86/100: Training Loss: 0.001341554502777706
Epoch 87/100: Training Loss: 0.0010976701580136027
Epoch 88/100: Training Loss: 0.0010683862184057173
Epoch 89/100: Training Loss: 0.0011694162886663779
Epoch 90/100: Training Loss: 0.001117146567793082
Epoch 91/100: Training Loss: 0.0014287985318543895
Epoch 92/100: Training Loss: 0.0012365214477311696
Epoch 93/100: Training Loss: 0.0014043969429091902
Epoch 94/100: Training Loss: 0.001705413819938306
Epoch 95/100: Training Loss: 0.001015136968221096
Epoch 96/100: Training Loss: 0.0005791143865774799
Epoch 97/100: Training Loss: 0.0009748383073617291
Epoch 98/100: Training Loss: 0.0011051162978671244
Epoch 99/100: Training Loss: 0.001211464602426188
Epoch 0/100: Training Loss: 0.0038160859354284425
Epoch 1/100: Training Loss: 0.0035374859310933295
Epoch 2/100: Training Loss: 0.004408782286359774
Epoch 3/100: Training Loss: 0.004585241639850945
Epoch 4/100: Training Loss: 0.0038467728539018442
Epoch 5/100: Training Loss: 0.004135959985240406
Epoch 6/100: Training Loss: 0.004243095189530329
Epoch 7/100: Training Loss: 0.0028930549195270664
Epoch 8/100: Training Loss: 0.004115594143899072
Epoch 9/100: Training Loss: 0.005083670679307142
Epoch 10/100: Training Loss: 0.0037807202497065462
Epoch 11/100: Training Loss: 0.00459526785162111
Epoch 12/100: Training Loss: 0.004600845425334198
Epoch 13/100: Training Loss: 0.004330232838131734
Epoch 14/100: Training Loss: 0.0036184444332754374
Epoch 15/100: Training Loss: 0.004411758571271076
Epoch 16/100: Training Loss: 0.003667000113733557
Epoch 17/100: Training Loss: 0.004140092047634504
Epoch 18/100: Training Loss: 0.00396355019499924
Epoch 19/100: Training Loss: 0.004137429969989701
Epoch 20/100: Training Loss: 0.004028920306275222
Epoch 21/100: Training Loss: 0.003805531176510236
Epoch 22/100: Training Loss: 0.0040263892799023765
Epoch 23/100: Training Loss: 0.0036572940302210927
Epoch 24/100: Training Loss: 0.004312081447500267
Epoch 25/100: Training Loss: 0.0030834959042782815
Epoch 26/100: Training Loss: 0.003774812679417086
Epoch 27/100: Training Loss: 0.0037605801954964137
Epoch 28/100: Training Loss: 0.002930638805919925
Epoch 29/100: Training Loss: 0.0030627600009867687
Epoch 30/100: Training Loss: 0.0027965723678765706
Epoch 31/100: Training Loss: 0.004238199714003809
Epoch 32/100: Training Loss: 0.0035186362582326726
Epoch 33/100: Training Loss: 0.002565493844202812
Epoch 34/100: Training Loss: 0.0032887746956174737
Epoch 35/100: Training Loss: 0.002198555414250355
Epoch 36/100: Training Loss: 0.0023840829631350687
Epoch 37/100: Training Loss: 0.002749161215017963
Epoch 38/100: Training Loss: 0.002711947587941656
Epoch 39/100: Training Loss: 0.002571201679722363
Epoch 40/100: Training Loss: 0.0029954168180756223
Epoch 41/100: Training Loss: 0.002766978661745589
Epoch 42/100: Training Loss: 0.0024484099931274816
Epoch 43/100: Training Loss: 0.0026133682159398567
Epoch 44/100: Training Loss: 0.002471409688722219
Epoch 45/100: Training Loss: 0.0024113872193342804
Epoch 46/100: Training Loss: 0.003215649270063994
Epoch 47/100: Training Loss: 0.0022622153459005796
Epoch 48/100: Training Loss: 0.0017766172917473395
Epoch 49/100: Training Loss: 0.0020660252760577673
Epoch 50/100: Training Loss: 0.0023653686836065837
Epoch 51/100: Training Loss: 0.002440162447114654
Epoch 52/100: Training Loss: 0.0020468876456582784
Epoch 53/100: Training Loss: 0.0021576810356796973
Epoch 54/100: Training Loss: 0.0023326826411367253
Epoch 55/100: Training Loss: 0.002295026716017565
Epoch 56/100: Training Loss: 0.0017928528469919368
Epoch 57/100: Training Loss: 0.002067706837559378
Epoch 58/100: Training Loss: 0.0012970933851027332
Epoch 59/100: Training Loss: 0.002211878039189522
Epoch 60/100: Training Loss: 0.001990405139544152
Epoch 61/100: Training Loss: 0.0021209673376272847
Epoch 62/100: Training Loss: 0.002262628631086539
Epoch 63/100: Training Loss: 0.001716031538729636
Epoch 64/100: Training Loss: 0.0021060389398739037
Epoch 65/100: Training Loss: 0.00119909466497156
Epoch 66/100: Training Loss: 0.001258277241757374
Epoch 67/100: Training Loss: 0.0015844898113351784
Epoch 68/100: Training Loss: 0.0015092050989732049
Epoch 69/100: Training Loss: 0.0018043758853381832
Epoch 70/100: Training Loss: 0.0015093774985003945
Epoch 71/100: Training Loss: 0.0018186367900166291
Epoch 72/100: Training Loss: 0.0012833252055755515
Epoch 73/100: Training Loss: 0.0017066943329691097
Epoch 74/100: Training Loss: 0.0018160118172500306
Epoch 75/100: Training Loss: 0.0017960924186453914
Epoch 76/100: Training Loss: 0.0014919349491990955
Epoch 77/100: Training Loss: 0.0010367576649646885
Epoch 78/100: Training Loss: 0.0012389421463012695
Epoch 79/100: Training Loss: 0.0010600453180982577
Epoch 80/100: Training Loss: 0.00168189781391068
Epoch 81/100: Training Loss: 0.0015647212006398383
Epoch 82/100: Training Loss: 0.001392019505532372
Epoch 83/100: Training Loss: 0.0013145930719691396
Epoch 84/100: Training Loss: 0.0014708109249342357
Epoch 85/100: Training Loss: 0.001700411174471015
Epoch 86/100: Training Loss: 0.001380634130231592
Epoch 87/100: Training Loss: 0.0016916117920780813
Epoch 88/100: Training Loss: 0.0013932155457553484
Epoch 89/100: Training Loss: 0.0013018641250812455
Epoch 90/100: Training Loss: 0.0016388074056991678
Epoch 91/100: Training Loss: 0.002080142300649984
Epoch 92/100: Training Loss: 0.0015308229339043825
Epoch 93/100: Training Loss: 0.002006916810345176
Epoch 94/100: Training Loss: 0.001462019042463492
Epoch 95/100: Training Loss: 0.000915536323919991
Epoch 96/100: Training Loss: 0.0008888169629684348
Epoch 97/100: Training Loss: 0.0012988226105835265
Epoch 98/100: Training Loss: 0.0015655363237620978
Epoch 99/100: Training Loss: 0.0011269582423153303
Epoch 0/100: Training Loss: 0.0035714412367107064
Epoch 1/100: Training Loss: 0.0036147228929380706
Epoch 2/100: Training Loss: 0.00401021944766013
Epoch 3/100: Training Loss: 0.004731635384212267
Epoch 4/100: Training Loss: 0.003960134572540687
Epoch 5/100: Training Loss: 0.0036272544734525366
Epoch 6/100: Training Loss: 0.0037412161858666024
Epoch 7/100: Training Loss: 0.003883490499281725
Epoch 8/100: Training Loss: 0.00459977767325395
Epoch 9/100: Training Loss: 0.004298362510883256
Epoch 10/100: Training Loss: 0.0040986770825670255
Epoch 11/100: Training Loss: 0.003828290677228511
Epoch 12/100: Training Loss: 0.004251241683959961
Epoch 13/100: Training Loss: 0.0039304265912794905
Epoch 14/100: Training Loss: 0.0036824992950388927
Epoch 15/100: Training Loss: 0.0033880014293241187
Epoch 16/100: Training Loss: 0.0029762209645959716
Epoch 17/100: Training Loss: 0.003640898805580392
Epoch 18/100: Training Loss: 0.003353639943710226
Epoch 19/100: Training Loss: 0.0035233166044121547
Epoch 20/100: Training Loss: 0.003632841126018802
Epoch 21/100: Training Loss: 0.0035872088362839047
Epoch 22/100: Training Loss: 0.0037651492270412822
Epoch 23/100: Training Loss: 0.0033539395458650903
Epoch 24/100: Training Loss: 0.0033278950792274727
Epoch 25/100: Training Loss: 0.0032466622377862993
Epoch 26/100: Training Loss: 0.0034065372896510244
Epoch 27/100: Training Loss: 0.0032178566155844175
Epoch 28/100: Training Loss: 0.0031870966715528476
Epoch 29/100: Training Loss: 0.0029469823205707877
Epoch 30/100: Training Loss: 0.002236655413709729
Epoch 31/100: Training Loss: 0.002681349879069044
Epoch 32/100: Training Loss: 0.0028900302798542756
Epoch 33/100: Training Loss: 0.0029487807229654677
Epoch 34/100: Training Loss: 0.0023711702681535126
Epoch 35/100: Training Loss: 0.0024285715147359483
Epoch 36/100: Training Loss: 0.0023317605454400677
Epoch 37/100: Training Loss: 0.0025762209039650215
Epoch 38/100: Training Loss: 0.0015585034690945353
Epoch 39/100: Training Loss: 0.0019680622792401853
Epoch 40/100: Training Loss: 0.0017388443283687365
Epoch 41/100: Training Loss: 0.0021048308603021483
Epoch 42/100: Training Loss: 0.0021016552353536845
Epoch 43/100: Training Loss: 0.0023531789416508958
Epoch 44/100: Training Loss: 0.00225661981184751
Epoch 45/100: Training Loss: 0.0014775596707072478
Epoch 46/100: Training Loss: 0.0023326463257240145
Epoch 47/100: Training Loss: 0.0016578733921051025
Epoch 48/100: Training Loss: 0.002142177512314146
Epoch 49/100: Training Loss: 0.002361131424935448
Epoch 50/100: Training Loss: 0.002057510693341691
Epoch 51/100: Training Loss: 0.0022920261550423327
Epoch 52/100: Training Loss: 0.0022817757350719527
Epoch 53/100: Training Loss: 0.0007295340595655884
Epoch 54/100: Training Loss: 0.002374812861941508
Epoch 55/100: Training Loss: 0.0011244263277938034
Epoch 56/100: Training Loss: 0.0021476429819271264
Epoch 57/100: Training Loss: 0.002080345390648242
Epoch 58/100: Training Loss: 0.0016335308157055582
Epoch 59/100: Training Loss: 0.0017984762097036603
Epoch 60/100: Training Loss: 0.0017989112051906965
Epoch 61/100: Training Loss: 0.0018244296904431274
Epoch 62/100: Training Loss: 0.0017453043271374229
Epoch 63/100: Training Loss: 0.0011061072744281088
Epoch 64/100: Training Loss: 0.0018628973834561985
Epoch 65/100: Training Loss: 0.001853187352616266
Epoch 66/100: Training Loss: 0.0019414665683215816
Epoch 67/100: Training Loss: 0.0013119530993581608
Epoch 68/100: Training Loss: 0.0009026296486128245
Epoch 69/100: Training Loss: 0.0016283787638935823
Epoch 70/100: Training Loss: 0.0019884553571410525
Epoch 71/100: Training Loss: 0.0019916702579978285
Epoch 72/100: Training Loss: 0.001543093892122736
Epoch 73/100: Training Loss: 0.0015969663266314576
Epoch 74/100: Training Loss: 0.00169749508630361
Epoch 75/100: Training Loss: 0.0016679740109980501
Epoch 76/100: Training Loss: 0.001595983047359037
Epoch 77/100: Training Loss: 0.0015423548537374333
Epoch 78/100: Training Loss: 0.0013044396575713
Epoch 79/100: Training Loss: 0.0013379893752912813
Epoch 80/100: Training Loss: 0.001566357762608307
Epoch 81/100: Training Loss: 0.0015182808929721252
Epoch 82/100: Training Loss: 0.0017403135236525378
Epoch 83/100: Training Loss: 0.0012891367571243387
Epoch 84/100: Training Loss: 0.0004042517763889389
Epoch 85/100: Training Loss: 0.0016340361723047219
Epoch 86/100: Training Loss: 0.0014803527996240073
Epoch 87/100: Training Loss: 0.0015291867666686607
Epoch 88/100: Training Loss: 0.0008726845316539537
Epoch 89/100: Training Loss: 0.001145248973606438
Epoch 90/100: Training Loss: 0.001540989177116495
Epoch 91/100: Training Loss: 0.0014093958384153859
Epoch 92/100: Training Loss: 0.0014608474756708208
Epoch 93/100: Training Loss: 0.0009276438627811457
Epoch 94/100: Training Loss: 0.0013504692458159086
Epoch 95/100: Training Loss: 0.0008351448553287431
Epoch 96/100: Training Loss: 0.0005930054266721208
Epoch 97/100: Training Loss: 0.0009876722531602872
Epoch 98/100: Training Loss: 0.0017966839256665565
Epoch 99/100: Training Loss: 0.0008984670931140319
Epoch 0/100: Training Loss: 0.003043489596422981
Epoch 1/100: Training Loss: 0.0016272993648753446
Epoch 2/100: Training Loss: 0.002203113366575802
Epoch 3/100: Training Loss: 0.0014346943182103773
Epoch 4/100: Training Loss: 0.0013181119280702928
Epoch 5/100: Training Loss: 0.0016414789592518527
Epoch 6/100: Training Loss: 0.0013573585187687594
Epoch 7/100: Training Loss: 0.0009380233638426837
Epoch 8/100: Training Loss: 0.0012093805214937994
Epoch 9/100: Training Loss: 0.0014407904709086699
Epoch 10/100: Training Loss: 0.0016886462183559642
Epoch 11/100: Training Loss: 0.002021261874367209
Epoch 12/100: Training Loss: 0.0005204139386906343
Epoch 13/100: Training Loss: 0.002079501222161686
Epoch 14/100: Training Loss: 0.0009984293404747457
Epoch 15/100: Training Loss: 0.0003448430010501076
Epoch 16/100: Training Loss: 0.001847833044388715
Epoch 17/100: Training Loss: 0.0013537100132773904
Epoch 18/100: Training Loss: 0.001275718562743243
Epoch 19/100: Training Loss: 0.0010363103712306304
Epoch 20/100: Training Loss: 0.0011085912585258484
Epoch 21/100: Training Loss: 0.0014013556873097139
Epoch 22/100: Training Loss: 0.001388638510423548
Epoch 23/100: Training Loss: 0.0008042048882035648
Epoch 24/100: Training Loss: 0.000755851759630091
Epoch 25/100: Training Loss: 0.0008082077783696792
Epoch 26/100: Training Loss: 0.0007360409287845387
Epoch 27/100: Training Loss: 0.0008082306560348062
Epoch 28/100: Training Loss: 0.0005316085236914018
Epoch 29/100: Training Loss: 0.00012712126926464192
Epoch 30/100: Training Loss: 0.0005043425980736227
Epoch 31/100: Training Loss: 0.00014371591455796185
Epoch 32/100: Training Loss: 0.0005286722498781541
Epoch 33/100: Training Loss: 0.0009429449544233434
Epoch 34/100: Training Loss: 0.0003693793626392589
Epoch 35/100: Training Loss: 0.0008557134691406699
Epoch 36/100: Training Loss: 0.00038853519979645224
Epoch 37/100: Training Loss: 0.0005994427291785969
Epoch 38/100: Training Loss: 0.0010718659443013808
Epoch 39/100: Training Loss: 0.0011319270905326393
Epoch 40/100: Training Loss: 0.0011370975305052364
Epoch 41/100: Training Loss: 0.0008225963396184584
Epoch 42/100: Training Loss: 0.0010236980283961576
Epoch 43/100: Training Loss: 0.0011067472836550545
Epoch 44/100: Training Loss: 0.0006436729255844565
Epoch 45/100: Training Loss: 0.001054278191398172
Epoch 46/100: Training Loss: 0.0005632467129651238
Epoch 47/100: Training Loss: 0.0005881221855387968
Epoch 48/100: Training Loss: 0.00029893221662325017
Epoch 49/100: Training Loss: 0.0011167515726650463
Epoch 50/100: Training Loss: 0.0008275288869352902
Epoch 51/100: Training Loss: 6.184817346579889e-05
Epoch 52/100: Training Loss: 0.0006170467857052298
Epoch 53/100: Training Loss: 0.0009083448087467867
Epoch 54/100: Training Loss: 0.0009862712200950175
Epoch 55/100: Training Loss: 0.0005098655819892883
Epoch 56/100: Training Loss: 0.00031175223343512593
Epoch 57/100: Training Loss: 0.00034372002324637243
Epoch 58/100: Training Loss: 0.00013107243267928854
Epoch 59/100: Training Loss: 7.561573977856076e-05
Epoch 60/100: Training Loss: 4.3092931018156165e-05
Epoch 61/100: Training Loss: 0.00027659417075269363
Epoch 62/100: Training Loss: 9.025312302743688e-05
Epoch 63/100: Training Loss: 0.0005430317538626054
Epoch 64/100: Training Loss: 9.6973535769126e-05
Epoch 65/100: Training Loss: 0.000247504430658677
Epoch 66/100: Training Loss: 0.0004443119553958668
Epoch 67/100: Training Loss: 0.0003741740303881028
Epoch 68/100: Training Loss: 0.000322305564494694
Epoch 69/100: Training Loss: 7.204507094095735e-05
Epoch 70/100: Training Loss: 0.0006603394799372729
Epoch 71/100: Training Loss: 0.0007017431890263277
Epoch 72/100: Training Loss: 0.0005454318926614873
Epoch 73/100: Training Loss: 2.444766680983936e-05
Epoch 74/100: Training Loss: 0.0002693512203062282
Epoch 75/100: Training Loss: 1.6967592048732673e-05
Epoch 76/100: Training Loss: 0.00044693863567184
Epoch 77/100: Training Loss: 0.0001800701131715494
Epoch 78/100: Training Loss: 0.00046815311207490807
Epoch 79/100: Training Loss: 0.0004999219494707444
Epoch 80/100: Training Loss: 0.0005085629137123333
Epoch 81/100: Training Loss: 9.702103595961543e-06
Epoch 82/100: Training Loss: 1.2880407602471464e-05
Epoch 83/100: Training Loss: 0.0005499212180866915
Epoch 84/100: Training Loss: 2.4137555566780707e-05
Epoch 85/100: Training Loss: 6.620286662569818e-06
Epoch 86/100: Training Loss: 7.511924075729707e-05
Epoch 87/100: Training Loss: 2.6890290353228065e-05
Epoch 88/100: Training Loss: 0.00017254109330037062
Epoch 89/100: Training Loss: 2.4539914310855024e-05
Epoch 90/100: Training Loss: 7.921799140817979e-05
Epoch 91/100: Training Loss: 1.7607365460956798e-05
Epoch 92/100: Training Loss: 9.605715818264905e-05
Epoch 93/100: Training Loss: 0.0003717132350977729
Epoch 94/100: Training Loss: 1.5754456741406637e-05
Epoch 95/100: Training Loss: 3.8588630473789045e-05
Epoch 96/100: Training Loss: 6.346214179168729e-06
Epoch 97/100: Training Loss: 2.6919473620022044e-05
Epoch 98/100: Training Loss: 5.484429352423724e-05
Epoch 99/100: Training Loss: 1.7928378998904544e-06
Epoch 0/100: Training Loss: 0.003272906471701229
Epoch 1/100: Training Loss: 0.001784974687239703
Epoch 2/100: Training Loss: 0.0018428141579908484
Epoch 3/100: Training Loss: 0.0017709355143939749
Epoch 4/100: Training Loss: 0.00147589830791249
Epoch 5/100: Training Loss: 0.001164794318816241
Epoch 6/100: Training Loss: 0.0010741896489087272
Epoch 7/100: Training Loss: 0.0012005449217908522
Epoch 8/100: Training Loss: 0.0016526462400660794
Epoch 9/100: Training Loss: 0.0018153011798858643
Epoch 10/100: Training Loss: 0.0014500355019288905
Epoch 11/100: Training Loss: 0.0009683019974652459
Epoch 12/100: Training Loss: 0.002025928216822007
Epoch 13/100: Training Loss: 0.0021973113803302542
Epoch 14/100: Training Loss: 0.00136393641724306
Epoch 15/100: Training Loss: 0.002025638783679289
Epoch 16/100: Training Loss: 0.0018512641682344325
Epoch 17/100: Training Loss: 0.0008673498735708349
Epoch 18/100: Training Loss: 0.001645261575193966
Epoch 19/100: Training Loss: 0.000514432174317977
Epoch 20/100: Training Loss: 0.0004676338504342472
Epoch 21/100: Training Loss: 0.001122337579727173
Epoch 22/100: Training Loss: 0.0013026724843417897
Epoch 23/100: Training Loss: 0.001414752094184651
Epoch 24/100: Training Loss: 0.001222071928136489
Epoch 25/100: Training Loss: 0.0005586581633371465
Epoch 26/100: Training Loss: 0.0009683873723534977
Epoch 27/100: Training Loss: 0.0005685597658157349
Epoch 28/100: Training Loss: 0.0005451446070390589
Epoch 29/100: Training Loss: 0.0003874800222761491
Epoch 30/100: Training Loss: 0.0005901999333325555
Epoch 31/100: Training Loss: 0.0002782927935614305
Epoch 32/100: Training Loss: 0.00044805657337693605
Epoch 33/100: Training Loss: 0.0010331515880192027
Epoch 34/100: Training Loss: 0.0009280108353670906
Epoch 35/100: Training Loss: 4.980824887752533e-05
Epoch 36/100: Training Loss: 0.0005630335387061624
Epoch 37/100: Training Loss: 7.578255608677864e-05
Epoch 38/100: Training Loss: 0.00011909910861183615
Epoch 39/100: Training Loss: 0.0006095591713400448
Epoch 40/100: Training Loss: 0.0007171249126686769
Epoch 41/100: Training Loss: 0.00010942898909835255
Epoch 42/100: Training Loss: 0.0003730097237755271
Epoch 43/100: Training Loss: 0.00013502344269962873
Epoch 44/100: Training Loss: 0.0007399623884874232
Epoch 45/100: Training Loss: 0.0009004226502250223
Epoch 46/100: Training Loss: 0.00017319489270448685
Epoch 47/100: Training Loss: 0.00045360791332581463
Epoch 48/100: Training Loss: 0.000914907455444336
Epoch 49/100: Training Loss: 0.0008968255975667168
Epoch 50/100: Training Loss: 0.0007039325202212615
Epoch 51/100: Training Loss: 5.860179114867659e-05
Epoch 52/100: Training Loss: 0.0005008980193558861
Epoch 53/100: Training Loss: 3.6277422024046675e-05
Epoch 54/100: Training Loss: 0.00027026095811058493
Epoch 55/100: Training Loss: 0.0008928516331840964
Epoch 56/100: Training Loss: 0.0004942479379036847
Epoch 57/100: Training Loss: 0.00019325844066984512
Epoch 58/100: Training Loss: 0.00016999573392026564
Epoch 59/100: Training Loss: 0.00034384262912413654
Epoch 60/100: Training Loss: 0.00013546292834422168
Epoch 61/100: Training Loss: 0.00017770235152805554
Epoch 62/100: Training Loss: 0.000650748479015687
Epoch 63/100: Training Loss: 0.0004379445577369017
Epoch 64/100: Training Loss: 0.00024008601903915405
Epoch 65/100: Training Loss: 0.00029547091792611515
Epoch 66/100: Training Loss: 0.0005484667771002825
Epoch 67/100: Training Loss: 0.00014479444088304745
Epoch 68/100: Training Loss: 2.4572420207893148e-05
Epoch 69/100: Training Loss: 0.00039387849323889787
Epoch 70/100: Training Loss: 0.00025862582466181584
Epoch 71/100: Training Loss: 0.0005226688788217657
Epoch 72/100: Training Loss: 8.93803477725562e-05
Epoch 73/100: Training Loss: 6.701277897638433e-05
Epoch 74/100: Training Loss: 7.790324880796321e-05
Epoch 75/100: Training Loss: 0.00026000006233944614
Epoch 76/100: Training Loss: 8.22899753556532e-05
Epoch 77/100: Training Loss: 0.00014930230510585448
Epoch 78/100: Training Loss: 0.000750654234605677
Epoch 79/100: Training Loss: 0.00045339544029796827
Epoch 80/100: Training Loss: 0.0004544659572489121
Epoch 81/100: Training Loss: 0.0001129945959238445
Epoch 82/100: Training Loss: 0.0007211680797969594
Epoch 83/100: Training Loss: 0.00012091134181793998
Epoch 84/100: Training Loss: 5.775998510858592e-05
Epoch 85/100: Training Loss: 5.1743274225908165e-05
Epoch 86/100: Training Loss: 4.800983211573432e-05
Epoch 87/100: Training Loss: 2.075735178283032e-05
Epoch 88/100: Training Loss: 4.11612495286938e-06
Epoch 89/100: Training Loss: 3.0684524544459934e-06
Epoch 90/100: Training Loss: 1.422547160045189e-05
Epoch 91/100: Training Loss: 3.5510403925881665e-05
Epoch 92/100: Training Loss: 2.0270120790776084e-05
Epoch 93/100: Training Loss: 5.7626844328992505e-05
Epoch 94/100: Training Loss: 2.1802916136734626e-05
Epoch 95/100: Training Loss: 1.0506388977827395e-05
Epoch 96/100: Training Loss: 1.8266778822769135e-05
Epoch 97/100: Training Loss: 1.731257721343461e-05
Epoch 98/100: Training Loss: 9.321676676764208e-05
Epoch 99/100: Training Loss: 8.574936895028633e-06
Epoch 0/100: Training Loss: 0.00266075519954457
Epoch 1/100: Training Loss: 0.002161963196361766
Epoch 2/100: Training Loss: 0.002214279244927799
Epoch 3/100: Training Loss: 0.0016409712679245892
Epoch 4/100: Training Loss: 0.0017192896674661075
Epoch 5/100: Training Loss: 0.0015544912394355325
Epoch 6/100: Training Loss: 0.0013180469765382654
Epoch 7/100: Training Loss: 0.00298464158002068
Epoch 8/100: Training Loss: 0.0026032926405177396
Epoch 9/100: Training Loss: 0.0014099238549961763
Epoch 10/100: Training Loss: 0.000906691130469827
Epoch 11/100: Training Loss: 0.0018259002881891588
Epoch 12/100: Training Loss: 0.0017026387593325446
Epoch 13/100: Training Loss: 0.002271518637152279
Epoch 14/100: Training Loss: 0.0002254608361160054
Epoch 15/100: Training Loss: 0.000820270531317767
Epoch 16/100: Training Loss: 0.0008502572774887085
Epoch 17/100: Training Loss: 0.00032621505944167866
Epoch 18/100: Training Loss: 0.0011947594144765068
Epoch 19/100: Training Loss: 0.0004972553866751054
Epoch 20/100: Training Loss: 0.0009666828548207003
Epoch 21/100: Training Loss: 0.0016472825232674094
Epoch 22/100: Training Loss: 0.0011883822434088764
Epoch 23/100: Training Loss: 0.001121916578096502
Epoch 24/100: Training Loss: 0.0006795870030627531
Epoch 25/100: Training Loss: 0.0002991595688988181
Epoch 26/100: Training Loss: 0.0007692655219751246
Epoch 27/100: Training Loss: 0.0001790004618027631
Epoch 28/100: Training Loss: 0.0008410064613117892
Epoch 29/100: Training Loss: 0.0005101559793247896
Epoch 30/100: Training Loss: 0.00026500093586304607
Epoch 31/100: Training Loss: 0.00016302767702761818
Epoch 32/100: Training Loss: 0.001431313157081604
Epoch 33/100: Training Loss: 0.0007820879711824305
Epoch 34/100: Training Loss: 0.00026977695524692534
Epoch 35/100: Training Loss: 0.0011638594024321613
Epoch 36/100: Training Loss: 0.0009640117778497584
Epoch 37/100: Training Loss: 0.00017796230009373498
Epoch 38/100: Training Loss: 6.355443311964765e-05
Epoch 39/100: Training Loss: 3.54235471390626e-05
Epoch 40/100: Training Loss: 0.0004550040206488441
Epoch 41/100: Training Loss: 0.001408703625202179
Epoch 42/100: Training Loss: 0.0008645518737680772
Epoch 43/100: Training Loss: 0.0009524329620249131
Epoch 44/100: Training Loss: 0.001202941729741938
Epoch 45/100: Training Loss: 0.00010374331956400591
Epoch 46/100: Training Loss: 0.0008340604165021111
Epoch 47/100: Training Loss: 0.00022865845000042635
Epoch 48/100: Training Loss: 0.0009281334631583269
Epoch 49/100: Training Loss: 0.0012695370351566988
Epoch 50/100: Training Loss: 0.003241268326254452
Epoch 51/100: Training Loss: 9.187562391161918e-05
Epoch 52/100: Training Loss: 0.00038514891091515036
Epoch 53/100: Training Loss: 0.0005734577775001526
Epoch 54/100: Training Loss: 0.00015013243126518587
Epoch 55/100: Training Loss: 0.0007342296488144818
Epoch 56/100: Training Loss: 5.7878246640457824e-05
Epoch 57/100: Training Loss: 6.14939202718875e-05
Epoch 58/100: Training Loss: 0.00020454942303545334
Epoch 59/100: Training Loss: 0.0006260658888255848
Epoch 60/100: Training Loss: 0.001062507401494419
Epoch 61/100: Training Loss: 8.133259766242083e-05
Epoch 62/100: Training Loss: 6.042306366212228e-05
Epoch 63/100: Training Loss: 0.00039584417553508984
Epoch 64/100: Training Loss: 0.00011183285318753299
Epoch 65/100: Training Loss: 4.435989403111093e-05
Epoch 66/100: Training Loss: 6.68053241337047e-05
Epoch 67/100: Training Loss: 5.156775279080166e-05
Epoch 68/100: Training Loss: 0.00010167447959675508
Epoch 69/100: Training Loss: 0.00013689222362111596
Epoch 70/100: Training Loss: 6.568262024837382e-05
Epoch 71/100: Training Loss: 0.0003970544566126431
Epoch 72/100: Training Loss: 0.00020616295583107891
Epoch 73/100: Training Loss: 0.0005782785222810858
Epoch 74/100: Training Loss: 0.0003092842286123949
Epoch 75/100: Training Loss: 0.0004670338157345267
Epoch 76/100: Training Loss: 0.0006005358608329997
Epoch 77/100: Training Loss: 0.0006121711257626028
Epoch 78/100: Training Loss: 0.0006131262463681839
Epoch 79/100: Training Loss: 4.640506470904631e-05
Epoch 80/100: Training Loss: 0.0006251549896071939
Epoch 81/100: Training Loss: 2.209019217201892e-05
Epoch 82/100: Training Loss: 0.00043539992150138404
Epoch 83/100: Training Loss: 3.591610995285651e-05
Epoch 84/100: Training Loss: 0.00044518148197847255
Epoch 85/100: Training Loss: 0.00044633264050764196
Epoch 86/100: Training Loss: 0.0002796673818546183
Epoch 87/100: Training Loss: 0.0004932943512411679
Epoch 88/100: Training Loss: 4.889337236390394e-05
Epoch 89/100: Training Loss: 3.14210623721866e-05
Epoch 90/100: Training Loss: 0.0005046123967451208
Epoch 91/100: Training Loss: 0.00023932242218185874
Epoch 92/100: Training Loss: 0.0002619962043621961
Epoch 93/100: Training Loss: 0.00018683652229168837
Epoch 94/100: Training Loss: 5.421270890270962e-05
Epoch 95/100: Training Loss: 4.8134695081149834e-05
Epoch 96/100: Training Loss: 1.5238539644462221e-05
Epoch 97/100: Training Loss: 1.0776938185753191e-05
Epoch 98/100: Training Loss: 0.00033196500119040995
Epoch 99/100: Training Loss: 0.0002835555111660677
dataset: capitals layer_num_from_end:-1 Avg_acc:tensor(0.7984) Avg_AUC:0.8904531892220083 Avg_threshold:0.5389919579029083
dataset: inventions layer_num_from_end:-1 Avg_acc:tensor(0.5422) Avg_AUC:0.7187334351634863 Avg_threshold:0.39599374930063885
dataset: elements layer_num_from_end:-1 Avg_acc:tensor(0.5627) Avg_AUC:0.6134820981230971 Avg_threshold:0.49777472019195557
dataset: animals layer_num_from_end:-1 Avg_acc:tensor(0.5635) Avg_AUC:0.6447861552028219 Avg_threshold:0.3000573664903641
dataset: companies layer_num_from_end:-1 Avg_acc:tensor(0.6917) Avg_AUC:0.8242981481481482 Avg_threshold:0.5871695876121521
dataset: facts layer_num_from_end:-1 Avg_acc:tensor(0.6471) Avg_AUC:0.7737568883762656 Avg_threshold:0.8497386376063029
