2024-04-24 02:18:02.027601: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-04-24 02:18:03.131926: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
================layer -1================
Epoch 0/10: Training Loss: 0.002995784257675384
Epoch 1/10: Training Loss: 0.0016332383339221661
Epoch 2/10: Training Loss: 0.002586991636903136
Epoch 3/10: Training Loss: 0.001950327749852534
Epoch 4/10: Training Loss: 0.002356031766304603
Epoch 5/10: Training Loss: 0.0016360898743142615
Epoch 6/10: Training Loss: 0.0026501144145752165
Epoch 7/10: Training Loss: 0.0015977424133074034
Epoch 8/10: Training Loss: 0.0018328349073450049
Epoch 9/10: Training Loss: 0.0009765165460693253
Epoch 0/10: Training Loss: 0.003023648595476484
Epoch 1/10: Training Loss: 0.0035061027620222184
Epoch 2/10: Training Loss: 0.0025567789594610254
Epoch 3/10: Training Loss: 0.0024306017618912915
Epoch 4/10: Training Loss: 0.0005515940956302456
Epoch 5/10: Training Loss: 0.0017315673452990871
Epoch 6/10: Training Loss: 0.0018372739945258295
Epoch 7/10: Training Loss: 0.0025624034704861943
Epoch 8/10: Training Loss: 0.001590245670371956
Epoch 9/10: Training Loss: 0.0017855154884445085
Epoch 0/10: Training Loss: 0.002623868363720554
Epoch 1/10: Training Loss: 0.0014145668153162604
Epoch 2/10: Training Loss: 0.003100983329586216
Epoch 3/10: Training Loss: 0.0028430065075000684
Epoch 4/10: Training Loss: 0.0017762921906851387
Epoch 5/10: Training Loss: 0.0012499000225867426
Epoch 6/10: Training Loss: 0.002122283607096105
Epoch 7/10: Training Loss: 0.0020729957760630786
Epoch 8/10: Training Loss: 0.0018498907972882678
Epoch 9/10: Training Loss: 0.0016404192347626587
Epoch 0/10: Training Loss: 0.001742024180347934
Epoch 1/10: Training Loss: 0.0016938166384316662
Epoch 2/10: Training Loss: 0.001529331861829465
Epoch 3/10: Training Loss: 0.001963822571046513
Epoch 4/10: Training Loss: 0.001349219058188924
Epoch 5/10: Training Loss: 0.0010866745849328538
Epoch 6/10: Training Loss: 0.0006325191881027689
Epoch 7/10: Training Loss: 0.00246861449048563
Epoch 8/10: Training Loss: 0.0013337300781823375
Epoch 9/10: Training Loss: 0.0007415249760896882
Epoch 0/10: Training Loss: 0.0018891110742018998
Epoch 1/10: Training Loss: 0.002076054826104568
Epoch 2/10: Training Loss: 0.0022351531528987767
Epoch 3/10: Training Loss: 0.0015448212989268859
Epoch 4/10: Training Loss: 0.0012438608093495748
Epoch 5/10: Training Loss: 0.0009962694228061138
Epoch 6/10: Training Loss: 0.001133012259664711
Epoch 7/10: Training Loss: 0.0023208337327453987
Epoch 8/10: Training Loss: 0.000896983442862341
Epoch 9/10: Training Loss: 0.00157404366446419
Epoch 0/10: Training Loss: 0.001966738444895832
Epoch 1/10: Training Loss: 0.0015961237114631326
Epoch 2/10: Training Loss: 0.0017282156490840794
Epoch 3/10: Training Loss: 0.0015625651994365857
Epoch 4/10: Training Loss: 0.0009892395486129574
Epoch 5/10: Training Loss: 0.0017469494255042514
Epoch 6/10: Training Loss: 0.0007960543127879043
Epoch 7/10: Training Loss: 0.0008784692528789029
Epoch 8/10: Training Loss: 0.0009213249924724088
Epoch 9/10: Training Loss: 0.0009259719607288852
Epoch 0/10: Training Loss: 0.0030341025441884993
Epoch 1/10: Training Loss: 0.0018163757398724557
Epoch 2/10: Training Loss: 0.0019454557448625564
Epoch 3/10: Training Loss: 0.0020129000768065453
Epoch 4/10: Training Loss: 0.0020372124388813974
Epoch 5/10: Training Loss: 0.0018937911838293076
Epoch 6/10: Training Loss: 0.0016781888902187348
Epoch 7/10: Training Loss: 0.001640806905925274
Epoch 8/10: Training Loss: 0.0015896478667855262
Epoch 9/10: Training Loss: 0.0012531289830803871
Epoch 0/10: Training Loss: 0.002458958700299263
Epoch 1/10: Training Loss: 0.002429891191422939
Epoch 2/10: Training Loss: 0.002367810346186161
Epoch 3/10: Training Loss: 0.0022626033052802084
Epoch 4/10: Training Loss: 0.002159722335636616
Epoch 5/10: Training Loss: 0.0018619906157255172
Epoch 6/10: Training Loss: 0.0019253822043538093
Epoch 7/10: Training Loss: 0.0018961379304528237
Epoch 8/10: Training Loss: 0.00192455742508173
Epoch 9/10: Training Loss: 0.0018413137644529344
Epoch 0/10: Training Loss: 0.0022889045998454095
Epoch 1/10: Training Loss: 0.0023738106712698935
Epoch 2/10: Training Loss: 0.0022807534784078597
Epoch 3/10: Training Loss: 0.0020464098080992697
Epoch 4/10: Training Loss: 0.0019411545246839524
Epoch 5/10: Training Loss: 0.0017099890857934952
Epoch 6/10: Training Loss: 0.001501045562326908
Epoch 7/10: Training Loss: 0.0012978089042007923
Epoch 8/10: Training Loss: 0.0011401841416954993
Epoch 9/10: Training Loss: 0.0011743560433387756
Epoch 0/10: Training Loss: 0.003311033461503922
Epoch 1/10: Training Loss: 0.0030440137644482264
Epoch 2/10: Training Loss: 0.002121161693220685
Epoch 3/10: Training Loss: 0.0018263248501309923
Epoch 4/10: Training Loss: 0.002325270396129341
Epoch 5/10: Training Loss: 0.001356858261831247
Epoch 6/10: Training Loss: 0.0018433434948040422
Epoch 7/10: Training Loss: 0.0017728175327276728
Epoch 8/10: Training Loss: 0.001996941247563453
Epoch 9/10: Training Loss: 0.002134103114437905
Epoch 0/10: Training Loss: 0.003825663001673996
Epoch 1/10: Training Loss: 0.0027123701040911827
Epoch 2/10: Training Loss: 0.001944947774243203
Epoch 3/10: Training Loss: 0.0021430605156406474
Epoch 4/10: Training Loss: 0.0025748035330681283
Epoch 5/10: Training Loss: 0.0027109118783549897
Epoch 6/10: Training Loss: 0.0021628226823867504
Epoch 7/10: Training Loss: 0.0021975845288319195
Epoch 8/10: Training Loss: 0.001958892223941293
Epoch 9/10: Training Loss: 0.0016422666561831335
Epoch 0/10: Training Loss: 0.0029342855997146313
Epoch 1/10: Training Loss: 0.0023238123602168574
Epoch 2/10: Training Loss: 0.0020634911622211434
Epoch 3/10: Training Loss: 0.0021565605880348544
Epoch 4/10: Training Loss: 0.003100863687551705
Epoch 5/10: Training Loss: 0.0020210532246121935
Epoch 6/10: Training Loss: 0.0021820358789650498
Epoch 7/10: Training Loss: 0.0019266388978168463
Epoch 8/10: Training Loss: 0.0018499571426658873
Epoch 9/10: Training Loss: 0.0020630154640051968
Epoch 0/10: Training Loss: 0.0028039515018463135
Epoch 1/10: Training Loss: 0.0028741478130517415
Epoch 2/10: Training Loss: 0.0027810149634910734
Epoch 3/10: Training Loss: 0.00275372728606723
Epoch 4/10: Training Loss: 0.0028584909360140367
Epoch 5/10: Training Loss: 0.002711041873653993
Epoch 6/10: Training Loss: 0.002512840245733198
Epoch 7/10: Training Loss: 0.002298772137686117
Epoch 8/10: Training Loss: 0.002541057321409516
Epoch 9/10: Training Loss: 0.0024790507278695012
Epoch 0/10: Training Loss: 0.0026279999720339744
Epoch 1/10: Training Loss: 0.002768758511701167
Epoch 2/10: Training Loss: 0.0026112583694079066
Epoch 3/10: Training Loss: 0.002433985866458211
Epoch 4/10: Training Loss: 0.002606739074189142
Epoch 5/10: Training Loss: 0.0026577519659964455
Epoch 6/10: Training Loss: 0.0020220135616150913
Epoch 7/10: Training Loss: 0.002638254931430943
Epoch 8/10: Training Loss: 0.001908475397438403
Epoch 9/10: Training Loss: 0.0022413446413760154
Epoch 0/10: Training Loss: 0.0028448039727495206
Epoch 1/10: Training Loss: 0.0027194338918521705
Epoch 2/10: Training Loss: 0.0030583824148241257
Epoch 3/10: Training Loss: 0.0029142958833681828
Epoch 4/10: Training Loss: 0.0027949938316218902
Epoch 5/10: Training Loss: 0.0024896404206358044
Epoch 6/10: Training Loss: 0.002295162109349737
Epoch 7/10: Training Loss: 0.002453662109690786
Epoch 8/10: Training Loss: 0.002658517352792601
Epoch 9/10: Training Loss: 0.002693824420701589
Epoch 0/10: Training Loss: 0.0005411232219022863
Epoch 1/10: Training Loss: 0.00037548029247452233
Epoch 2/10: Training Loss: 0.00016315681312014074
Epoch 3/10: Training Loss: 0.0010005332967814276
Epoch 4/10: Training Loss: 0.0004327877041171579
Epoch 5/10: Training Loss: 0.0003735999412396375
Epoch 6/10: Training Loss: 0.00023624379845226513
Epoch 7/10: Training Loss: 0.0003151343806701548
Epoch 8/10: Training Loss: 0.00013344959520241793
Epoch 9/10: Training Loss: 0.00024008869248278
Epoch 0/10: Training Loss: 0.000578783846953336
Epoch 1/10: Training Loss: 0.0006459760753547444
Epoch 2/10: Training Loss: 0.0001863160334965762
Epoch 3/10: Training Loss: 0.0001577572349239798
Epoch 4/10: Training Loss: 0.00016536506659844342
Epoch 5/10: Training Loss: 0.0010235306094674504
Epoch 6/10: Training Loss: 0.00044633947751101323
Epoch 7/10: Training Loss: 0.00030758917769965
Epoch 8/10: Training Loss: 0.001052608823075014
Epoch 9/10: Training Loss: 0.0015297479489270378
Epoch 0/10: Training Loss: 0.0006767254103632535
Epoch 1/10: Training Loss: 0.000573525665437474
Epoch 2/10: Training Loss: 0.0005495808142073014
Epoch 3/10: Training Loss: 0.0017425840391832239
Epoch 4/10: Training Loss: 0.0005034930565777947
Epoch 5/10: Training Loss: 0.0007382688276907977
Epoch 6/10: Training Loss: 0.001467709769220913
Epoch 7/10: Training Loss: 0.00020436925046584184
Epoch 8/10: Training Loss: 0.00024250055498936597
Epoch 9/10: Training Loss: 0.00019433680264388813
dataset: capitals layer_num_from_end:-1 Avg_acc:tensor(0.6621) Avg_AUC:0.7721399616012565 Avg_threshold:0.6079632838567098
dataset: inventions layer_num_from_end:-1 Avg_acc:tensor(0.7287) Avg_AUC:0.8088836346389913 Avg_threshold:0.26080162326494855
dataset: elements layer_num_from_end:-1 Avg_acc:tensor(0.6211) Avg_AUC:0.6951601341195515 Avg_threshold:0.7326322396596273
dataset: animals layer_num_from_end:-1 Avg_acc:tensor(0.6333) Avg_AUC:0.7814533992609389 Avg_threshold:0.5947181185086569
dataset: companies layer_num_from_end:-1 Avg_acc:tensor(0.6497) Avg_AUC:0.7419870370370371 Avg_threshold:0.46367595593134564
dataset: facts layer_num_from_end:-1 Avg_acc:tensor(0.6569) Avg_AUC:0.7385709912142054 Avg_threshold:0.8311158816019694


================layer -5================
Epoch 0/10: Training Loss: 0.002623423204555378
Epoch 1/10: Training Loss: 0.0020619993443255656
Epoch 2/10: Training Loss: 0.0021756112158715308
Epoch 3/10: Training Loss: 0.0032576862331870553
Epoch 4/10: Training Loss: 0.0023068489728274044
Epoch 5/10: Training Loss: 0.001157423207809875
Epoch 6/10: Training Loss: 0.0024249378617826876
Epoch 7/10: Training Loss: 0.0015901454261966518
Epoch 8/10: Training Loss: 0.002401732273035116
Epoch 9/10: Training Loss: 0.0016925833025178709
Epoch 0/10: Training Loss: 0.0034558748031829623
Epoch 1/10: Training Loss: 0.002157213596197275
Epoch 2/10: Training Loss: 0.002506135435371132
Epoch 3/10: Training Loss: 0.001957353178437773
Epoch 4/10: Training Loss: 0.002215184115029715
Epoch 5/10: Training Loss: 0.0018369753460784058
Epoch 6/10: Training Loss: 0.0014646935921448928
Epoch 7/10: Training Loss: 0.0015086899478952368
Epoch 8/10: Training Loss: 0.0016187580523791014
Epoch 9/10: Training Loss: 0.0017480995063181525
Epoch 0/10: Training Loss: 0.003011480286404803
Epoch 1/10: Training Loss: 0.0020173748473187427
Epoch 2/10: Training Loss: 0.001940255815332586
Epoch 3/10: Training Loss: 0.0019015663570457406
Epoch 4/10: Training Loss: 0.0029470155705938806
Epoch 5/10: Training Loss: 0.0020536167638285177
Epoch 6/10: Training Loss: 0.002442927210481017
Epoch 7/10: Training Loss: 0.0023645479362327734
Epoch 8/10: Training Loss: 0.0017539125222426194
Epoch 9/10: Training Loss: 0.0018176955359799046
Epoch 0/10: Training Loss: 0.001972473289337626
Epoch 1/10: Training Loss: 0.0010650643358932682
Epoch 2/10: Training Loss: 0.0006068224098784792
Epoch 3/10: Training Loss: 0.0015710842755674585
Epoch 4/10: Training Loss: 0.0007180350781218406
Epoch 5/10: Training Loss: 0.0008111820081991652
Epoch 6/10: Training Loss: 0.00038498608064066415
Epoch 7/10: Training Loss: 0.00022413325638858818
Epoch 8/10: Training Loss: 5.120612635202934e-05
Epoch 9/10: Training Loss: 0.0006393536559643189
Epoch 0/10: Training Loss: 0.002018030070088392
Epoch 1/10: Training Loss: 0.0009381029686313466
Epoch 2/10: Training Loss: 0.0005866519055483531
Epoch 3/10: Training Loss: 0.0005633923813609258
Epoch 4/10: Training Loss: 0.0006938785588814437
Epoch 5/10: Training Loss: 0.0006626127230609122
Epoch 6/10: Training Loss: 0.0012319251620696366
Epoch 7/10: Training Loss: 0.0005687793339688354
Epoch 8/10: Training Loss: 0.0003969525358428253
Epoch 9/10: Training Loss: 0.0009459008468440705
Epoch 0/10: Training Loss: 0.0018454089852198501
Epoch 1/10: Training Loss: 0.0011583046496280132
Epoch 2/10: Training Loss: 0.0006638453142043272
Epoch 3/10: Training Loss: 0.0007501292082429664
Epoch 4/10: Training Loss: 0.0009131207605081102
Epoch 5/10: Training Loss: 0.000459049788355096
Epoch 6/10: Training Loss: 0.0007561048938452832
Epoch 7/10: Training Loss: 0.0009605258337559144
Epoch 8/10: Training Loss: 0.0002624502439806066
Epoch 9/10: Training Loss: 0.001343897880952051
Epoch 0/10: Training Loss: 0.0015375969000160695
Epoch 1/10: Training Loss: 0.0008630918338894844
Epoch 2/10: Training Loss: 0.00022021904587745666
Epoch 3/10: Training Loss: 0.0009401551447808743
Epoch 4/10: Training Loss: 0.0006228155922144652
Epoch 5/10: Training Loss: 0.00024072064552456142
Epoch 6/10: Training Loss: 0.0006589311640709638
Epoch 7/10: Training Loss: 0.0004905613604933023
Epoch 8/10: Training Loss: 0.00045960857532918454
Epoch 9/10: Training Loss: 0.00036119178403168915
Epoch 0/10: Training Loss: 0.001144050806760788
Epoch 1/10: Training Loss: 0.0009163124486804008
Epoch 2/10: Training Loss: 0.0012443062849342823
Epoch 3/10: Training Loss: 0.001101528387516737
Epoch 4/10: Training Loss: 0.0005022110883146525
Epoch 5/10: Training Loss: 0.0010887520387768746
Epoch 6/10: Training Loss: 0.00026785440277308223
Epoch 7/10: Training Loss: 0.0007724353112280369
Epoch 8/10: Training Loss: 0.0007322103716433048
Epoch 9/10: Training Loss: 0.0004481193609535694
Epoch 0/10: Training Loss: 0.0010676279664039612
Epoch 1/10: Training Loss: 0.0014067309908568858
Epoch 2/10: Training Loss: 0.0011003884486854077
Epoch 3/10: Training Loss: 0.0008275900967419147
Epoch 4/10: Training Loss: 0.0014000599272549152
Epoch 5/10: Training Loss: 0.00030197158921509983
Epoch 6/10: Training Loss: 0.0004996652249246835
Epoch 7/10: Training Loss: 0.0009434556588530541
Epoch 8/10: Training Loss: 0.0008973287418484688
Epoch 9/10: Training Loss: 0.0007798731327056885
Epoch 0/10: Training Loss: 0.0013520932121641317
Epoch 1/10: Training Loss: 0.0009784546627360545
Epoch 2/10: Training Loss: 0.0009414974101789438
Epoch 3/10: Training Loss: 0.0009094434938613017
Epoch 4/10: Training Loss: 0.0006435069308918753
Epoch 5/10: Training Loss: 0.0010178029347377218
Epoch 6/10: Training Loss: 0.0008176289925909346
Epoch 7/10: Training Loss: 0.0008484564579216538
Epoch 8/10: Training Loss: 0.0005614347518629329
Epoch 9/10: Training Loss: 0.0007413425453149589
Epoch 0/10: Training Loss: 0.0010319392962060917
Epoch 1/10: Training Loss: 0.0009218312942298355
Epoch 2/10: Training Loss: 0.0009397959253590578
Epoch 3/10: Training Loss: 0.0009014204998684537
Epoch 4/10: Training Loss: 0.0007669490518843292
Epoch 5/10: Training Loss: 0.0011610730438475396
Epoch 6/10: Training Loss: 0.0010873923066315378
Epoch 7/10: Training Loss: 0.0006292288660243818
Epoch 8/10: Training Loss: 0.0005912270610499533
Epoch 9/10: Training Loss: 0.0006337162510604616
Epoch 0/10: Training Loss: 0.0014367719554597405
Epoch 1/10: Training Loss: 0.0009500169829957804
Epoch 2/10: Training Loss: 0.0005067852651996978
Epoch 3/10: Training Loss: 0.0005890567116676622
Epoch 4/10: Training Loss: 0.0006585536868708908
Epoch 5/10: Training Loss: 0.0008501811012340959
Epoch 6/10: Training Loss: 0.0008551618855470305
Epoch 7/10: Training Loss: 0.0008413157645304492
Epoch 8/10: Training Loss: 0.0007415718523560057
Epoch 9/10: Training Loss: 0.00044777457881125675
Epoch 0/10: Training Loss: 0.00214733232725535
Epoch 1/10: Training Loss: 0.0014398453251415531
Epoch 2/10: Training Loss: 0.0016111546991676685
Epoch 3/10: Training Loss: 0.0011710644952508788
Epoch 4/10: Training Loss: 0.0017367934943824414
Epoch 5/10: Training Loss: 0.001573396340900699
Epoch 6/10: Training Loss: 0.00128390339036651
Epoch 7/10: Training Loss: 0.0010056868688949686
Epoch 8/10: Training Loss: 0.00019934188747248112
Epoch 9/10: Training Loss: 0.0006588515362992192
Epoch 0/10: Training Loss: 0.0018545414833043585
Epoch 1/10: Training Loss: 0.0011918509243339893
Epoch 2/10: Training Loss: 0.0013295970610435436
Epoch 3/10: Training Loss: 0.0016969248948507751
Epoch 4/10: Training Loss: 0.00121639363023619
Epoch 5/10: Training Loss: 0.0021145687198007345
Epoch 6/10: Training Loss: 0.000989512201176574
Epoch 7/10: Training Loss: 0.001925304630734273
Epoch 8/10: Training Loss: 0.001276319982200269
Epoch 9/10: Training Loss: 0.0009815917899277037
Epoch 0/10: Training Loss: 0.0017582768241301277
Epoch 1/10: Training Loss: 0.0015408390799895027
Epoch 2/10: Training Loss: 0.0010360625405974736
Epoch 3/10: Training Loss: 0.0019590759908916146
Epoch 4/10: Training Loss: 0.001417661246085009
Epoch 5/10: Training Loss: 0.0017537932522249538
Epoch 6/10: Training Loss: 0.0016165774389608016
Epoch 7/10: Training Loss: 0.0010726421281991416
Epoch 8/10: Training Loss: 0.0014489044416819187
Epoch 9/10: Training Loss: 0.0011899367665612933
Epoch 0/10: Training Loss: 0.0004508020684999578
Epoch 1/10: Training Loss: 9.179815978688352e-05
Epoch 2/10: Training Loss: 8.574456853025099e-05
Epoch 3/10: Training Loss: 9.563862181761686e-05
Epoch 4/10: Training Loss: 0.0007986257181448095
Epoch 5/10: Training Loss: 0.0011068070636076086
Epoch 6/10: Training Loss: 0.0006555561195401584
Epoch 7/10: Training Loss: 0.0012919685419868021
Epoch 8/10: Training Loss: 0.001212602503159467
Epoch 9/10: Training Loss: 0.00015141419408952489
Epoch 0/10: Training Loss: 0.0002753128900247462
Epoch 1/10: Training Loss: 0.00014588916126419515
Epoch 2/10: Training Loss: 7.26531697985004e-05
Epoch 3/10: Training Loss: 5.899489145068561e-05
Epoch 4/10: Training Loss: 0.00010373806033064337
Epoch 5/10: Training Loss: 0.0009700524456360761
Epoch 6/10: Training Loss: 0.00018517578787663404
Epoch 7/10: Training Loss: 5.752049824770759e-05
Epoch 8/10: Training Loss: 8.600881761487792e-05
Epoch 9/10: Training Loss: 8.821014643591993e-05
Epoch 0/10: Training Loss: 0.00031971808742074407
Epoch 1/10: Training Loss: 0.00032310807967887207
Epoch 2/10: Training Loss: 9.021813816883985e-05
Epoch 3/10: Training Loss: 0.00013128908022361644
Epoch 4/10: Training Loss: 0.00020761049407369951
Epoch 5/10: Training Loss: 0.00012159523937632055
Epoch 6/10: Training Loss: 0.0009951716836761025
Epoch 7/10: Training Loss: 0.0002701636403799057
Epoch 8/10: Training Loss: 9.975965846987332e-05
Epoch 9/10: Training Loss: 8.048072786015622e-05
dataset: capitals layer_num_from_end:-5 Avg_acc:tensor(0.6555) Avg_AUC:0.8097998335343589 Avg_threshold:0.5075082381566366
dataset: inventions layer_num_from_end:-5 Avg_acc:tensor(0.7896) Avg_AUC:0.9151409580403973 Avg_threshold:0.2225684647758802
dataset: elements layer_num_from_end:-5 Avg_acc:tensor(0.6201) Avg_AUC:0.6862357883377653 Avg_threshold:0.3325948516527812
dataset: animals layer_num_from_end:-5 Avg_acc:tensor(0.6438) Avg_AUC:0.7686038149827833 Avg_threshold:0.33161095281442005
dataset: companies layer_num_from_end:-5 Avg_acc:tensor(0.6961) Avg_AUC:0.8100546296296297 Avg_threshold:0.550188422203064
dataset: facts layer_num_from_end:-5 Avg_acc:tensor(0.5757) Avg_AUC:0.6583240776339585 Avg_threshold:0.8486432035764059


================layer -9================
Epoch 0/10: Training Loss: 0.0019543498129277795
Epoch 1/10: Training Loss: 0.0013318052450260083
Epoch 2/10: Training Loss: 0.0011020353832444944
Epoch 3/10: Training Loss: 0.0018360152111186847
Epoch 4/10: Training Loss: 0.0014168277278646722
Epoch 5/10: Training Loss: 0.0015605094132723509
Epoch 6/10: Training Loss: 0.0006422126418227082
Epoch 7/10: Training Loss: 0.0009234304819907341
Epoch 8/10: Training Loss: 0.0007869729941541499
Epoch 9/10: Training Loss: 0.0005657214384812575
Epoch 0/10: Training Loss: 0.001833903205978287
Epoch 1/10: Training Loss: 0.001921406367441991
Epoch 2/10: Training Loss: 0.0010598503001086363
Epoch 3/10: Training Loss: 0.001441358269511403
Epoch 4/10: Training Loss: 0.000976497685159003
Epoch 5/10: Training Loss: 0.0006394369097856375
Epoch 6/10: Training Loss: 0.0012001341039484198
Epoch 7/10: Training Loss: 0.001072478565302762
Epoch 8/10: Training Loss: 0.0008529942039843206
Epoch 9/10: Training Loss: 0.0006357972230110969
Epoch 0/10: Training Loss: 0.0018761295538682204
Epoch 1/10: Training Loss: 0.0012575272288355794
Epoch 2/10: Training Loss: 0.001074324329416235
Epoch 3/10: Training Loss: 0.0008920525962656194
Epoch 4/10: Training Loss: 0.0012303491780807922
Epoch 5/10: Training Loss: 0.0007230012671097175
Epoch 6/10: Training Loss: 0.0008745070520814483
Epoch 7/10: Training Loss: 0.001030003691053057
Epoch 8/10: Training Loss: 0.0006951718376233027
Epoch 9/10: Training Loss: 0.0008194358511404557
Epoch 0/10: Training Loss: 0.001750564099820845
Epoch 1/10: Training Loss: 0.0009981716520215836
Epoch 2/10: Training Loss: 0.0007495918522583196
Epoch 3/10: Training Loss: 0.0006128042662070573
Epoch 4/10: Training Loss: 0.0005947153992448117
Epoch 5/10: Training Loss: 0.0008317212934142973
Epoch 6/10: Training Loss: 0.00017841837934555452
Epoch 7/10: Training Loss: 0.0006423610851077215
Epoch 8/10: Training Loss: 0.0007976736941951915
Epoch 9/10: Training Loss: 0.0007222625284107185
Epoch 0/10: Training Loss: 0.001761433537021005
Epoch 1/10: Training Loss: 0.0011596338697737712
Epoch 2/10: Training Loss: 0.000621076193323896
Epoch 3/10: Training Loss: 0.0005752504146172225
Epoch 4/10: Training Loss: 0.0009276469243816071
Epoch 5/10: Training Loss: 0.0003688124471281204
Epoch 6/10: Training Loss: 0.0005362523388277534
Epoch 7/10: Training Loss: 0.000495334115862115
Epoch 8/10: Training Loss: 0.00034381712049794344
Epoch 9/10: Training Loss: 7.084668528265749e-05
Epoch 0/10: Training Loss: 0.0017874551697011373
Epoch 1/10: Training Loss: 0.001002000427684901
Epoch 2/10: Training Loss: 0.0007533284784094688
Epoch 3/10: Training Loss: 0.0006644105527298582
Epoch 4/10: Training Loss: 0.00056081073232955
Epoch 5/10: Training Loss: 0.0003061640801970944
Epoch 6/10: Training Loss: 0.00035355054984794804
Epoch 7/10: Training Loss: 0.00029308335181394237
Epoch 8/10: Training Loss: 0.0004032859407319613
Epoch 9/10: Training Loss: 0.001120012870595499
Epoch 0/10: Training Loss: 0.0007007204461842775
Epoch 1/10: Training Loss: 0.00035266506019979713
Epoch 2/10: Training Loss: 0.0007011631969362497
Epoch 3/10: Training Loss: 0.0004290068987756968
Epoch 4/10: Training Loss: 0.0005578872747719288
Epoch 5/10: Training Loss: 0.0002099987119436264
Epoch 6/10: Training Loss: 0.0014494678005576133
Epoch 7/10: Training Loss: 0.0005164825357496739
Epoch 8/10: Training Loss: 0.0001988592091947794
Epoch 9/10: Training Loss: 0.00016127449925988911
Epoch 0/10: Training Loss: 0.0007297486066818237
Epoch 1/10: Training Loss: 0.0007610219530761242
Epoch 2/10: Training Loss: 0.00032237765844911337
Epoch 3/10: Training Loss: 0.0005349745508283377
Epoch 4/10: Training Loss: 0.00018771681934595107
Epoch 5/10: Training Loss: 0.0005080793052911758
Epoch 6/10: Training Loss: 0.0011102832853794099
Epoch 7/10: Training Loss: 0.0006447928957641125
Epoch 8/10: Training Loss: 0.0009236055426299572
Epoch 9/10: Training Loss: 0.0002777148270979524
Epoch 0/10: Training Loss: 0.0006733093876391649
Epoch 1/10: Training Loss: 0.0005581122823059559
Epoch 2/10: Training Loss: 0.0010978035628795625
Epoch 3/10: Training Loss: 0.0005410494748502969
Epoch 4/10: Training Loss: 0.0005883663892745971
Epoch 5/10: Training Loss: 0.0010209926404058933
Epoch 6/10: Training Loss: 0.000984154734760523
Epoch 7/10: Training Loss: 0.0007095637265592813
Epoch 8/10: Training Loss: 0.000840772595256567
Epoch 9/10: Training Loss: 0.0009185077622532844
Epoch 0/10: Training Loss: 0.0010869089205553578
Epoch 1/10: Training Loss: 0.0005497471162468005
Epoch 2/10: Training Loss: 0.0010878297553700246
Epoch 3/10: Training Loss: 0.0014083866670632818
Epoch 4/10: Training Loss: 0.0009279642135474333
Epoch 5/10: Training Loss: 0.0017943820755952483
Epoch 6/10: Training Loss: 0.0017055960217858576
Epoch 7/10: Training Loss: 0.001848319153876821
Epoch 8/10: Training Loss: 0.0017429275117861996
Epoch 9/10: Training Loss: 0.001607356937068283
Epoch 0/10: Training Loss: 0.0013054058809948575
Epoch 1/10: Training Loss: 0.0007962239016393187
Epoch 2/10: Training Loss: 0.0008318959527714237
Epoch 3/10: Training Loss: 0.0010045204952264287
Epoch 4/10: Training Loss: 0.0011400726570445262
Epoch 5/10: Training Loss: 0.0013058077377878178
Epoch 6/10: Training Loss: 0.0007876890954697967
Epoch 7/10: Training Loss: 0.0006583815167663963
Epoch 8/10: Training Loss: 0.0014716878438451487
Epoch 9/10: Training Loss: 0.0019626513028600415
Epoch 0/10: Training Loss: 0.0012187703399901178
Epoch 1/10: Training Loss: 0.000660177628705456
Epoch 2/10: Training Loss: 0.001149243135361155
Epoch 3/10: Training Loss: 0.0013139596220793997
Epoch 4/10: Training Loss: 0.0018176939457085481
Epoch 5/10: Training Loss: 0.0012171885390190563
Epoch 6/10: Training Loss: 0.0012785055834776276
Epoch 7/10: Training Loss: 0.0012636892734819156
Epoch 8/10: Training Loss: 0.0009510486748567812
Epoch 9/10: Training Loss: 0.000888258503500823
Epoch 0/10: Training Loss: 0.0016009157264469476
Epoch 1/10: Training Loss: 0.001036372504486943
Epoch 2/10: Training Loss: 0.0009661674104778972
Epoch 3/10: Training Loss: 0.0011108186071282192
Epoch 4/10: Training Loss: 0.0005915550305353885
Epoch 5/10: Training Loss: 0.0020812430129145943
Epoch 6/10: Training Loss: 0.0018421877298923518
Epoch 7/10: Training Loss: 0.0013456949532426746
Epoch 8/10: Training Loss: 0.001736222118731366
Epoch 9/10: Training Loss: 0.0013048362652987044
Epoch 0/10: Training Loss: 0.0013598165172614798
Epoch 1/10: Training Loss: 0.0019712659302136754
Epoch 2/10: Training Loss: 0.0011383946368236416
Epoch 3/10: Training Loss: 0.001392731208674955
Epoch 4/10: Training Loss: 0.0012374216357603767
Epoch 5/10: Training Loss: 0.001469753435905406
Epoch 6/10: Training Loss: 0.0012126729780475036
Epoch 7/10: Training Loss: 0.001578340960654202
Epoch 8/10: Training Loss: 0.001419131921616611
Epoch 9/10: Training Loss: 0.0014109142766093577
Epoch 0/10: Training Loss: 0.0013542269041996128
Epoch 1/10: Training Loss: 0.00158969455996886
Epoch 2/10: Training Loss: 0.0014060512678512674
Epoch 3/10: Training Loss: 0.0015612020595184226
Epoch 4/10: Training Loss: 0.000824070233382926
Epoch 5/10: Training Loss: 0.0013916524040777953
Epoch 6/10: Training Loss: 0.000968217948414632
Epoch 7/10: Training Loss: 0.0010962046140077098
Epoch 8/10: Training Loss: 0.0013332474508032893
Epoch 9/10: Training Loss: 0.000927764256268937
Epoch 0/10: Training Loss: 0.00026453067274654615
Epoch 1/10: Training Loss: 0.00011726014096947277
Epoch 2/10: Training Loss: 7.198462262749671e-05
Epoch 3/10: Training Loss: 6.875053924672743e-05
Epoch 4/10: Training Loss: 5.0185998792157455e-05
Epoch 5/10: Training Loss: 4.3662495034582475e-05
Epoch 6/10: Training Loss: 0.00011644349159563289
Epoch 7/10: Training Loss: 0.0009342302294338451
Epoch 8/10: Training Loss: 0.0007182927692637724
Epoch 9/10: Training Loss: 0.00040776444708599764
Epoch 0/10: Training Loss: 0.00020689585191362045
Epoch 1/10: Training Loss: 0.00012698560295736088
Epoch 2/10: Training Loss: 0.00012012079138966167
Epoch 3/10: Training Loss: 0.00041966788909014534
Epoch 4/10: Training Loss: 3.606548587627271e-05
Epoch 5/10: Training Loss: 7.885209668208571e-05
Epoch 6/10: Training Loss: 0.00016558350009076734
Epoch 7/10: Training Loss: 7.877933299716782e-05
Epoch 8/10: Training Loss: 0.0007222783916136798
Epoch 9/10: Training Loss: 0.000367900071775212
Epoch 0/10: Training Loss: 0.00030462145805358886
Epoch 1/10: Training Loss: 0.00011025053613326129
Epoch 2/10: Training Loss: 3.501466913696598e-05
Epoch 3/10: Training Loss: 5.702548386419521e-05
Epoch 4/10: Training Loss: 0.00023298590060542612
Epoch 5/10: Training Loss: 0.0008113847059362074
Epoch 6/10: Training Loss: 0.0004600744037067189
Epoch 7/10: Training Loss: 0.00017474268508308073
Epoch 8/10: Training Loss: 3.381152089466067e-05
Epoch 9/10: Training Loss: 0.00012019254705485175
dataset: capitals layer_num_from_end:-9 Avg_acc:tensor(0.7286) Avg_AUC:0.8672185623615792 Avg_threshold:0.37458046277364093
dataset: inventions layer_num_from_end:-9 Avg_acc:tensor(0.7789) Avg_AUC:0.909186335230443 Avg_threshold:0.40183964371681213
dataset: elements layer_num_from_end:-9 Avg_acc:tensor(0.6240) Avg_AUC:0.7077242070374224 Avg_threshold:0.66657026608785
dataset: animals layer_num_from_end:-9 Avg_acc:tensor(0.6405) Avg_AUC:0.802986163601243 Avg_threshold:0.18552997211615244
dataset: companies layer_num_from_end:-9 Avg_acc:tensor(0.7883) Avg_AUC:0.8959537037037036 Avg_threshold:0.5364565054575602
dataset: facts layer_num_from_end:-9 Avg_acc:tensor(0.7364) Avg_AUC:0.8495931052159427 Avg_threshold:0.810566782951355


================layer -13================
Epoch 0/10: Training Loss: 0.001705162487663589
Epoch 1/10: Training Loss: 0.0005995521282816267
Epoch 2/10: Training Loss: 0.0012744018039503298
Epoch 3/10: Training Loss: 0.0012356627445954543
Epoch 4/10: Training Loss: 0.0018583732051449223
Epoch 5/10: Training Loss: 0.0008075854682422184
Epoch 6/10: Training Loss: 0.001252370906042886
Epoch 7/10: Training Loss: 0.001147769026822977
Epoch 8/10: Training Loss: 0.000887258382110329
Epoch 9/10: Training Loss: 0.0008755364826509168
Epoch 0/10: Training Loss: 0.0013933250537285437
Epoch 1/10: Training Loss: 0.0005408644780412421
Epoch 2/10: Training Loss: 0.0006855144263147474
Epoch 3/10: Training Loss: 0.002559058524511911
Epoch 4/10: Training Loss: 0.002334560964490984
Epoch 5/10: Training Loss: 0.0016703016899682426
Epoch 6/10: Training Loss: 0.0011650968890090088
Epoch 7/10: Training Loss: 0.0009939763304236886
Epoch 8/10: Training Loss: 0.0006442958866799628
Epoch 9/10: Training Loss: 0.00045135408848315684
Epoch 0/10: Training Loss: 0.0015016897366597103
Epoch 1/10: Training Loss: 0.0009060010209783807
Epoch 2/10: Training Loss: 0.0016537426443366738
Epoch 3/10: Training Loss: 0.0009708115896144947
Epoch 4/10: Training Loss: 0.0025718524739458846
Epoch 5/10: Training Loss: 0.0016485439105467362
Epoch 6/10: Training Loss: 0.0015997359385857214
Epoch 7/10: Training Loss: 0.0011499090628190474
Epoch 8/10: Training Loss: 0.00106251208515434
Epoch 9/10: Training Loss: 0.001021647369944966
Epoch 0/10: Training Loss: 0.0017405037133971606
Epoch 1/10: Training Loss: 0.000836973449935211
Epoch 2/10: Training Loss: 0.0005735244396274076
Epoch 3/10: Training Loss: 0.0004639928600539459
Epoch 4/10: Training Loss: 0.0004922908965063973
Epoch 5/10: Training Loss: 0.0006200859974498398
Epoch 6/10: Training Loss: 0.0005878583510960538
Epoch 7/10: Training Loss: 0.0012130147657511425
Epoch 8/10: Training Loss: 0.000536786495176561
Epoch 9/10: Training Loss: 0.001224620941957813
Epoch 0/10: Training Loss: 0.0018582428159889268
Epoch 1/10: Training Loss: 0.0009192964233503752
Epoch 2/10: Training Loss: 0.00044230499699071874
Epoch 3/10: Training Loss: 0.0008683154370887148
Epoch 4/10: Training Loss: 0.0010427939562709784
Epoch 5/10: Training Loss: 0.0004027958479395673
Epoch 6/10: Training Loss: 0.00021071558349702987
Epoch 7/10: Training Loss: 0.0006607273604972231
Epoch 8/10: Training Loss: 9.81614565008257e-05
Epoch 9/10: Training Loss: 0.0007638188501808541
Epoch 0/10: Training Loss: 0.0017476593789878799
Epoch 1/10: Training Loss: 0.0009857151040270284
Epoch 2/10: Training Loss: 0.0006970443699988851
Epoch 3/10: Training Loss: 0.000607239870937324
Epoch 4/10: Training Loss: 0.000721699438212108
Epoch 5/10: Training Loss: 0.0010330217199091532
Epoch 6/10: Training Loss: 0.0009157506783315741
Epoch 7/10: Training Loss: 0.000876726731200891
Epoch 8/10: Training Loss: 0.0006064160104178213
Epoch 9/10: Training Loss: 0.0006063800373691722
Epoch 0/10: Training Loss: 0.0011713615618646144
Epoch 1/10: Training Loss: 0.000554609252139926
Epoch 2/10: Training Loss: 0.0005609997548162937
Epoch 3/10: Training Loss: 0.00047616134397685526
Epoch 4/10: Training Loss: 0.0006730835884809494
Epoch 5/10: Training Loss: 0.00040652123279869554
Epoch 6/10: Training Loss: 0.00045533445663750174
Epoch 7/10: Training Loss: 0.0002623108681291342
Epoch 8/10: Training Loss: 0.00022890190593898295
Epoch 9/10: Training Loss: 0.00022153679747134448
Epoch 0/10: Training Loss: 0.0010199095122516155
Epoch 1/10: Training Loss: 0.0006129082292318344
Epoch 2/10: Training Loss: 0.0004947242792695761
Epoch 3/10: Training Loss: 0.00035205285530537367
Epoch 4/10: Training Loss: 0.0003632004139944911
Epoch 5/10: Training Loss: 0.0001805813633836806
Epoch 6/10: Training Loss: 0.00021787825971841812
Epoch 7/10: Training Loss: 0.00041156820952892303
Epoch 8/10: Training Loss: 0.00020968723110854627
Epoch 9/10: Training Loss: 0.0002295510843396187
Epoch 0/10: Training Loss: 0.0010808422230184078
Epoch 1/10: Training Loss: 0.0005632699467241764
Epoch 2/10: Training Loss: 0.0005784234497696161
Epoch 3/10: Training Loss: 0.0003948566969484091
Epoch 4/10: Training Loss: 0.0005537436809390783
Epoch 5/10: Training Loss: 0.0004640848375856876
Epoch 6/10: Training Loss: 0.0003455787431448698
Epoch 7/10: Training Loss: 0.00029301575850695374
Epoch 8/10: Training Loss: 0.00022091788705438375
Epoch 9/10: Training Loss: 0.00010860100155696273
Epoch 0/10: Training Loss: 0.0016595419425113945
Epoch 1/10: Training Loss: 0.0008543068246476969
Epoch 2/10: Training Loss: 0.0005271501222233864
Epoch 3/10: Training Loss: 0.00038423984768284355
Epoch 4/10: Training Loss: 0.0010716292508848155
Epoch 5/10: Training Loss: 0.0009773771284492152
Epoch 6/10: Training Loss: 0.00042365515118191957
Epoch 7/10: Training Loss: 0.0004887122447323648
Epoch 8/10: Training Loss: 0.0007904576742725007
Epoch 9/10: Training Loss: 0.0008768472512056873
Epoch 0/10: Training Loss: 0.0014521879185536864
Epoch 1/10: Training Loss: 0.0006034842627063678
Epoch 2/10: Training Loss: 0.0006871137554478494
Epoch 3/10: Training Loss: 0.0007747947030765995
Epoch 4/10: Training Loss: 0.0010951394867745176
Epoch 5/10: Training Loss: 0.0006201909321128942
Epoch 6/10: Training Loss: 0.000639233904279721
Epoch 7/10: Training Loss: 0.0008522549252601186
Epoch 8/10: Training Loss: 0.0007151327789968745
Epoch 9/10: Training Loss: 0.0006819285309998093
Epoch 0/10: Training Loss: 0.0014256866304737748
Epoch 1/10: Training Loss: 0.0007289987840470235
Epoch 2/10: Training Loss: 0.0009397568216749058
Epoch 3/10: Training Loss: 0.0004937060794253258
Epoch 4/10: Training Loss: 0.0007286669722028599
Epoch 5/10: Training Loss: 0.0008661164219971675
Epoch 6/10: Training Loss: 0.0010229383304620244
Epoch 7/10: Training Loss: 0.0004369643558362487
Epoch 8/10: Training Loss: 0.0005812125790650677
Epoch 9/10: Training Loss: 0.0005604741489811309
Epoch 0/10: Training Loss: 0.0016475273481267967
Epoch 1/10: Training Loss: 0.0009782032066623107
Epoch 2/10: Training Loss: 0.0011134214748609935
Epoch 3/10: Training Loss: 0.0011527463300338645
Epoch 4/10: Training Loss: 0.0009772598940805094
Epoch 5/10: Training Loss: 0.0010836117512342946
Epoch 6/10: Training Loss: 0.0012937109201949164
Epoch 7/10: Training Loss: 0.001391335433682069
Epoch 8/10: Training Loss: 0.0012934878961929422
Epoch 9/10: Training Loss: 0.0011375447772196587
Epoch 0/10: Training Loss: 0.0015693393764116906
Epoch 1/10: Training Loss: 0.0015965685347058126
Epoch 2/10: Training Loss: 0.0015374839503244058
Epoch 3/10: Training Loss: 0.0016615826957273167
Epoch 4/10: Training Loss: 0.001302909574761296
Epoch 5/10: Training Loss: 0.0016037099408787608
Epoch 6/10: Training Loss: 0.0013199595623458458
Epoch 7/10: Training Loss: 0.0010847817390959784
Epoch 8/10: Training Loss: 0.0011330617974136049
Epoch 9/10: Training Loss: 0.0009750972520436673
Epoch 0/10: Training Loss: 0.0016194985599707293
Epoch 1/10: Training Loss: 0.0013343832942823701
Epoch 2/10: Training Loss: 0.0012821223562126918
Epoch 3/10: Training Loss: 0.0012401000948141741
Epoch 4/10: Training Loss: 0.001824590741403845
Epoch 5/10: Training Loss: 0.001700840446333222
Epoch 6/10: Training Loss: 0.0016400246628072877
Epoch 7/10: Training Loss: 0.0012995289848340267
Epoch 8/10: Training Loss: 0.0008938987523514704
Epoch 9/10: Training Loss: 0.0008150139786549752
Epoch 0/10: Training Loss: 0.0008570718414643232
Epoch 1/10: Training Loss: 0.00021366277600035947
Epoch 2/10: Training Loss: 0.00037428793661734635
Epoch 3/10: Training Loss: 0.00027699284255504607
Epoch 4/10: Training Loss: 0.0001711646861889783
Epoch 5/10: Training Loss: 0.0008166215875569512
Epoch 6/10: Training Loss: 0.00022576740559409646
Epoch 7/10: Training Loss: 0.00029895586125990926
Epoch 8/10: Training Loss: 0.00020336826496264514
Epoch 9/10: Training Loss: 0.00028342278126408074
Epoch 0/10: Training Loss: 0.0009133399409406326
Epoch 1/10: Training Loss: 0.0005883669590248781
Epoch 2/10: Training Loss: 0.0006227085257277769
Epoch 3/10: Training Loss: 0.0006075059228083667
Epoch 4/10: Training Loss: 0.0006244645837475271
Epoch 5/10: Training Loss: 0.0002521176548565135
Epoch 6/10: Training Loss: 0.0006630143698524027
Epoch 7/10: Training Loss: 0.0005354658207472633
Epoch 8/10: Training Loss: 0.0004371417795910555
Epoch 9/10: Training Loss: 0.0004870032562929041
Epoch 0/10: Training Loss: 0.0006896909983719096
Epoch 1/10: Training Loss: 0.0005149302675443537
Epoch 2/10: Training Loss: 0.0006404634345980252
Epoch 3/10: Training Loss: 0.0006784530246959013
Epoch 4/10: Training Loss: 0.0003959622891510234
Epoch 5/10: Training Loss: 0.0005895904320127824
Epoch 6/10: Training Loss: 0.0006015232818968155
Epoch 7/10: Training Loss: 0.0004186054801239687
Epoch 8/10: Training Loss: 0.00010731105199631523
Epoch 9/10: Training Loss: 0.0004964449826408834
dataset: capitals layer_num_from_end:-13 Avg_acc:tensor(0.8169) Avg_AUC:0.958679953811116 Avg_threshold:0.4058147768179576
dataset: inventions layer_num_from_end:-13 Avg_acc:tensor(0.8269) Avg_AUC:0.9341242606851914 Avg_threshold:0.2186556321879228
dataset: elements layer_num_from_end:-13 Avg_acc:tensor(0.6480) Avg_AUC:0.7324299533664779 Avg_threshold:0.5327231784661611
dataset: animals layer_num_from_end:-13 Avg_acc:tensor(0.6872) Avg_AUC:0.8085107499790039 Avg_threshold:0.5284706552823385
dataset: companies layer_num_from_end:-13 Avg_acc:tensor(0.8158) Avg_AUC:0.9143046296296297 Avg_threshold:0.5128811101118723
dataset: facts layer_num_from_end:-13 Avg_acc:tensor(0.7800) Avg_AUC:0.8679549888219631 Avg_threshold:0.6087499658266703


================layer -17================
Epoch 0/10: Training Loss: 0.001861783799591598
Epoch 1/10: Training Loss: 0.0014250441031022506
Epoch 2/10: Training Loss: 0.001242804673168209
Epoch 3/10: Training Loss: 0.000999142969404901
Epoch 4/10: Training Loss: 0.0009703422551388507
Epoch 5/10: Training Loss: 0.0011564154516566884
Epoch 6/10: Training Loss: 0.0009735493393211098
Epoch 7/10: Training Loss: 0.0014659340356613373
Epoch 8/10: Training Loss: 0.0009174583466736586
Epoch 9/10: Training Loss: 0.0011026918679684192
Epoch 0/10: Training Loss: 0.0016462622614173623
Epoch 1/10: Training Loss: 0.000979671528289368
Epoch 2/10: Training Loss: 0.0015428885296508148
Epoch 3/10: Training Loss: 0.001457953057089052
Epoch 4/10: Training Loss: 0.0014279290929540887
Epoch 5/10: Training Loss: 0.0012258686177380434
Epoch 6/10: Training Loss: 0.0016078292489885451
Epoch 7/10: Training Loss: 0.0011455071764392452
Epoch 8/10: Training Loss: 0.001037217207721897
Epoch 9/10: Training Loss: 0.0012523380818066897
Epoch 0/10: Training Loss: 0.001896390964934876
Epoch 1/10: Training Loss: 0.0010856544221197809
Epoch 2/10: Training Loss: 0.0010825042332802618
Epoch 3/10: Training Loss: 0.0011259683153846047
Epoch 4/10: Training Loss: 0.0016084708325512758
Epoch 5/10: Training Loss: 0.0010373560073492411
Epoch 6/10: Training Loss: 0.0014768485631142462
Epoch 7/10: Training Loss: 0.0014035786156887775
Epoch 8/10: Training Loss: 0.0011549739987700136
Epoch 9/10: Training Loss: 0.00091172733923772
Epoch 0/10: Training Loss: 0.001652330891486326
Epoch 1/10: Training Loss: 0.0009838042815038762
Epoch 2/10: Training Loss: 0.0007966178143682655
Epoch 3/10: Training Loss: 0.0003801061331860127
Epoch 4/10: Training Loss: 0.0008699536506383697
Epoch 5/10: Training Loss: 0.0006617674706903703
Epoch 6/10: Training Loss: 0.0006990019072052892
Epoch 7/10: Training Loss: 0.00035510296744802977
Epoch 8/10: Training Loss: 0.0003091552140522588
Epoch 9/10: Training Loss: 0.0002490254404720353
Epoch 0/10: Training Loss: 0.0016175652939849105
Epoch 1/10: Training Loss: 0.0010444612407976865
Epoch 2/10: Training Loss: 0.0009675388870063735
Epoch 3/10: Training Loss: 0.0005765165562278654
Epoch 4/10: Training Loss: 0.0002775187461288429
Epoch 5/10: Training Loss: 0.001059887599359992
Epoch 6/10: Training Loss: 0.0006389240819983687
Epoch 7/10: Training Loss: 0.0004768505509645661
Epoch 8/10: Training Loss: 0.0005699573941757342
Epoch 9/10: Training Loss: 0.00032157482910741324
Epoch 0/10: Training Loss: 0.001601516469124636
Epoch 1/10: Training Loss: 0.0009353913229667336
Epoch 2/10: Training Loss: 0.0007952383142307493
Epoch 3/10: Training Loss: 0.0003615446800103217
Epoch 4/10: Training Loss: 0.000477905653737074
Epoch 5/10: Training Loss: 0.0007940965013269998
Epoch 6/10: Training Loss: 0.0006832949862889717
Epoch 7/10: Training Loss: 0.00037023909618518104
Epoch 8/10: Training Loss: 0.0005393019574551494
Epoch 9/10: Training Loss: 0.00011372777627655334
Epoch 0/10: Training Loss: 0.001799185760319233
Epoch 1/10: Training Loss: 0.0011682311072945596
Epoch 2/10: Training Loss: 0.0013063110411167145
Epoch 3/10: Training Loss: 0.0010586483404040337
Epoch 4/10: Training Loss: 0.0006056696176528931
Epoch 5/10: Training Loss: 0.000592541741207242
Epoch 6/10: Training Loss: 0.0004682880826294422
Epoch 7/10: Training Loss: 0.00032735883723944426
Epoch 8/10: Training Loss: 0.00025423858314752577
Epoch 9/10: Training Loss: 0.0002538787433877587
Epoch 0/10: Training Loss: 0.0016825620085000991
Epoch 1/10: Training Loss: 0.000915437750518322
Epoch 2/10: Training Loss: 0.0007858932949602604
Epoch 3/10: Training Loss: 0.0007903889752924442
Epoch 4/10: Training Loss: 0.0008125063963234424
Epoch 5/10: Training Loss: 0.0006577954161912202
Epoch 6/10: Training Loss: 0.00044333753176033496
Epoch 7/10: Training Loss: 0.0004393666051328182
Epoch 8/10: Training Loss: 0.00037002540193498135
Epoch 9/10: Training Loss: 0.0003492878982797265
Epoch 0/10: Training Loss: 0.0015774602070450783
Epoch 1/10: Training Loss: 0.0010024681687355042
Epoch 2/10: Training Loss: 0.0010957627557218075
Epoch 3/10: Training Loss: 0.0010015429928898812
Epoch 4/10: Training Loss: 0.0006755502428859473
Epoch 5/10: Training Loss: 0.0004868632648140192
Epoch 6/10: Training Loss: 0.0004758517257869244
Epoch 7/10: Training Loss: 0.0003379507688805461
Epoch 8/10: Training Loss: 0.0002130672102794051
Epoch 9/10: Training Loss: 0.00032151008490473033
Epoch 0/10: Training Loss: 0.002360512876206902
Epoch 1/10: Training Loss: 0.0018977268486265924
Epoch 2/10: Training Loss: 0.0018280981832249148
Epoch 3/10: Training Loss: 0.0014135634443562502
Epoch 4/10: Training Loss: 0.001062252529107841
Epoch 5/10: Training Loss: 0.0008716532948670114
Epoch 6/10: Training Loss: 0.0009058796486277489
Epoch 7/10: Training Loss: 0.0006219032843401477
Epoch 8/10: Training Loss: 0.0005175419578886336
Epoch 9/10: Training Loss: 0.0005425031113017137
Epoch 0/10: Training Loss: 0.0022198229458681336
Epoch 1/10: Training Loss: 0.001319394939264674
Epoch 2/10: Training Loss: 0.0019080209883914631
Epoch 3/10: Training Loss: 0.0018281577878696902
Epoch 4/10: Training Loss: 0.001487151073042754
Epoch 5/10: Training Loss: 0.0012831897682445065
Epoch 6/10: Training Loss: 0.000808114743536445
Epoch 7/10: Training Loss: 0.0006142587988239945
Epoch 8/10: Training Loss: 0.0004157095578066103
Epoch 9/10: Training Loss: 0.00042518204564501524
Epoch 0/10: Training Loss: 0.002157629675166622
Epoch 1/10: Training Loss: 0.0014971067571336297
Epoch 2/10: Training Loss: 0.0016939349614890518
Epoch 3/10: Training Loss: 0.0016782963351838908
Epoch 4/10: Training Loss: 0.0012264283979014985
Epoch 5/10: Training Loss: 0.0010678852629509702
Epoch 6/10: Training Loss: 0.0008253150494994631
Epoch 7/10: Training Loss: 0.0007470753162529817
Epoch 8/10: Training Loss: 0.0006249230473664156
Epoch 9/10: Training Loss: 0.0005340129611598458
Epoch 0/10: Training Loss: 0.001314679617123888
Epoch 1/10: Training Loss: 0.0006912981240165155
Epoch 2/10: Training Loss: 0.0006143857311728774
Epoch 3/10: Training Loss: 0.0005441518216733112
Epoch 4/10: Training Loss: 0.0004922539390475545
Epoch 5/10: Training Loss: 0.00028656837561272626
Epoch 6/10: Training Loss: 0.0007281297484770516
Epoch 7/10: Training Loss: 0.00042014850291195293
Epoch 8/10: Training Loss: 0.0005978082880279086
Epoch 9/10: Training Loss: 0.0006681551404346693
Epoch 0/10: Training Loss: 0.0011850256990912735
Epoch 1/10: Training Loss: 0.0008599646636192372
Epoch 2/10: Training Loss: 0.0009825643127327724
Epoch 3/10: Training Loss: 0.0005891502692999429
Epoch 4/10: Training Loss: 0.0008191180643656396
Epoch 5/10: Training Loss: 0.0007219395988824352
Epoch 6/10: Training Loss: 0.0005660837651878004
Epoch 7/10: Training Loss: 0.0003793252422320132
Epoch 8/10: Training Loss: 0.0006379358322415131
Epoch 9/10: Training Loss: 0.00032697534126951207
Epoch 0/10: Training Loss: 0.0013318194064083479
Epoch 1/10: Training Loss: 0.0007325492256524547
Epoch 2/10: Training Loss: 0.0009787290893643107
Epoch 3/10: Training Loss: 0.001069085586149961
Epoch 4/10: Training Loss: 0.0009271462034705459
Epoch 5/10: Training Loss: 0.0008704035684762412
Epoch 6/10: Training Loss: 0.0007004163024441296
Epoch 7/10: Training Loss: 0.0009039920686886011
Epoch 8/10: Training Loss: 0.0007203897794350883
Epoch 9/10: Training Loss: 0.0007822513580322266
Epoch 0/10: Training Loss: 0.0002395493581014521
Epoch 1/10: Training Loss: 0.00036097381483106054
Epoch 2/10: Training Loss: 0.00021858721533242394
Epoch 3/10: Training Loss: 0.00030720696291502785
Epoch 4/10: Training Loss: 7.905502091435825e-05
Epoch 5/10: Training Loss: 0.0002779850407558329
Epoch 6/10: Training Loss: 0.0005768315318752737
Epoch 7/10: Training Loss: 0.001143297114792992
Epoch 8/10: Training Loss: 0.0004967675051268409
Epoch 9/10: Training Loss: 8.752944276613347e-05
Epoch 0/10: Training Loss: 0.0001906721688368741
Epoch 1/10: Training Loss: 0.00028254608897601856
Epoch 2/10: Training Loss: 0.0004997507175978492
Epoch 3/10: Training Loss: 0.00020936365951510036
Epoch 4/10: Training Loss: 0.0012042504023103153
Epoch 5/10: Training Loss: 0.00039831956519800077
Epoch 6/10: Training Loss: 0.0007914988433613497
Epoch 7/10: Training Loss: 0.000582020466818529
Epoch 8/10: Training Loss: 0.00014158773290760376
Epoch 9/10: Training Loss: 0.000174956297611489
Epoch 0/10: Training Loss: 0.00021110518890268663
Epoch 1/10: Training Loss: 0.00022604577243328093
Epoch 2/10: Training Loss: 0.0004720936803256764
Epoch 3/10: Training Loss: 0.00021725958761046915
Epoch 4/10: Training Loss: 0.000539056387017755
Epoch 5/10: Training Loss: 0.00040857739308301143
Epoch 6/10: Training Loss: 0.0003593308741555494
Epoch 7/10: Training Loss: 9.064219125053462e-05
Epoch 8/10: Training Loss: 0.0005258000510580399
Epoch 9/10: Training Loss: 0.0006354493253371295
dataset: capitals layer_num_from_end:-17 Avg_acc:tensor(0.9401) Avg_AUC:0.9830724388972624 Avg_threshold:0.5473689635594686
dataset: inventions layer_num_from_end:-17 Avg_acc:tensor(0.8672) Avg_AUC:0.9437780730387232 Avg_threshold:0.5821878015995026
dataset: elements layer_num_from_end:-17 Avg_acc:tensor(0.7419) Avg_AUC:0.8358037538058349 Avg_threshold:0.4883097807566325
dataset: animals layer_num_from_end:-17 Avg_acc:tensor(0.7560) Avg_AUC:0.8469020324179054 Avg_threshold:0.5468711654345194
dataset: companies layer_num_from_end:-17 Avg_acc:tensor(0.8678) Avg_AUC:0.9364546296296297 Avg_threshold:0.5013363560040792
dataset: facts layer_num_from_end:-17 Avg_acc:tensor(0.7249) Avg_AUC:0.8276143790849674 Avg_threshold:0.8066153724988302


