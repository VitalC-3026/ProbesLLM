2024-04-24 00:55:12.456529: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-04-24 00:55:13.481484: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
================layer -9================
Epoch 0/10: Training Loss: 0.0018335159841950956
Epoch 1/10: Training Loss: 0.0018639228977523484
Epoch 2/10: Training Loss: 0.0013793220261593799
Epoch 3/10: Training Loss: 0.0022071622885190523
Epoch 4/10: Training Loss: 0.0020364620885648926
Epoch 5/10: Training Loss: 0.001627631537564151
Epoch 6/10: Training Loss: 0.001970000408746146
Epoch 7/10: Training Loss: 0.0018617131493308327
Epoch 8/10: Training Loss: 0.0012544235149463574
Epoch 9/10: Training Loss: 0.000761593607338992
Epoch 0/10: Training Loss: 0.0020531551404432817
Epoch 1/10: Training Loss: 0.002489574930884621
Epoch 2/10: Training Loss: 0.001150368185310097
Epoch 3/10: Training Loss: 0.0027086888993536674
Epoch 4/10: Training Loss: 0.002192277799953114
Epoch 5/10: Training Loss: 0.0011312993881585714
Epoch 6/10: Training Loss: 0.0018350286083621578
Epoch 7/10: Training Loss: 0.001945494355021657
Epoch 8/10: Training Loss: 0.002232548448589298
Epoch 9/10: Training Loss: 0.001713615614217478
Epoch 0/10: Training Loss: 0.0021219734962169942
Epoch 1/10: Training Loss: 0.0018781071359461005
Epoch 2/10: Training Loss: 0.002104688149232131
Epoch 3/10: Training Loss: 0.0022571191087469356
Epoch 4/10: Training Loss: 0.001798556192771538
Epoch 5/10: Training Loss: 0.002126771461713564
Epoch 6/10: Training Loss: 0.0011394029105459894
Epoch 7/10: Training Loss: 0.001591197156405949
Epoch 8/10: Training Loss: 0.0015210281093637427
Epoch 9/10: Training Loss: 0.0015335872873559698
Epoch 0/10: Training Loss: 0.0023689644834015266
Epoch 1/10: Training Loss: 0.0018198919808206382
Epoch 2/10: Training Loss: 0.002107600310097443
Epoch 3/10: Training Loss: 0.002279318183477671
Epoch 4/10: Training Loss: 0.0014061967105221894
Epoch 5/10: Training Loss: 0.0011107325553894043
Epoch 6/10: Training Loss: 0.0008239278946917481
Epoch 7/10: Training Loss: 0.001158518293883903
Epoch 8/10: Training Loss: 0.0010872054502276554
Epoch 9/10: Training Loss: 0.001351614031323626
Epoch 0/10: Training Loss: 0.002189236367407021
Epoch 1/10: Training Loss: 0.0017255409363588672
Epoch 2/10: Training Loss: 0.002236111763796192
Epoch 3/10: Training Loss: 0.0019522845013741335
Epoch 4/10: Training Loss: 0.001180288524715447
Epoch 5/10: Training Loss: 0.0007429881238498571
Epoch 6/10: Training Loss: 0.001265666693266184
Epoch 7/10: Training Loss: 0.0007616293850851937
Epoch 8/10: Training Loss: 0.0005404146536727624
Epoch 9/10: Training Loss: 0.0006220713714880446
Epoch 0/10: Training Loss: 0.0024355913232440595
Epoch 1/10: Training Loss: 0.0022152517836518085
Epoch 2/10: Training Loss: 0.0023467922868904163
Epoch 3/10: Training Loss: 0.001908039571317427
Epoch 4/10: Training Loss: 0.0018394978865524012
Epoch 5/10: Training Loss: 0.0011237931946303946
Epoch 6/10: Training Loss: 0.0009300493023878226
Epoch 7/10: Training Loss: 0.0009364528707200033
Epoch 8/10: Training Loss: 0.0009773570152879492
Epoch 9/10: Training Loss: 0.0008615962018264583
Epoch 0/10: Training Loss: 0.0009801420383155346
Epoch 1/10: Training Loss: 0.0005032060202211141
Epoch 2/10: Training Loss: 0.0009171218611299991
Epoch 3/10: Training Loss: 0.002239668928086758
Epoch 4/10: Training Loss: 0.000995380338281393
Epoch 5/10: Training Loss: 0.0016114616766572
Epoch 6/10: Training Loss: 0.0009421077556908131
Epoch 7/10: Training Loss: 0.0012118281796574593
Epoch 8/10: Training Loss: 0.0010130121372640133
Epoch 9/10: Training Loss: 0.0011647858656942845
Epoch 0/10: Training Loss: 0.0008255109190940857
Epoch 1/10: Training Loss: 0.0006889416836202145
Epoch 2/10: Training Loss: 0.0008517716079950333
Epoch 3/10: Training Loss: 0.0017581472173333168
Epoch 4/10: Training Loss: 0.000846876110881567
Epoch 5/10: Training Loss: 0.0010225019417703153
Epoch 6/10: Training Loss: 0.0009898379445075988
Epoch 7/10: Training Loss: 0.0009149541147053242
Epoch 8/10: Training Loss: 0.0016491079702973365
Epoch 9/10: Training Loss: 0.0007613071240484715
Epoch 0/10: Training Loss: 0.001153498888015747
Epoch 1/10: Training Loss: 0.0009608817286789417
Epoch 2/10: Training Loss: 0.0014801874756813049
Epoch 3/10: Training Loss: 0.0016963837668299675
Epoch 4/10: Training Loss: 0.001178149413317442
Epoch 5/10: Training Loss: 0.001779128797352314
Epoch 6/10: Training Loss: 0.0011478260159492493
Epoch 7/10: Training Loss: 0.0005107469856739044
Epoch 8/10: Training Loss: 0.0011371436528861522
Epoch 9/10: Training Loss: 0.0012432411313056946
Epoch 0/10: Training Loss: 0.0021785135102120176
Epoch 1/10: Training Loss: 0.0020746653247031436
Epoch 2/10: Training Loss: 0.0022866919541814526
Epoch 3/10: Training Loss: 0.002327863008353361
Epoch 4/10: Training Loss: 0.0018108796541857872
Epoch 5/10: Training Loss: 0.001842189556474139
Epoch 6/10: Training Loss: 0.0011699999783449113
Epoch 7/10: Training Loss: 0.0012203061087116315
Epoch 8/10: Training Loss: 0.0011165645092156282
Epoch 9/10: Training Loss: 0.001165636026175918
Epoch 0/10: Training Loss: 0.0023286836162494245
Epoch 1/10: Training Loss: 0.0011402640942555324
Epoch 2/10: Training Loss: 0.0015237207055851152
Epoch 3/10: Training Loss: 0.0015414250884086462
Epoch 4/10: Training Loss: 0.0017036396986360002
Epoch 5/10: Training Loss: 0.0017146217595240113
Epoch 6/10: Training Loss: 0.0021102462604547004
Epoch 7/10: Training Loss: 0.0014918947675425535
Epoch 8/10: Training Loss: 0.0014319636259868646
Epoch 9/10: Training Loss: 0.001726538132710062
Epoch 0/10: Training Loss: 0.0017167255756961313
Epoch 1/10: Training Loss: 0.0013000747770260854
Epoch 2/10: Training Loss: 0.0014168640990166149
Epoch 3/10: Training Loss: 0.0019664153171952362
Epoch 4/10: Training Loss: 0.001989148604642054
Epoch 5/10: Training Loss: 0.001023790164358297
Epoch 6/10: Training Loss: 0.0012133987086593726
Epoch 7/10: Training Loss: 0.0011996429437285017
Epoch 8/10: Training Loss: 0.0008707596997546542
Epoch 9/10: Training Loss: 0.000752684132308717
Epoch 0/10: Training Loss: 0.0025388926070257526
Epoch 1/10: Training Loss: 0.00211381201712501
Epoch 2/10: Training Loss: 0.0023472942658607533
Epoch 3/10: Training Loss: 0.00217939212622232
Epoch 4/10: Training Loss: 0.00165556479763511
Epoch 5/10: Training Loss: 0.0016888320840747152
Epoch 6/10: Training Loss: 0.001955194189059024
Epoch 7/10: Training Loss: 0.00191629919784748
Epoch 8/10: Training Loss: 0.0019114439850611402
Epoch 9/10: Training Loss: 0.0016641903002530534
Epoch 0/10: Training Loss: 0.002981713078669365
Epoch 1/10: Training Loss: 0.0023874747042624367
Epoch 2/10: Training Loss: 0.0023309027911811477
Epoch 3/10: Training Loss: 0.002580146521132513
Epoch 4/10: Training Loss: 0.002481184258366263
Epoch 5/10: Training Loss: 0.0021426969806090097
Epoch 6/10: Training Loss: 0.0019043056380669801
Epoch 7/10: Training Loss: 0.001880223782646735
Epoch 8/10: Training Loss: 0.0018586048622005033
Epoch 9/10: Training Loss: 0.0021223115999967055
Epoch 0/10: Training Loss: 0.0027105977993137787
Epoch 1/10: Training Loss: 0.0023840529634463078
Epoch 2/10: Training Loss: 0.0019465996729617087
Epoch 3/10: Training Loss: 0.002361815694152125
Epoch 4/10: Training Loss: 0.0017803396215502
Epoch 5/10: Training Loss: 0.00202007345016429
Epoch 6/10: Training Loss: 0.0023200044174068023
Epoch 7/10: Training Loss: 0.0014946100530245446
Epoch 8/10: Training Loss: 0.0019184862146314407
Epoch 9/10: Training Loss: 0.0015233338668646402
Epoch 0/10: Training Loss: 0.001078933389747844
Epoch 1/10: Training Loss: 0.0008321068742695977
Epoch 2/10: Training Loss: 0.0006993033868425033
Epoch 3/10: Training Loss: 0.0009409134002292857
Epoch 4/10: Training Loss: 0.0008695743539754082
Epoch 5/10: Training Loss: 0.000848522168748519
Epoch 6/10: Training Loss: 0.0010591349181006935
Epoch 7/10: Training Loss: 0.000895854304818546
Epoch 8/10: Training Loss: 0.0007491975146181443
Epoch 9/10: Training Loss: 0.0005693070590496064
Epoch 0/10: Training Loss: 0.0011623644653488607
Epoch 1/10: Training Loss: 0.000683104816605063
Epoch 2/10: Training Loss: 0.000790940663393806
Epoch 3/10: Training Loss: 0.0008642364950741039
Epoch 4/10: Training Loss: 0.0008814717916881337
Epoch 5/10: Training Loss: 0.0009184690959313336
Epoch 6/10: Training Loss: 0.0005097893669324762
Epoch 7/10: Training Loss: 0.001035693463157205
Epoch 8/10: Training Loss: 0.0006959965562119203
Epoch 9/10: Training Loss: 0.0010605664814219756
Epoch 0/10: Training Loss: 0.001367736651616938
Epoch 1/10: Training Loss: 0.0007323217304313884
Epoch 2/10: Training Loss: 0.0012048567042631261
Epoch 3/10: Training Loss: 0.0012362023486810572
Epoch 4/10: Training Loss: 0.000647652631296831
Epoch 5/10: Training Loss: 0.0006614204277010525
Epoch 6/10: Training Loss: 0.0008745695738231435
Epoch 7/10: Training Loss: 0.0011523787589634165
Epoch 8/10: Training Loss: 0.0010240396156030542
Epoch 9/10: Training Loss: 0.0005817776655449587
dataset: capitals layer_num_from_end:-9 Avg_acc:tensor(0.8203) Avg_AUC:0.9363936918679591 Avg_threshold:0.49949978788693744
dataset: inventions layer_num_from_end:-9 Avg_acc:tensor(0.6773) Avg_AUC:0.8103744699252315 Avg_threshold:0.44227418303489685
dataset: elements layer_num_from_end:-9 Avg_acc:tensor(0.5971) Avg_AUC:0.6634370062049563 Avg_threshold:0.47725696365038556
dataset: animals layer_num_from_end:-9 Avg_acc:tensor(0.5529) Avg_AUC:0.7015896636432349 Avg_threshold:0.49461959799130756
dataset: companies layer_num_from_end:-9 Avg_acc:tensor(0.7661) Avg_AUC:0.9052319444444445 Avg_threshold:0.4372588296731313
dataset: facts layer_num_from_end:-9 Avg_acc:tensor(0.6503) Avg_AUC:0.7351036638329987 Avg_threshold:0.7137264907360077


================layer -17================
Epoch 0/10: Training Loss: 0.0032489035096201864
Epoch 1/10: Training Loss: 0.0034875031951424125
Epoch 2/10: Training Loss: 0.002966141158884222
Epoch 3/10: Training Loss: 0.0030023015879250907
Epoch 4/10: Training Loss: 0.0031961936217087964
Epoch 5/10: Training Loss: 0.003501856660509443
Epoch 6/10: Training Loss: 0.0032579796714382572
Epoch 7/10: Training Loss: 0.002631419813716328
Epoch 8/10: Training Loss: 0.0031220994212410665
Epoch 9/10: Training Loss: 0.002397445115176114
Epoch 0/10: Training Loss: 0.004329004904606959
Epoch 1/10: Training Loss: 0.003996623145950424
Epoch 2/10: Training Loss: 0.003787447522570203
Epoch 3/10: Training Loss: 0.0035892262325420247
Epoch 4/10: Training Loss: 0.0028487400158301933
Epoch 5/10: Training Loss: 0.0030729347592467196
Epoch 6/10: Training Loss: 0.003100689474519316
Epoch 7/10: Training Loss: 0.0029403037958211833
Epoch 8/10: Training Loss: 0.0028015052105163363
Epoch 9/10: Training Loss: 0.0028715110742128813
Epoch 0/10: Training Loss: 0.004452263975476886
Epoch 1/10: Training Loss: 0.0035252429388619804
Epoch 2/10: Training Loss: 0.0030625283301293433
Epoch 3/10: Training Loss: 0.002805955760128848
Epoch 4/10: Training Loss: 0.0027311435946217786
Epoch 5/10: Training Loss: 0.002685237806160133
Epoch 6/10: Training Loss: 0.0027300748791727987
Epoch 7/10: Training Loss: 0.003072841392530428
Epoch 8/10: Training Loss: 0.0023373215348570496
Epoch 9/10: Training Loss: 0.0022820603180598547
Epoch 0/10: Training Loss: 0.0034331119864996224
Epoch 1/10: Training Loss: 0.0030666039399574138
Epoch 2/10: Training Loss: 0.002908763527138833
Epoch 3/10: Training Loss: 0.0022509355852208986
Epoch 4/10: Training Loss: 0.0018320429178834692
Epoch 5/10: Training Loss: 0.001579117921232446
Epoch 6/10: Training Loss: 0.0016663085463588224
Epoch 7/10: Training Loss: 0.00140047356760575
Epoch 8/10: Training Loss: 0.0009250831933109307
Epoch 9/10: Training Loss: 0.0021180709812538756
Epoch 0/10: Training Loss: 0.0036833758003141253
Epoch 1/10: Training Loss: 0.002793845040666545
Epoch 2/10: Training Loss: 0.0024860462893737606
Epoch 3/10: Training Loss: 0.0016246688512205347
Epoch 4/10: Training Loss: 0.0017348054362220997
Epoch 5/10: Training Loss: 0.0012028558305436116
Epoch 6/10: Training Loss: 0.0011686987123606396
Epoch 7/10: Training Loss: 0.0009318176039888815
Epoch 8/10: Training Loss: 0.0006785196669262611
Epoch 9/10: Training Loss: 0.0006377066662706481
Epoch 0/10: Training Loss: 0.003381698775145174
Epoch 1/10: Training Loss: 0.0032259675622717736
Epoch 2/10: Training Loss: 0.002973301096196555
Epoch 3/10: Training Loss: 0.002537776356094454
Epoch 4/10: Training Loss: 0.0019310866396851335
Epoch 5/10: Training Loss: 0.0012365701191264429
Epoch 6/10: Training Loss: 0.0008270469911259376
Epoch 7/10: Training Loss: 0.0006220765366144707
Epoch 8/10: Training Loss: 0.00029696201894180907
Epoch 9/10: Training Loss: 0.0006985960562536322
Epoch 0/10: Training Loss: 0.0018360987305641174
Epoch 1/10: Training Loss: 0.0015399952419102192
Epoch 2/10: Training Loss: 0.0014726735651493072
Epoch 3/10: Training Loss: 0.0014542967081069946
Epoch 4/10: Training Loss: 0.00144217973574996
Epoch 5/10: Training Loss: 0.0014324070885777473
Epoch 6/10: Training Loss: 0.0013754135929048062
Epoch 7/10: Training Loss: 0.0012118851765990258
Epoch 8/10: Training Loss: 0.0012562562711536883
Epoch 9/10: Training Loss: 0.0012351899407804013
Epoch 0/10: Training Loss: 0.0018531931564211845
Epoch 1/10: Training Loss: 0.0007594633381813765
Epoch 2/10: Training Loss: 0.0022923963144421576
Epoch 3/10: Training Loss: 0.001790071651339531
Epoch 4/10: Training Loss: 0.0014937675558030605
Epoch 5/10: Training Loss: 0.0013985387049615383
Epoch 6/10: Training Loss: 0.0010831602849066257
Epoch 7/10: Training Loss: 0.0009437692351639271
Epoch 8/10: Training Loss: 0.0010965931229293346
Epoch 9/10: Training Loss: 0.0010230349376797675
Epoch 0/10: Training Loss: 0.0011704965494573117
Epoch 1/10: Training Loss: 0.0013862228952348232
Epoch 2/10: Training Loss: 0.0007636913098394871
Epoch 3/10: Training Loss: 0.0017362968996167184
Epoch 4/10: Training Loss: 0.0011429399251937865
Epoch 5/10: Training Loss: 0.0013990129344165325
Epoch 6/10: Training Loss: 0.0014025366865098477
Epoch 7/10: Training Loss: 0.0011316074058413505
Epoch 8/10: Training Loss: 0.0010064312256872654
Epoch 9/10: Training Loss: 0.0007765769492834807
Epoch 0/10: Training Loss: 0.0021382978387699007
Epoch 1/10: Training Loss: 0.0019830586803946527
Epoch 2/10: Training Loss: 0.0008466289301586759
Epoch 3/10: Training Loss: 0.0029713934773852113
Epoch 4/10: Training Loss: 0.003480751043672015
Epoch 5/10: Training Loss: 0.0022556544489161982
Epoch 6/10: Training Loss: 0.0019833026038613287
Epoch 7/10: Training Loss: 0.0021672649368359026
Epoch 8/10: Training Loss: 0.0018519780058769664
Epoch 9/10: Training Loss: 0.0017933735422268036
Epoch 0/10: Training Loss: 0.0027996639537203843
Epoch 1/10: Training Loss: 0.0015453703843863907
Epoch 2/10: Training Loss: 0.0011770633185745046
Epoch 3/10: Training Loss: 0.0014208218284473297
Epoch 4/10: Training Loss: 0.0018798572242639626
Epoch 5/10: Training Loss: 0.0021992192906179248
Epoch 6/10: Training Loss: 0.0014159991672843884
Epoch 7/10: Training Loss: 0.0019349943680368412
Epoch 8/10: Training Loss: 0.0015485670156539625
Epoch 9/10: Training Loss: 0.0020282625392743737
Epoch 0/10: Training Loss: 0.002369369481019913
Epoch 1/10: Training Loss: 0.0016297625887925458
Epoch 2/10: Training Loss: 0.0009682543907955194
Epoch 3/10: Training Loss: 0.0010660286921604424
Epoch 4/10: Training Loss: 0.0012407664470611864
Epoch 5/10: Training Loss: 0.0018382421724355905
Epoch 6/10: Training Loss: 0.0017017084322157939
Epoch 7/10: Training Loss: 0.0009131314838008516
Epoch 8/10: Training Loss: 0.0015596014678857888
Epoch 9/10: Training Loss: 0.0018554010968299427
Epoch 0/10: Training Loss: 0.0025114533522271163
Epoch 1/10: Training Loss: 0.0023881992362192925
Epoch 2/10: Training Loss: 0.0015875332994966318
Epoch 3/10: Training Loss: 0.0013270672188689376
Epoch 4/10: Training Loss: 0.002652115774470449
Epoch 5/10: Training Loss: 0.002754856813822361
Epoch 6/10: Training Loss: 0.00271640352855455
Epoch 7/10: Training Loss: 0.0022875500830593486
Epoch 8/10: Training Loss: 0.002345426193136253
Epoch 9/10: Training Loss: 0.0020970629145767514
Epoch 0/10: Training Loss: 0.002596942004778527
Epoch 1/10: Training Loss: 0.0020635386571189428
Epoch 2/10: Training Loss: 0.0024211136315832075
Epoch 3/10: Training Loss: 0.0017392491268006381
Epoch 4/10: Training Loss: 0.0021105349458606037
Epoch 5/10: Training Loss: 0.0018017347285289639
Epoch 6/10: Training Loss: 0.0027469595931223687
Epoch 7/10: Training Loss: 0.002651434860482121
Epoch 8/10: Training Loss: 0.0026234175195757126
Epoch 9/10: Training Loss: 0.002406986343939573
Epoch 0/10: Training Loss: 0.0028847886237087627
Epoch 1/10: Training Loss: 0.002570018863046406
Epoch 2/10: Training Loss: 0.0018345164936899349
Epoch 3/10: Training Loss: 0.002063887206134417
Epoch 4/10: Training Loss: 0.0028619395186569516
Epoch 5/10: Training Loss: 0.0024728062531805986
Epoch 6/10: Training Loss: 0.0024861951932212377
Epoch 7/10: Training Loss: 0.002738813493425483
Epoch 8/10: Training Loss: 0.002494090242891122
Epoch 9/10: Training Loss: 0.002596319905969481
Epoch 0/10: Training Loss: 0.0009739370030515334
Epoch 1/10: Training Loss: 0.00048662630074164446
Epoch 2/10: Training Loss: 0.0002964925897472045
Epoch 3/10: Training Loss: 0.0001886967131320168
Epoch 4/10: Training Loss: 0.0004904770237558028
Epoch 5/10: Training Loss: 0.00045562645091730007
Epoch 6/10: Training Loss: 0.0004346902317860547
Epoch 7/10: Training Loss: 0.0005006361095344319
Epoch 8/10: Training Loss: 0.0002542035763754564
Epoch 9/10: Training Loss: 0.00044062851982958176
Epoch 0/10: Training Loss: 0.0008362984832595376
Epoch 1/10: Training Loss: 0.0005966307924074285
Epoch 2/10: Training Loss: 0.0002708990565117668
Epoch 3/10: Training Loss: 9.183440467014032e-05
Epoch 4/10: Training Loss: 0.00041254601057837993
Epoch 5/10: Training Loss: 0.00041768726180581485
Epoch 6/10: Training Loss: 0.0006062297698329477
Epoch 7/10: Training Loss: 0.00023170667974387898
Epoch 8/10: Training Loss: 0.00017151283648084192
Epoch 9/10: Training Loss: 0.00012742575696286033
Epoch 0/10: Training Loss: 0.000578303196850945
Epoch 1/10: Training Loss: 0.00040063169949194964
Epoch 2/10: Training Loss: 0.0001693315804004669
Epoch 3/10: Training Loss: 0.00047304994043181924
Epoch 4/10: Training Loss: 0.0001445266801644774
Epoch 5/10: Training Loss: 0.00020614389111013975
Epoch 6/10: Training Loss: 0.0007212884724140167
Epoch 7/10: Training Loss: 0.00015565021073116975
Epoch 8/10: Training Loss: 0.000581112372524598
Epoch 9/10: Training Loss: 0.0003494529382270925
dataset: capitals layer_num_from_end:-17 Avg_acc:tensor(0.7211) Avg_AUC:0.8198351275118028 Avg_threshold:0.5153524378935496
dataset: inventions layer_num_from_end:-17 Avg_acc:tensor(0.6123) Avg_AUC:0.7276374707063944 Avg_threshold:0.8859071135520935
dataset: elements layer_num_from_end:-17 Avg_acc:tensor(0.5513) Avg_AUC:0.6075769838516977 Avg_threshold:0.500551849603653
dataset: animals layer_num_from_end:-17 Avg_acc:tensor(0.5602) Avg_AUC:0.618585049760645 Avg_threshold:0.46126771966616315
dataset: companies layer_num_from_end:-17 Avg_acc:tensor(0.6722) Avg_AUC:0.8357194444444446 Avg_threshold:0.7718469003836314
dataset: facts layer_num_from_end:-17 Avg_acc:tensor(0.5436) Avg_AUC:0.6185459296282058 Avg_threshold:0.924306849638621


