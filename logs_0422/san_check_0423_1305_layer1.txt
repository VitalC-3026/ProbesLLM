2024-04-23 17:05:26.143759: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-04-23 17:05:27.208502: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Epoch 0/100: Training Loss: 0.0030970752656042993
Epoch 1/100: Training Loss: 0.0015913533580886735
Epoch 2/100: Training Loss: 0.002796590119808704
Epoch 3/100: Training Loss: 0.0022461320553626212
Epoch 4/100: Training Loss: 0.0023234096857217643
Epoch 5/100: Training Loss: 0.0019158312490770034
Epoch 6/100: Training Loss: 0.001306870287948555
Epoch 7/100: Training Loss: 0.0009743318065896735
Epoch 8/100: Training Loss: 0.00044700435616753316
Epoch 9/100: Training Loss: 0.0014478016149747622
Epoch 10/100: Training Loss: 0.0019928473275858207
Epoch 11/100: Training Loss: 0.000859210384892417
Epoch 12/100: Training Loss: 0.00202234224839644
Epoch 13/100: Training Loss: 0.000947286928450311
Epoch 14/100: Training Loss: 0.0012504978405012118
Epoch 15/100: Training Loss: 0.0014158958321684725
Epoch 16/100: Training Loss: 0.000998559740039852
Epoch 17/100: Training Loss: 0.0008455155404297622
Epoch 18/100: Training Loss: 0.0009746601531555603
Epoch 19/100: Training Loss: 0.0010899979543019008
Epoch 20/100: Training Loss: 0.0010191120884635232
Epoch 21/100: Training Loss: 0.00037602126806766
Epoch 22/100: Training Loss: 0.0009631524969647814
Epoch 23/100: Training Loss: 0.0007966821427111859
Epoch 24/100: Training Loss: 0.0009387563783805687
Epoch 25/100: Training Loss: 0.001470609769954548
Epoch 26/100: Training Loss: 0.0006771727979600013
Epoch 27/100: Training Loss: 0.0012472030166145805
Epoch 28/100: Training Loss: 0.000741883070318849
Epoch 29/100: Training Loss: 0.0008554098593605148
Epoch 30/100: Training Loss: 0.0010739526340177842
Epoch 31/100: Training Loss: 0.0007744831117716702
Epoch 32/100: Training Loss: 0.0008219907229596919
Epoch 33/100: Training Loss: 0.0008824853213516982
Epoch 34/100: Training Loss: 0.0007228935723538165
Epoch 35/100: Training Loss: 0.0006949473823700752
Epoch 36/100: Training Loss: 0.0007807644722344992
Epoch 37/100: Training Loss: 0.0008097841710477442
Epoch 38/100: Training Loss: 0.0002931324647856759
Epoch 39/100: Training Loss: 0.0009475843264506414
Epoch 40/100: Training Loss: 0.000820439387034703
Epoch 41/100: Training Loss: 0.001051401654323498
Epoch 42/100: Training Loss: 0.0007338412366546951
Epoch 43/100: Training Loss: 0.00021947568634173253
Epoch 44/100: Training Loss: 0.0008853504082539698
Epoch 45/100: Training Loss: 0.0007438771166167892
Epoch 46/100: Training Loss: 0.0009181777705679407
Epoch 47/100: Training Loss: 0.0005451701320968307
Epoch 48/100: Training Loss: 0.001038884678920666
Epoch 49/100: Training Loss: 0.00033085999059510396
Epoch 50/100: Training Loss: 0.0006674085776289026
Epoch 51/100: Training Loss: 0.0005512342794791802
Epoch 52/100: Training Loss: 0.0004069754606360322
Epoch 53/100: Training Loss: 0.0006514499550099139
Epoch 54/100: Training Loss: 0.0006358430206358849
Epoch 55/100: Training Loss: 0.0005499765381112799
Epoch 56/100: Training Loss: 0.0008293959748494875
Epoch 57/100: Training Loss: 0.0003170848570086739
Epoch 58/100: Training Loss: 0.000373198722417538
Epoch 59/100: Training Loss: 0.0009030507994698478
Epoch 60/100: Training Loss: 0.0005641618704462385
Epoch 61/100: Training Loss: 0.0004630852620918434
Epoch 62/100: Training Loss: 0.0008097963108049406
Epoch 63/100: Training Loss: 0.0013551584907345005
Epoch 64/100: Training Loss: 0.0004923104510440693
Epoch 65/100: Training Loss: 0.0006456732541531116
Epoch 66/100: Training Loss: 0.0007181706336828378
Epoch 67/100: Training Loss: 0.0004551278336064799
Epoch 68/100: Training Loss: 0.0005841587077487599
Epoch 69/100: Training Loss: 0.0007626717011411707
Epoch 70/100: Training Loss: 0.0007921382263823823
Epoch 71/100: Training Loss: 0.000780670480294661
Epoch 72/100: Training Loss: 0.0006958718274856781
Epoch 73/100: Training Loss: 0.0007586074354765298
Epoch 74/100: Training Loss: 0.0007690765953564144
Epoch 75/100: Training Loss: 0.0006831962016078975
Epoch 76/100: Training Loss: 0.0008609456928459914
Epoch 77/100: Training Loss: 0.0007937321296105018
Epoch 78/100: Training Loss: 0.0007369727208897784
Epoch 79/100: Training Loss: 0.0007924611022422363
Epoch 80/100: Training Loss: 2.609956293151929e-05
Epoch 81/100: Training Loss: 0.00041601575025311717
Epoch 82/100: Training Loss: 0.00024874813177368856
Epoch 83/100: Training Loss: 4.796417742654994e-05
Epoch 84/100: Training Loss: 9.911963911531689e-05
Epoch 85/100: Training Loss: 0.0007241136961049966
Epoch 86/100: Training Loss: 0.0007043598623542519
Epoch 87/100: Training Loss: 0.0007901412102725956
Epoch 88/100: Training Loss: 0.0009665118230806364
Epoch 89/100: Training Loss: 0.0008849403658113279
Epoch 90/100: Training Loss: 0.0007174968511074573
Epoch 91/100: Training Loss: 0.0006790729565220279
Epoch 92/100: Training Loss: 0.00016369020657522695
Epoch 93/100: Training Loss: 0.0006811088927975901
Epoch 94/100: Training Loss: 0.00016122072987206332
Epoch 95/100: Training Loss: 0.0004863147656400721
Epoch 96/100: Training Loss: 0.0002024891831866511
Epoch 97/100: Training Loss: 0.0004293855566244859
Epoch 98/100: Training Loss: 0.0007043861738451711
Epoch 99/100: Training Loss: 0.000744961254246585
Epoch 0/100: Training Loss: 0.0036984507020536837
Epoch 1/100: Training Loss: 0.0022416337803527193
Epoch 2/100: Training Loss: 0.0013304776870287382
Epoch 3/100: Training Loss: 0.0020681584214830733
Epoch 4/100: Training Loss: 0.0014583973826228322
Epoch 5/100: Training Loss: 0.0008308669695487389
Epoch 6/100: Training Loss: 0.0018833723518398258
Epoch 7/100: Training Loss: 0.0006554372139743992
Epoch 8/100: Training Loss: 0.0008734345331892267
Epoch 9/100: Training Loss: 0.001552023775094039
Epoch 10/100: Training Loss: 0.0012949852051434816
Epoch 11/100: Training Loss: 0.0008334946278091911
Epoch 12/100: Training Loss: 0.0008187852018362992
Epoch 13/100: Training Loss: 0.0015686810850263474
Epoch 14/100: Training Loss: 0.0007376811512700328
Epoch 15/100: Training Loss: 0.0007584995323127799
Epoch 16/100: Training Loss: 0.0012566659834001449
Epoch 17/100: Training Loss: 0.0011023028747185128
Epoch 18/100: Training Loss: 0.0011192063143203308
Epoch 19/100: Training Loss: 0.001748668980765176
Epoch 20/100: Training Loss: 0.000668355791301994
Epoch 21/100: Training Loss: 0.0017122928496007318
Epoch 22/100: Training Loss: 0.00033495340730760483
Epoch 23/100: Training Loss: 0.0007185265942886992
Epoch 24/100: Training Loss: 0.0002730238948251818
Epoch 25/100: Training Loss: 0.0008138800105014881
Epoch 26/100: Training Loss: 0.0007096325288285742
Epoch 27/100: Training Loss: 0.0006309125077474367
Epoch 28/100: Training Loss: 0.0006634673767990165
Epoch 29/100: Training Loss: 0.0009560597526443588
Epoch 30/100: Training Loss: 0.0011535743316570361
Epoch 31/100: Training Loss: 0.0008423501899192383
Epoch 32/100: Training Loss: 0.0008061950231765533
Epoch 33/100: Training Loss: 0.001157423207809875
Epoch 34/100: Training Loss: 0.0003442078880913608
Epoch 35/100: Training Loss: 0.0003524795956128127
Epoch 36/100: Training Loss: 0.00042875637332876246
Epoch 37/100: Training Loss: 0.00045855572590461146
Epoch 38/100: Training Loss: 0.0005628379074843614
Epoch 39/100: Training Loss: 0.0006950286093291703
Epoch 40/100: Training Loss: 0.0008416360283231402
Epoch 41/100: Training Loss: 0.000583894915514059
Epoch 42/100: Training Loss: 0.0008107760882044172
Epoch 43/100: Training Loss: 0.00044481602165248846
Epoch 44/100: Training Loss: 0.0007015307258059095
Epoch 45/100: Training Loss: 0.0007755503162637457
Epoch 46/100: Training Loss: 0.0006134064568506254
Epoch 47/100: Training Loss: 0.0008753624620971147
Epoch 48/100: Training Loss: 0.0006034464894474803
Epoch 49/100: Training Loss: 0.0006514885104619539
Epoch 50/100: Training Loss: 0.0004198052041180484
Epoch 51/100: Training Loss: 0.0004546199957807581
Epoch 52/100: Training Loss: 0.0005252841156679433
Epoch 53/100: Training Loss: 0.0007752622445146521
Epoch 54/100: Training Loss: 0.00043436903115752696
Epoch 55/100: Training Loss: 0.00037119571458209646
Epoch 56/100: Training Loss: 0.0006432634341966856
Epoch 57/100: Training Loss: 0.00028945419650811417
Epoch 58/100: Training Loss: 0.00018391884550764844
Epoch 59/100: Training Loss: 0.0002569507520932418
Epoch 60/100: Training Loss: 0.00028170887511093297
Epoch 61/100: Training Loss: 0.0002806198138457078
Epoch 62/100: Training Loss: 0.000601017079153261
Epoch 63/100: Training Loss: 0.0004732980282156618
Epoch 64/100: Training Loss: 0.0004798352614149347
Epoch 65/100: Training Loss: 0.0002767968063171093
Epoch 66/100: Training Loss: 0.0008289024650633751
Epoch 67/100: Training Loss: 0.00041647974427763397
Epoch 68/100: Training Loss: 0.00030204192637563583
Epoch 69/100: Training Loss: 0.00034597734888116796
Epoch 70/100: Training Loss: 0.0007516419762498015
Epoch 71/100: Training Loss: 0.0001775654019800933
Epoch 72/100: Training Loss: 0.0005380734280272798
Epoch 73/100: Training Loss: 0.0002453275337502673
Epoch 74/100: Training Loss: 0.00033938991544129964
Epoch 75/100: Training Loss: 0.0003643994304266843
Epoch 76/100: Training Loss: 0.00018556806799415108
Epoch 77/100: Training Loss: 0.0004350245519951507
Epoch 78/100: Training Loss: 0.0006584486552885363
Epoch 79/100: Training Loss: 0.0008193233109020687
Epoch 80/100: Training Loss: 0.0007700120339860449
Epoch 81/100: Training Loss: 0.0006691728542734693
Epoch 82/100: Training Loss: 8.2238915969025e-05
Epoch 83/100: Training Loss: 0.0006770670309767023
Epoch 84/100: Training Loss: 0.0006531298785776525
Epoch 85/100: Training Loss: 0.00011673102064149363
Epoch 86/100: Training Loss: 0.00039569192103572657
Epoch 87/100: Training Loss: 0.0006147373493734773
Epoch 88/100: Training Loss: 0.000332476427921882
Epoch 89/100: Training Loss: 0.00026986490075404826
Epoch 90/100: Training Loss: 0.0008097287345599461
Epoch 91/100: Training Loss: 0.0005620274093601253
Epoch 92/100: Training Loss: 0.0004898722876201977
Epoch 93/100: Training Loss: 0.0005695985017956553
Epoch 94/100: Training Loss: 0.0007108100852766237
Epoch 95/100: Training Loss: 0.0007001451262227305
Epoch 96/100: Training Loss: 0.0004192252832276004
Epoch 97/100: Training Loss: 0.00037158212878487325
Epoch 98/100: Training Loss: 0.0007306594740260731
Epoch 99/100: Training Loss: 5.648923399565103e-05
Epoch 0/100: Training Loss: 0.002538003913172475
Epoch 1/100: Training Loss: 0.0020347162560149508
Epoch 2/100: Training Loss: 0.0012765384012168937
Epoch 3/100: Training Loss: 0.0019148494397010004
Epoch 4/100: Training Loss: 0.001161444854069423
Epoch 5/100: Training Loss: 0.0013616797807333354
Epoch 6/100: Training Loss: 0.0011817470088705316
Epoch 7/100: Training Loss: 0.001498799432407726
Epoch 8/100: Training Loss: 0.0014926805779650494
Epoch 9/100: Training Loss: 0.001123332060300387
Epoch 10/100: Training Loss: 0.0005250692992777257
Epoch 11/100: Training Loss: 0.001578413939976192
Epoch 12/100: Training Loss: 0.0008970985671023389
Epoch 13/100: Training Loss: 0.001130780348410973
Epoch 14/100: Training Loss: 0.0006345211938544587
Epoch 15/100: Training Loss: 0.0013056372429107452
Epoch 16/100: Training Loss: 0.0011480760115843553
Epoch 17/100: Training Loss: 0.000243081001343427
Epoch 18/100: Training Loss: 0.0015111791712420803
Epoch 19/100: Training Loss: 0.0011057731780138884
Epoch 20/100: Training Loss: 0.0001850198511477117
Epoch 21/100: Training Loss: 0.0018715354112478404
Epoch 22/100: Training Loss: 0.0008406249897463338
Epoch 23/100: Training Loss: 0.0009618463007720201
Epoch 24/100: Training Loss: 0.0008159314210598285
Epoch 25/100: Training Loss: 0.0009247115650377073
Epoch 26/100: Training Loss: 0.000717191742016719
Epoch 27/100: Training Loss: 0.00109607314730024
Epoch 28/100: Training Loss: 0.0007499365748225393
Epoch 29/100: Training Loss: 0.0007861786476382009
Epoch 30/100: Training Loss: 0.0005260130221193487
Epoch 31/100: Training Loss: 0.0006945359852764156
Epoch 32/100: Training Loss: 0.0008681912522215943
Epoch 33/100: Training Loss: 0.0007018683465210708
Epoch 34/100: Training Loss: 0.00024039170646167303
Epoch 35/100: Training Loss: 0.00031520363750991287
Epoch 36/100: Training Loss: 0.0007792492429693262
Epoch 37/100: Training Loss: 0.0006866294083061751
Epoch 38/100: Training Loss: 0.0010520971113151603
Epoch 39/100: Training Loss: 0.0007883830816595704
Epoch 40/100: Training Loss: 0.0007159681274340703
Epoch 41/100: Training Loss: 0.0006448803664921047
Epoch 42/100: Training Loss: 0.0007905360910442326
Epoch 43/100: Training Loss: 0.0011699863455512307
Epoch 44/100: Training Loss: 0.0007981776774346412
Epoch 45/100: Training Loss: 0.0010104592029864972
Epoch 46/100: Training Loss: 0.0011103669038185706
Epoch 47/100: Training Loss: 0.0008464592632713851
Epoch 48/100: Training Loss: 0.0010268417271700773
Epoch 49/100: Training Loss: 0.0006039925180115067
Epoch 50/100: Training Loss: 0.0007767930001645655
Epoch 51/100: Training Loss: 0.0007029658222531939
Epoch 52/100: Training Loss: 0.0008637641484920795
Epoch 53/100: Training Loss: 0.0021709620535790503
Epoch 54/100: Training Loss: 0.0011094588708210659
Epoch 55/100: Training Loss: 0.00026119078372741913
Epoch 56/100: Training Loss: 0.00034850911348016113
Epoch 57/100: Training Loss: 0.00039380820064277913
Epoch 58/100: Training Loss: 0.00021063789100080104
Epoch 59/100: Training Loss: 0.0007435524171882577
Epoch 60/100: Training Loss: 0.0006419673978865563
Epoch 61/100: Training Loss: 0.0008167700021416991
Epoch 62/100: Training Loss: 0.00021118167918045204
Epoch 63/100: Training Loss: 0.0008431712647418042
Epoch 64/100: Training Loss: 0.0009378541808028321
Epoch 65/100: Training Loss: 0.00030863962390206075
Epoch 66/100: Training Loss: 0.0007852756685310311
Epoch 67/100: Training Loss: 0.0009453341469064459
Epoch 68/100: Training Loss: 0.0008886315814265004
Epoch 69/100: Training Loss: 0.0009220327947523211
Epoch 70/100: Training Loss: 0.0009592826758231316
Epoch 71/100: Training Loss: 0.0009022528058165437
Epoch 72/100: Training Loss: 0.0006047979101434454
Epoch 73/100: Training Loss: 0.0006718228642757123
Epoch 74/100: Training Loss: 0.0005994004594696152
Epoch 75/100: Training Loss: 0.0003301838894824048
Epoch 76/100: Training Loss: 0.0007589219229204671
Epoch 77/100: Training Loss: 0.0003065092528199816
Epoch 78/100: Training Loss: 0.0008472867987372658
Epoch 79/100: Training Loss: 0.0008428582882547712
Epoch 80/100: Training Loss: 0.0009364829613612249
Epoch 81/100: Training Loss: 0.0008254198657049166
Epoch 82/100: Training Loss: 0.0009548735993725436
Epoch 83/100: Training Loss: 0.0008993563535330179
Epoch 84/100: Training Loss: 0.0006713906263971662
Epoch 85/100: Training Loss: 0.0008497001095251603
Epoch 86/100: Training Loss: 0.0005361921824775375
Epoch 87/100: Training Loss: 0.0005792721168144599
Epoch 88/100: Training Loss: 0.000642904295371129
Epoch 89/100: Training Loss: 0.000541213821697902
Epoch 90/100: Training Loss: 0.0004186391361526676
Epoch 91/100: Training Loss: 0.00030932218565807475
Epoch 92/100: Training Loss: 0.0007065462690966946
Epoch 93/100: Training Loss: 0.0008629577664228586
Epoch 94/100: Training Loss: 0.0007760853513137444
Epoch 95/100: Training Loss: 0.0007660870368664081
Epoch 96/100: Training Loss: 0.0008518799514203639
Epoch 97/100: Training Loss: 0.0007726341173365399
Epoch 98/100: Training Loss: 0.0001677669767107997
Epoch 99/100: Training Loss: 0.0008217355796507189
Epoch 0/100: Training Loss: 0.0020092460641100364
Epoch 1/100: Training Loss: 0.0017473154638442524
Epoch 2/100: Training Loss: 0.0007233828123361786
Epoch 3/100: Training Loss: 0.0022142211352389283
Epoch 4/100: Training Loss: 0.0007966731223591997
Epoch 5/100: Training Loss: 0.0010021585811135227
Epoch 6/100: Training Loss: 0.0011414256563947245
Epoch 7/100: Training Loss: 0.0008145943741125563
Epoch 8/100: Training Loss: 0.00042652736412235564
Epoch 9/100: Training Loss: 0.000925291535313144
Epoch 10/100: Training Loss: 0.0011446117623452028
Epoch 11/100: Training Loss: 0.0011576674650051843
Epoch 12/100: Training Loss: 0.001404857342959913
Epoch 13/100: Training Loss: 0.0013186664669060269
Epoch 14/100: Training Loss: 0.000698183166103129
Epoch 15/100: Training Loss: 0.0006690321524450384
Epoch 16/100: Training Loss: 0.0002985007009623241
Epoch 17/100: Training Loss: 0.000195182197115904
Epoch 18/100: Training Loss: 0.00016176513184799007
Epoch 19/100: Training Loss: 0.00024866415587670965
Epoch 20/100: Training Loss: 0.0002165733168095899
Epoch 21/100: Training Loss: 0.0002551984521874621
Epoch 22/100: Training Loss: 0.00019353513885860793
Epoch 23/100: Training Loss: 0.0005008169661270329
Epoch 24/100: Training Loss: 0.00021466628814036128
Epoch 25/100: Training Loss: 0.00019052881816047833
Epoch 26/100: Training Loss: 0.00019223432690819347
Epoch 27/100: Training Loss: 0.0002170693060737446
Epoch 28/100: Training Loss: 0.00028085418365484364
Epoch 29/100: Training Loss: 0.00028349402034940897
Epoch 30/100: Training Loss: 0.00023189936678833756
Epoch 31/100: Training Loss: 0.00042945640218769844
Epoch 32/100: Training Loss: 0.0010031265166639551
Epoch 33/100: Training Loss: 0.0008033091670896378
Epoch 34/100: Training Loss: 0.00037726094843419785
Epoch 35/100: Training Loss: 0.0003804239712975508
Epoch 36/100: Training Loss: 0.00022960785616394933
Epoch 37/100: Training Loss: 6.919635311226171e-05
Epoch 38/100: Training Loss: 0.00015296253369995422
Epoch 39/100: Training Loss: 2.3537999636099383e-05
Epoch 40/100: Training Loss: 6.403983713476204e-05
Epoch 41/100: Training Loss: 5.250120752611043e-05
Epoch 42/100: Training Loss: 0.00015529472540858332
Epoch 43/100: Training Loss: 0.0005716984531630768
Epoch 44/100: Training Loss: 5.5042838254954916e-05
Epoch 45/100: Training Loss: 7.722011990159567e-05
Epoch 46/100: Training Loss: 3.177947450452056e-05
Epoch 47/100: Training Loss: 2.9576350963371662e-05
Epoch 48/100: Training Loss: 0.00017955438719571002
Epoch 49/100: Training Loss: 3.6537912931361814e-05
Epoch 50/100: Training Loss: 0.00019852823914925745
Epoch 51/100: Training Loss: 0.00013498034031113231
Epoch 52/100: Training Loss: 0.00016829272035075112
Epoch 53/100: Training Loss: 0.0006314428763155557
Epoch 54/100: Training Loss: 0.0002523687171058421
Epoch 55/100: Training Loss: 0.00019100357555538599
Epoch 56/100: Training Loss: 0.00018549832783959394
Epoch 57/100: Training Loss: 0.0001746197945318339
Epoch 58/100: Training Loss: 0.00017333642059674294
Epoch 59/100: Training Loss: 0.00012318869949849836
Epoch 60/100: Training Loss: 1.1560950639651963e-05
Epoch 61/100: Training Loss: 1.2970528068260912e-05
Epoch 62/100: Training Loss: 2.670433033726241e-06
Epoch 63/100: Training Loss: 0.00010368535352265177
Epoch 64/100: Training Loss: 1.5034240333397696e-05
Epoch 65/100: Training Loss: 8.08561189340119e-06
Epoch 66/100: Training Loss: 8.223521768863947e-05
Epoch 67/100: Training Loss: 7.308423204884573e-06
Epoch 68/100: Training Loss: 6.700938059005635e-06
Epoch 69/100: Training Loss: 1.7591410722195005e-05
Epoch 70/100: Training Loss: 8.173366334357875e-05
Epoch 71/100: Training Loss: 7.843701033195927e-07
Epoch 72/100: Training Loss: 4.7565215021554675e-05
Epoch 73/100: Training Loss: 3.1946071257894756e-06
Epoch 74/100: Training Loss: 3.511866374638366e-06
Epoch 75/100: Training Loss: 2.5883611245937875e-06
Epoch 76/100: Training Loss: 5.7000444739188515e-06
Epoch 77/100: Training Loss: 4.9342969832427665e-06
Epoch 78/100: Training Loss: 4.285606812754292e-05
Epoch 79/100: Training Loss: 8.360194213734082e-06
Epoch 80/100: Training Loss: 2.187113791841305e-05
Epoch 81/100: Training Loss: 7.392912874558221e-06
Epoch 82/100: Training Loss: 2.1285765970045804e-05
Epoch 83/100: Training Loss: 0.00045633073781896957
Epoch 84/100: Training Loss: 7.33401500648516e-05
Epoch 85/100: Training Loss: 0.00024362024993984245
Epoch 86/100: Training Loss: 0.00018918963343453554
Epoch 87/100: Training Loss: 5.597662554905276e-06
Epoch 88/100: Training Loss: 0.002138517202775171
Epoch 89/100: Training Loss: 0.0001796435427629143
Epoch 90/100: Training Loss: 0.0005049130485101712
Epoch 91/100: Training Loss: 1.3757038552976824e-05
Epoch 92/100: Training Loss: 4.982124039891856e-06
Epoch 93/100: Training Loss: 1.149433375724757e-06
Epoch 94/100: Training Loss: 8.71136224617256e-05
Epoch 95/100: Training Loss: 8.63458869830231e-06
Epoch 96/100: Training Loss: 1.0225919012460241e-05
Epoch 97/100: Training Loss: 3.904033169447462e-06
Epoch 98/100: Training Loss: 3.415621542857469e-05
Epoch 99/100: Training Loss: 3.200560733904502e-06
Epoch 0/100: Training Loss: 0.0025488403311536355
Epoch 1/100: Training Loss: 0.0016348418648257578
Epoch 2/100: Training Loss: 0.0009594928998888636
Epoch 3/100: Training Loss: 0.0010011773899288997
Epoch 4/100: Training Loss: 0.0006106477116514569
Epoch 5/100: Training Loss: 0.00033360799671682114
Epoch 6/100: Training Loss: 0.0005924563192151076
Epoch 7/100: Training Loss: 0.00036764819106441336
Epoch 8/100: Training Loss: 0.0007901223707784173
Epoch 9/100: Training Loss: 0.0003308149203201013
Epoch 10/100: Training Loss: 0.0001486990532260731
Epoch 11/100: Training Loss: 0.001433006916309427
Epoch 12/100: Training Loss: 0.0014200795647556796
Epoch 13/100: Training Loss: 0.0019816266247099893
Epoch 14/100: Training Loss: 0.0007448630190334437
Epoch 15/100: Training Loss: 0.0006263443937330889
Epoch 16/100: Training Loss: 0.0010325562186036374
Epoch 17/100: Training Loss: 0.0006877424801054176
Epoch 18/100: Training Loss: 0.0009605338785545958
Epoch 19/100: Training Loss: 0.0007023817282512876
Epoch 20/100: Training Loss: 0.000665849886057567
Epoch 21/100: Training Loss: 0.0006141152941376153
Epoch 22/100: Training Loss: 0.0003376888579751816
Epoch 23/100: Training Loss: 0.0002056432465102775
Epoch 24/100: Training Loss: 0.0001576450636233289
Epoch 25/100: Training Loss: 0.00010699834788869495
Epoch 26/100: Training Loss: 0.00018789623778290543
Epoch 27/100: Training Loss: 0.00018697153159811454
Epoch 28/100: Training Loss: 0.0002927838658993961
Epoch 29/100: Training Loss: 8.56919442674865e-05
Epoch 30/100: Training Loss: 0.00022487193803114394
Epoch 31/100: Training Loss: 1.2779811202931258e-05
Epoch 32/100: Training Loss: 0.0005165745784168594
Epoch 33/100: Training Loss: 0.00037416020419700014
Epoch 34/100: Training Loss: 0.00026262284147958817
Epoch 35/100: Training Loss: 0.0012068721962852711
Epoch 36/100: Training Loss: 0.00017889389094033855
Epoch 37/100: Training Loss: 0.00017402517649293676
Epoch 38/100: Training Loss: 0.00018267731176563567
Epoch 39/100: Training Loss: 0.0002965116244883625
Epoch 40/100: Training Loss: 8.195947192929274e-05
Epoch 41/100: Training Loss: 2.2361266421028442e-05
Epoch 42/100: Training Loss: 3.217151702775911e-06
Epoch 43/100: Training Loss: 0.0002669514002609838
Epoch 44/100: Training Loss: 0.00016738200114548572
Epoch 45/100: Training Loss: 5.962401399948846e-06
Epoch 46/100: Training Loss: 0.0010618721963437789
Epoch 47/100: Training Loss: 3.404223268703083e-06
Epoch 48/100: Training Loss: 0.00014562339184840032
Epoch 49/100: Training Loss: 8.711508515239493e-06
Epoch 50/100: Training Loss: 9.288541127019133e-05
Epoch 51/100: Training Loss: 2.5077960082541213e-05
Epoch 52/100: Training Loss: 9.229905943334833e-06
Epoch 53/100: Training Loss: 0.00016043452900245877
Epoch 54/100: Training Loss: 3.438024421685313e-05
Epoch 55/100: Training Loss: 1.2321439086019627e-05
Epoch 56/100: Training Loss: 1.0528476720253017e-05
Epoch 57/100: Training Loss: 7.577174188900578e-05
Epoch 58/100: Training Loss: 2.9930754199029843e-07
Epoch 59/100: Training Loss: 4.2277973922300925e-07
Epoch 60/100: Training Loss: 1.1606464031055295e-05
Epoch 61/100: Training Loss: 4.140751584907251e-06
Epoch 62/100: Training Loss: 6.053168926548007e-06
Epoch 63/100: Training Loss: 1.634455548793626e-05
Epoch 64/100: Training Loss: 9.817019122098486e-06
Epoch 65/100: Training Loss: 2.0797575818249054e-05
Epoch 66/100: Training Loss: 5.839606232804023e-05
Epoch 67/100: Training Loss: 1.7412482515160292e-05
Epoch 68/100: Training Loss: 0.0001694438464802467
Epoch 69/100: Training Loss: 2.8092325579352173e-05
Epoch 70/100: Training Loss: 9.197400642867469e-05
Epoch 71/100: Training Loss: 0.00011159654455316579
Epoch 72/100: Training Loss: 0.0006787133363126977
Epoch 73/100: Training Loss: 0.00013462019844289206
Epoch 74/100: Training Loss: 4.3254505637233245e-05
Epoch 75/100: Training Loss: 5.777836691565309e-05
Epoch 76/100: Training Loss: 0.00017504215788987517
Epoch 77/100: Training Loss: 4.088980334302399e-05
Epoch 78/100: Training Loss: 0.00014253289787315884
Epoch 79/100: Training Loss: 1.3949182934407715e-06
Epoch 80/100: Training Loss: 3.929351716433749e-06
Epoch 81/100: Training Loss: 6.216090383933731e-06
Epoch 82/100: Training Loss: 1.27314465288987e-06
Epoch 83/100: Training Loss: 1.0539398334371898e-05
Epoch 84/100: Training Loss: 0.001263811178733966
Epoch 85/100: Training Loss: 7.17295148049944e-06
Epoch 86/100: Training Loss: 3.8671095861247715e-05
Epoch 87/100: Training Loss: 1.2819502578385493e-06
Epoch 88/100: Training Loss: 4.658056137386275e-05
Epoch 89/100: Training Loss: 1.4580933354292179e-05
Epoch 90/100: Training Loss: 1.4571641554678875e-06
Epoch 91/100: Training Loss: 1.5990601486863535e-06
Epoch 92/100: Training Loss: 1.1217459019250664e-06
Epoch 93/100: Training Loss: 5.1309765985041305e-05
Epoch 94/100: Training Loss: 9.630083656752699e-08
Epoch 95/100: Training Loss: 4.669220009647264e-06
Epoch 96/100: Training Loss: 1.1035226188325809e-06
Epoch 97/100: Training Loss: 7.089068170385492e-06
Epoch 98/100: Training Loss: 5.7339270462059174e-06
Epoch 99/100: Training Loss: 0.00010945786224918131
Epoch 0/100: Training Loss: 0.001977873177616143
Epoch 1/100: Training Loss: 0.0014744390922090027
Epoch 2/100: Training Loss: 0.0005285996326639609
Epoch 3/100: Training Loss: 0.001774128960685496
Epoch 4/100: Training Loss: 0.0008083882690207359
Epoch 5/100: Training Loss: 0.0005368500308025102
Epoch 6/100: Training Loss: 0.00040382983311553676
Epoch 7/100: Training Loss: 0.0004800241326261883
Epoch 8/100: Training Loss: 0.0006107980945358978
Epoch 9/100: Training Loss: 0.0004798696359242398
Epoch 10/100: Training Loss: 0.0018901766443545102
Epoch 11/100: Training Loss: 0.0016064592665689854
Epoch 12/100: Training Loss: 0.002444924203896084
Epoch 13/100: Training Loss: 0.0016850125204566067
Epoch 14/100: Training Loss: 0.0013757261213349417
Epoch 15/100: Training Loss: 0.0012184319320631906
Epoch 16/100: Training Loss: 0.0009512499066218277
Epoch 17/100: Training Loss: 0.0005172158311481125
Epoch 18/100: Training Loss: 0.0004179890444673644
Epoch 19/100: Training Loss: 0.00020905874349588265
Epoch 20/100: Training Loss: 0.00010571581957164717
Epoch 21/100: Training Loss: 0.0005140430722499917
Epoch 22/100: Training Loss: 0.00046902104206611775
Epoch 23/100: Training Loss: 0.00020345640968691352
Epoch 24/100: Training Loss: 0.0006276423945748733
Epoch 25/100: Training Loss: 0.00018649022637700743
Epoch 26/100: Training Loss: 0.000315252257271047
Epoch 27/100: Training Loss: 0.00041478684891952327
Epoch 28/100: Training Loss: 0.001096024926454743
Epoch 29/100: Training Loss: 0.00036617875830527463
Epoch 30/100: Training Loss: 0.00024846470429122083
Epoch 31/100: Training Loss: 0.0002244973749470857
Epoch 32/100: Training Loss: 0.0001238795808670711
Epoch 33/100: Training Loss: 0.00011954097751459462
Epoch 34/100: Training Loss: 0.00015283754221135122
Epoch 35/100: Training Loss: 5.992924211763897e-05
Epoch 36/100: Training Loss: 7.971786991036012e-05
Epoch 37/100: Training Loss: 0.00037617030684933343
Epoch 38/100: Training Loss: 0.0001873448376823788
Epoch 39/100: Training Loss: 0.00019255013096551953
Epoch 40/100: Training Loss: 0.00015731170727797082
Epoch 41/100: Training Loss: 0.00019753461509394499
Epoch 42/100: Training Loss: 0.00028708302901566394
Epoch 43/100: Training Loss: 0.00016934685180523644
Epoch 44/100: Training Loss: 0.0006991417770005442
Epoch 45/100: Training Loss: 0.0004320215045308774
Epoch 46/100: Training Loss: 0.00011527450309574969
Epoch 47/100: Training Loss: 2.3561414114079593e-05
Epoch 48/100: Training Loss: 5.548628492772213e-05
Epoch 49/100: Training Loss: 0.00024274035008407078
Epoch 50/100: Training Loss: 0.00040770742607994313
Epoch 51/100: Training Loss: 5.959368030900604e-05
Epoch 52/100: Training Loss: 5.0938745354947866e-05
Epoch 53/100: Training Loss: 5.057828352312369e-05
Epoch 54/100: Training Loss: 0.00013192481378470461
Epoch 55/100: Training Loss: 9.610041107502452e-05
Epoch 56/100: Training Loss: 0.00018747033197089938
Epoch 57/100: Training Loss: 0.00019200212476443658
Epoch 58/100: Training Loss: 0.00011092235842731102
Epoch 59/100: Training Loss: 7.720661858108146e-05
Epoch 60/100: Training Loss: 9.114688342334302e-05
Epoch 61/100: Training Loss: 5.0560588394205995e-05
Epoch 62/100: Training Loss: 0.0001847961563274173
Epoch 63/100: Training Loss: 9.887771783796556e-05
Epoch 64/100: Training Loss: 4.8387807113992654e-05
Epoch 65/100: Training Loss: 0.00012745830498955732
Epoch 66/100: Training Loss: 0.000839220234221476
Epoch 67/100: Training Loss: 9.551011825814569e-05
Epoch 68/100: Training Loss: 2.2099389083272108e-05
Epoch 69/100: Training Loss: 6.651649811516511e-05
Epoch 70/100: Training Loss: 2.859197157018024e-05
Epoch 71/100: Training Loss: 9.108115947502522e-05
Epoch 72/100: Training Loss: 0.00010877135752534573
Epoch 73/100: Training Loss: 4.5472916009967315e-05
Epoch 74/100: Training Loss: 8.888111272655382e-05
Epoch 75/100: Training Loss: 1.0241117996900725e-05
Epoch 76/100: Training Loss: 7.918934434926583e-06
Epoch 77/100: Training Loss: 8.91651446856787e-06
Epoch 78/100: Training Loss: 0.0003135801047269552
Epoch 79/100: Training Loss: 3.724910627387784e-05
Epoch 80/100: Training Loss: 6.954453491314423e-06
Epoch 81/100: Training Loss: 5.68125568375997e-05
Epoch 82/100: Training Loss: 1.9848542402218457e-05
Epoch 83/100: Training Loss: 1.9807924167967283e-06
Epoch 84/100: Training Loss: 2.586911860967706e-05
Epoch 85/100: Training Loss: 8.279858357015929e-06
Epoch 86/100: Training Loss: 1.6073663461884837e-05
Epoch 87/100: Training Loss: 1.2431495131616212e-05
Epoch 88/100: Training Loss: 3.830901135696224e-05
Epoch 89/100: Training Loss: 5.575925028922916e-06
Epoch 90/100: Training Loss: 0.00012503511792311638
Epoch 91/100: Training Loss: 2.061378959269246e-06
Epoch 92/100: Training Loss: 1.8604452567506423e-05
Epoch 93/100: Training Loss: 0.0001618937000723704
Epoch 94/100: Training Loss: 2.2044472476364644e-05
Epoch 95/100: Training Loss: 3.880473707968289e-06
Epoch 96/100: Training Loss: 1.9820765563589664e-05
Epoch 97/100: Training Loss: 1.861388292277883e-05
Epoch 98/100: Training Loss: 8.345025654402247e-05
Epoch 99/100: Training Loss: 7.480285073304834e-06
Epoch 0/100: Training Loss: 0.0035741936415433885
Epoch 1/100: Training Loss: 0.0019296159967780112
Epoch 2/100: Training Loss: 0.002829141542315483
Epoch 3/100: Training Loss: 0.0016205409541726113
Epoch 4/100: Training Loss: 0.002286737784743309
Epoch 5/100: Training Loss: 0.002312772162258625
Epoch 6/100: Training Loss: 0.0009507364593446255
Epoch 7/100: Training Loss: 0.001406410988420248
Epoch 8/100: Training Loss: 0.0011748949065804482
Epoch 9/100: Training Loss: 0.0012169813737273217
Epoch 10/100: Training Loss: 0.0017315445467829705
Epoch 11/100: Training Loss: 0.001309253741055727
Epoch 12/100: Training Loss: 0.0013288761489093303
Epoch 13/100: Training Loss: 0.0011001241393387318
Epoch 14/100: Training Loss: 0.0018034044653177262
Epoch 15/100: Training Loss: 0.0015319416299462319
Epoch 16/100: Training Loss: 0.001282075233757496
Epoch 17/100: Training Loss: 0.0014196550473570824
Epoch 18/100: Training Loss: 0.0012656173668801785
Epoch 19/100: Training Loss: 0.0017040245234966277
Epoch 20/100: Training Loss: 0.0017038417980074882
Epoch 21/100: Training Loss: 0.0015870722010731698
Epoch 22/100: Training Loss: 0.001380976103246212
Epoch 23/100: Training Loss: 0.0015932951122522355
Epoch 24/100: Training Loss: 0.0016530752182006836
Epoch 25/100: Training Loss: 0.0014136762358248234
Epoch 26/100: Training Loss: 0.0014654467813670635
Epoch 27/100: Training Loss: 0.0015553704462945462
Epoch 28/100: Training Loss: 0.0016203215345740318
Epoch 29/100: Training Loss: 0.00150804053992033
Epoch 30/100: Training Loss: 0.0014395819045603276
Epoch 31/100: Training Loss: 0.0005119877867400646
Epoch 32/100: Training Loss: 0.0007868461310863495
Epoch 33/100: Training Loss: 0.0011033892631530761
Epoch 34/100: Training Loss: 0.001474428828805685
Epoch 35/100: Training Loss: 0.0013267507776618004
Epoch 36/100: Training Loss: 0.0008272885344922543
Epoch 37/100: Training Loss: 0.001103669125586748
Epoch 38/100: Training Loss: 0.0008867082186043262
Epoch 39/100: Training Loss: 0.0006314251571893692
Epoch 40/100: Training Loss: 0.0006979796104133129
Epoch 41/100: Training Loss: 0.0006925360299646855
Epoch 42/100: Training Loss: 0.000887493323534727
Epoch 43/100: Training Loss: 0.0007795800454914569
Epoch 44/100: Training Loss: 0.000829686876386404
Epoch 45/100: Training Loss: 0.000642302492633462
Epoch 46/100: Training Loss: 0.0008743278682231903
Epoch 47/100: Training Loss: 0.0010108153335750103
Epoch 48/100: Training Loss: 0.0008746213279664516
Epoch 49/100: Training Loss: 0.0009164063259959221
Epoch 50/100: Training Loss: 0.0007235505152493715
Epoch 51/100: Training Loss: 0.000728036742657423
Epoch 52/100: Training Loss: 0.001116964966058731
Epoch 53/100: Training Loss: 0.0009445548988878727
Epoch 54/100: Training Loss: 0.0007029480766505003
Epoch 55/100: Training Loss: 0.0005598381627351046
Epoch 56/100: Training Loss: 0.0006653869058936834
Epoch 57/100: Training Loss: 0.0004913581069558859
Epoch 58/100: Training Loss: 0.00046459645964205267
Epoch 59/100: Training Loss: 0.0005613252986222505
Epoch 60/100: Training Loss: 0.0004379983525723219
Epoch 61/100: Training Loss: 0.0002430278342217207
Epoch 62/100: Training Loss: 0.00030044075101614
Epoch 63/100: Training Loss: 0.0003103986382484436
Epoch 64/100: Training Loss: 0.0006535441149026156
Epoch 65/100: Training Loss: 0.0005007705185562372
Epoch 66/100: Training Loss: 0.0005044449586421252
Epoch 67/100: Training Loss: 0.00028573598247021436
Epoch 68/100: Training Loss: 0.0005489990580826998
Epoch 69/100: Training Loss: 0.0004340364597737789
Epoch 70/100: Training Loss: 0.0002012259094044566
Epoch 71/100: Training Loss: 0.0005776894744485616
Epoch 72/100: Training Loss: 0.0003651516744866967
Epoch 73/100: Training Loss: 0.0005227458197623492
Epoch 74/100: Training Loss: 0.0006169706117361784
Epoch 75/100: Training Loss: 0.0005597675684839487
Epoch 76/100: Training Loss: 0.0005721797700971365
Epoch 77/100: Training Loss: 0.0003820823272690177
Epoch 78/100: Training Loss: 0.0004496640525758266
Epoch 79/100: Training Loss: 0.00017791265854611994
Epoch 80/100: Training Loss: 0.00047088363207876683
Epoch 81/100: Training Loss: 0.00021010218188166618
Epoch 82/100: Training Loss: 0.0009301373735070229
Epoch 83/100: Training Loss: 0.0005701818503439426
Epoch 84/100: Training Loss: 0.0005142826121300459
Epoch 85/100: Training Loss: 0.0006061758380383253
Epoch 86/100: Training Loss: 0.0005136020015925169
Epoch 87/100: Training Loss: 0.00016098887426778673
Epoch 88/100: Training Loss: 0.0005099552683532238
Epoch 89/100: Training Loss: 0.0004934437572956085
Epoch 90/100: Training Loss: 0.000563925039023161
Epoch 91/100: Training Loss: 0.0003933019936084747
Epoch 92/100: Training Loss: 0.00046760779805481433
Epoch 93/100: Training Loss: 0.0004961770959198475
Epoch 94/100: Training Loss: 0.0005427083931863308
Epoch 95/100: Training Loss: 5.1428674487397076e-05
Epoch 96/100: Training Loss: 0.0010476413182914257
Epoch 97/100: Training Loss: 0.00019069729605689645
Epoch 98/100: Training Loss: 0.00035618848633021114
Epoch 99/100: Training Loss: 0.0004497885704040527
Epoch 0/100: Training Loss: 0.0029314110055565835
Epoch 1/100: Training Loss: 0.001946387067437172
Epoch 2/100: Training Loss: 0.0024208918213844298
Epoch 3/100: Training Loss: 0.0018416332080960275
Epoch 4/100: Training Loss: 0.0021989258006215096
Epoch 5/100: Training Loss: 0.0024343643337488173
Epoch 6/100: Training Loss: 0.0019912704825401305
Epoch 7/100: Training Loss: 0.0014067999087274074
Epoch 8/100: Training Loss: 0.0010989353992044925
Epoch 9/100: Training Loss: 0.0017115244641900062
Epoch 10/100: Training Loss: 0.0015084451995790004
Epoch 11/100: Training Loss: 0.0012480967678129673
Epoch 12/100: Training Loss: 0.0015041662380099296
Epoch 13/100: Training Loss: 0.0018559575080871582
Epoch 14/100: Training Loss: 0.0017106903716921807
Epoch 15/100: Training Loss: 0.001347933989018202
Epoch 16/100: Training Loss: 0.0014871202409267425
Epoch 17/100: Training Loss: 0.0016394365578889848
Epoch 18/100: Training Loss: 0.0015390944667160512
Epoch 19/100: Training Loss: 0.0016764892265200616
Epoch 20/100: Training Loss: 0.001514239702373743
Epoch 21/100: Training Loss: 0.0013119258917868137
Epoch 22/100: Training Loss: 0.0011894199065864085
Epoch 23/100: Training Loss: 0.0016265701502561568
Epoch 24/100: Training Loss: 0.001621505804359913
Epoch 25/100: Training Loss: 0.0016721947118639945
Epoch 26/100: Training Loss: 0.002005833387374878
Epoch 27/100: Training Loss: 0.0015994450077414513
Epoch 28/100: Training Loss: 0.0015987755730748176
Epoch 29/100: Training Loss: 0.0016379727050662041
Epoch 30/100: Training Loss: 0.0015644608065485954
Epoch 31/100: Training Loss: 0.0016040705144405366
Epoch 32/100: Training Loss: 0.00166411604732275
Epoch 33/100: Training Loss: 0.001541145984083414
Epoch 34/100: Training Loss: 0.0014790116809308528
Epoch 35/100: Training Loss: 0.00145883709192276
Epoch 36/100: Training Loss: 0.0014754926785826683
Epoch 37/100: Training Loss: 0.001144997589290142
Epoch 38/100: Training Loss: 0.0013572153635323047
Epoch 39/100: Training Loss: 0.0004598978906869888
Epoch 40/100: Training Loss: 0.0015824491158127785
Epoch 41/100: Training Loss: 0.0012989000417292119
Epoch 42/100: Training Loss: 0.0019870853051543237
Epoch 43/100: Training Loss: 0.0012814352288842202
Epoch 44/100: Training Loss: 0.001238736603409052
Epoch 45/100: Training Loss: 0.00106942905113101
Epoch 46/100: Training Loss: 0.0011481320485472678
Epoch 47/100: Training Loss: 0.0012605146504938603
Epoch 48/100: Training Loss: 0.0007191550917923451
Epoch 49/100: Training Loss: 0.0009952682070434094
Epoch 50/100: Training Loss: 0.0010846495628356934
Epoch 51/100: Training Loss: 0.0005749473813921214
Epoch 52/100: Training Loss: 0.001100268680602312
Epoch 53/100: Training Loss: 0.0007518318016082048
Epoch 54/100: Training Loss: 0.0009662541560828686
Epoch 55/100: Training Loss: 0.0008394626900553703
Epoch 56/100: Training Loss: 0.0006077458616346121
Epoch 57/100: Training Loss: 0.0009014160372316837
Epoch 58/100: Training Loss: 0.0008843475952744484
Epoch 59/100: Training Loss: 0.0008186225779354573
Epoch 60/100: Training Loss: 0.0023031232878565787
Epoch 61/100: Training Loss: 0.0006134732626378536
Epoch 62/100: Training Loss: 0.00032013221643865106
Epoch 63/100: Training Loss: 0.000265727867372334
Epoch 64/100: Training Loss: 0.00037994023878127334
Epoch 65/100: Training Loss: 0.0004852124024182558
Epoch 66/100: Training Loss: 0.0008404048159718514
Epoch 67/100: Training Loss: 0.0006670310627669096
Epoch 68/100: Training Loss: 0.0004971921909600497
Epoch 69/100: Training Loss: 0.0006713408511132002
Epoch 70/100: Training Loss: 0.0007439374923706055
Epoch 71/100: Training Loss: 0.0007860271260142326
Epoch 72/100: Training Loss: 0.0007167733274400234
Epoch 73/100: Training Loss: 0.0007031088229268789
Epoch 74/100: Training Loss: 0.0006950016599148512
Epoch 75/100: Training Loss: 0.0006409298162907362
Epoch 76/100: Training Loss: 0.0006898417137563229
Epoch 77/100: Training Loss: 0.00011138387490063905
Epoch 78/100: Training Loss: 0.0006681324448436499
Epoch 79/100: Training Loss: 0.0004624186549335718
Epoch 80/100: Training Loss: 0.0006040597800165415
Epoch 81/100: Training Loss: 0.0007246746215969324
Epoch 82/100: Training Loss: 0.0008125299587845803
Epoch 83/100: Training Loss: 0.0005743347574025393
Epoch 84/100: Training Loss: 0.0003208356210961938
Epoch 85/100: Training Loss: 0.0004998493008315563
Epoch 86/100: Training Loss: 0.0004963391926139593
Epoch 87/100: Training Loss: 0.0002940332982689142
Epoch 88/100: Training Loss: 0.0006736147683113813
Epoch 89/100: Training Loss: 0.0006162140518426896
Epoch 90/100: Training Loss: 0.0004047204274684191
Epoch 91/100: Training Loss: 0.0005953405518084765
Epoch 92/100: Training Loss: 0.0011623702943325044
Epoch 93/100: Training Loss: 0.00020446698181331158
Epoch 94/100: Training Loss: 0.000689783226698637
Epoch 95/100: Training Loss: 0.0004258214496076107
Epoch 96/100: Training Loss: 0.0007894686423242092
Epoch 97/100: Training Loss: 0.000538407638669014
Epoch 98/100: Training Loss: 0.00046052574180066586
Epoch 99/100: Training Loss: 0.0005209841299802064
Epoch 0/100: Training Loss: 0.002758529409766197
Epoch 1/100: Training Loss: 0.002541123516857624
Epoch 2/100: Training Loss: 0.0019488943740725516
Epoch 3/100: Training Loss: 0.0024216776713728906
Epoch 4/100: Training Loss: 0.002715268358588219
Epoch 5/100: Training Loss: 0.0018378864973783494
Epoch 6/100: Training Loss: 0.002417930029332638
Epoch 7/100: Training Loss: 0.0017865290865302086
Epoch 8/100: Training Loss: 0.0016773458570241927
Epoch 9/100: Training Loss: 0.00147157097235322
Epoch 10/100: Training Loss: 0.0016343554481863976
Epoch 11/100: Training Loss: 0.0015406924299895763
Epoch 12/100: Training Loss: 0.0011486102826893329
Epoch 13/100: Training Loss: 0.0014318824745714665
Epoch 14/100: Training Loss: 0.0019214911386370658
Epoch 15/100: Training Loss: 0.0015092958696186543
Epoch 16/100: Training Loss: 0.0015665091574192046
Epoch 17/100: Training Loss: 0.0009929236955940723
Epoch 18/100: Training Loss: 0.0014809036627411842
Epoch 19/100: Training Loss: 0.0015310286544263363
Epoch 20/100: Training Loss: 0.0015122897922992707
Epoch 21/100: Training Loss: 0.0013961145654320718
Epoch 22/100: Training Loss: 0.0013892552815377712
Epoch 23/100: Training Loss: 0.0014743152074515819
Epoch 24/100: Training Loss: 0.0010575800202786922
Epoch 25/100: Training Loss: 0.0013246042653918266
Epoch 26/100: Training Loss: 0.0014309719204902648
Epoch 27/100: Training Loss: 0.00137183154001832
Epoch 28/100: Training Loss: 0.0014249831438064575
Epoch 29/100: Training Loss: 0.0014447239227592945
Epoch 30/100: Training Loss: 0.001471785642206669
Epoch 31/100: Training Loss: 0.0013865123502910138
Epoch 32/100: Training Loss: 0.0015400242991745472
Epoch 33/100: Training Loss: 0.0014144950546324253
Epoch 34/100: Training Loss: 0.00135086914524436
Epoch 35/100: Training Loss: 0.0014480412006378173
Epoch 36/100: Training Loss: 0.0010257280431687833
Epoch 37/100: Training Loss: 0.0014294309541583062
Epoch 38/100: Training Loss: 0.0014466367661952972
Epoch 39/100: Training Loss: 0.0011979675851762294
Epoch 40/100: Training Loss: 0.0008960298262536525
Epoch 41/100: Training Loss: 0.000856654904782772
Epoch 42/100: Training Loss: 0.0005543747451156378
Epoch 43/100: Training Loss: 0.0006478419993072749
Epoch 44/100: Training Loss: 0.0007844382897019386
Epoch 45/100: Training Loss: 0.0008731631562113762
Epoch 46/100: Training Loss: 0.0007042656652629375
Epoch 47/100: Training Loss: 0.0008279145695269108
Epoch 48/100: Training Loss: 0.0010330843739211558
Epoch 49/100: Training Loss: 0.0003953920677304268
Epoch 50/100: Training Loss: 0.0008555809967219829
Epoch 51/100: Training Loss: 0.0007700876332819462
Epoch 52/100: Training Loss: 0.0005631640553474427
Epoch 53/100: Training Loss: 0.0007684053387492895
Epoch 54/100: Training Loss: 0.000679893558844924
Epoch 55/100: Training Loss: 0.0007485289592295885
Epoch 56/100: Training Loss: 0.000636407732963562
Epoch 57/100: Training Loss: 0.0004957533441483974
Epoch 58/100: Training Loss: 0.00035714430268853905
Epoch 59/100: Training Loss: 0.000699559971690178
Epoch 60/100: Training Loss: 0.0006164086516946554
Epoch 61/100: Training Loss: 0.0005594220012426377
Epoch 62/100: Training Loss: 0.000613704789429903
Epoch 63/100: Training Loss: 0.0006840053480118513
Epoch 64/100: Training Loss: 0.0003968109842389822
Epoch 65/100: Training Loss: 0.0005539850797504186
Epoch 66/100: Training Loss: 0.0006965068634599448
Epoch 67/100: Training Loss: 0.0006138251163065433
Epoch 68/100: Training Loss: 0.00037457142025232315
Epoch 69/100: Training Loss: 0.0006556206848472357
Epoch 70/100: Training Loss: 0.0006148410029709339
Epoch 71/100: Training Loss: 0.00047204154543578625
Epoch 72/100: Training Loss: 0.00041722473688423636
Epoch 73/100: Training Loss: 0.00022472881246358157
Epoch 74/100: Training Loss: 0.0004299452994018793
Epoch 75/100: Training Loss: 0.00027087575290352106
Epoch 76/100: Training Loss: 0.0005507179070264101
Epoch 77/100: Training Loss: 0.00047828094102442265
Epoch 78/100: Training Loss: 0.00037563808728009463
Epoch 79/100: Training Loss: 0.00034940901678055526
Epoch 80/100: Training Loss: 0.00032635729294270275
Epoch 81/100: Training Loss: 0.0005898742470890284
Epoch 82/100: Training Loss: 0.0006273091305047274
Epoch 83/100: Training Loss: 0.0005874860566109419
Epoch 84/100: Training Loss: 0.0005419549532234668
Epoch 85/100: Training Loss: 0.0015194430015981197
Epoch 86/100: Training Loss: 0.0005051915999501943
Epoch 87/100: Training Loss: 0.0006054319441318512
Epoch 88/100: Training Loss: 0.0007642829325050116
Epoch 89/100: Training Loss: 0.0005386040546000003
Epoch 90/100: Training Loss: 0.00042983838357031346
Epoch 91/100: Training Loss: 0.0007494538556784391
Epoch 92/100: Training Loss: 0.0004529528319835663
Epoch 93/100: Training Loss: 0.0004403992090374231
Epoch 94/100: Training Loss: 0.000761509733274579
Epoch 95/100: Training Loss: 0.0004080019425600767
Epoch 96/100: Training Loss: 0.00015715848421677948
Epoch 97/100: Training Loss: 0.0003900629933923483
Epoch 98/100: Training Loss: 0.0004055760335177183
Epoch 99/100: Training Loss: 0.0003608311759307981
Epoch 0/100: Training Loss: 0.0018661766295220443
Epoch 1/100: Training Loss: 0.0015359863543966013
Epoch 2/100: Training Loss: 0.0012791693969896644
Epoch 3/100: Training Loss: 0.002060214235524463
Epoch 4/100: Training Loss: 0.0016293531390512066
Epoch 5/100: Training Loss: 0.001643363267752775
Epoch 6/100: Training Loss: 0.0015423401336001742
Epoch 7/100: Training Loss: 0.0011897333867990287
Epoch 8/100: Training Loss: 0.0011304279991016266
Epoch 9/100: Training Loss: 0.0012946346193362193
Epoch 10/100: Training Loss: 0.001203784802157408
Epoch 11/100: Training Loss: 0.0009921962858005693
Epoch 12/100: Training Loss: 0.0012931752546577696
Epoch 13/100: Training Loss: 0.0009100826302911066
Epoch 14/100: Training Loss: 0.0010757212805899844
Epoch 15/100: Training Loss: 0.0010083522766259065
Epoch 16/100: Training Loss: 0.000810941977865377
Epoch 17/100: Training Loss: 0.0008971534527031479
Epoch 18/100: Training Loss: 0.0012453859019431338
Epoch 19/100: Training Loss: 0.0010002098835197983
Epoch 20/100: Training Loss: 0.0012082563843696741
Epoch 21/100: Training Loss: 0.0012585030999153284
Epoch 22/100: Training Loss: 0.001366012889868135
Epoch 23/100: Training Loss: 0.0011115619900879587
Epoch 24/100: Training Loss: 0.0010423706785129133
Epoch 25/100: Training Loss: 0.0008948575349370386
Epoch 26/100: Training Loss: 0.0010176827763296236
Epoch 27/100: Training Loss: 0.0010586016497034936
Epoch 28/100: Training Loss: 0.0014462165392128525
Epoch 29/100: Training Loss: 0.0005620220664200509
Epoch 30/100: Training Loss: 0.0008057342593077642
Epoch 31/100: Training Loss: 0.0006950729212183861
Epoch 32/100: Training Loss: 0.0009488654174622457
Epoch 33/100: Training Loss: 0.0014117015585018572
Epoch 34/100: Training Loss: 0.0011708488699736868
Epoch 35/100: Training Loss: 0.0017516783847930325
Epoch 36/100: Training Loss: 0.002227521056582214
Epoch 37/100: Training Loss: 0.0011267767400498603
Epoch 38/100: Training Loss: 0.001283998891806147
Epoch 39/100: Training Loss: 0.001011390784743485
Epoch 40/100: Training Loss: 0.000918262608491691
Epoch 41/100: Training Loss: 0.0008972264399194414
Epoch 42/100: Training Loss: 0.0017300772059495283
Epoch 43/100: Training Loss: 0.001011622179845336
Epoch 44/100: Training Loss: 0.0011690369077548858
Epoch 45/100: Training Loss: 0.0008944301468551539
Epoch 46/100: Training Loss: 0.0008559189025004199
Epoch 47/100: Training Loss: 0.0008830708113445598
Epoch 48/100: Training Loss: 0.0007285031068856549
Epoch 49/100: Training Loss: 0.0010775671263409268
Epoch 50/100: Training Loss: 0.0009922466839954352
Epoch 51/100: Training Loss: 0.0007412905336185626
Epoch 52/100: Training Loss: 0.0008085720288525722
Epoch 53/100: Training Loss: 0.0008404259658922815
Epoch 54/100: Training Loss: 0.0005788556329763619
Epoch 55/100: Training Loss: 0.0008237706437991683
Epoch 56/100: Training Loss: 0.0006150607565406021
Epoch 57/100: Training Loss: 0.00041768970383200673
Epoch 58/100: Training Loss: 0.00100732447615095
Epoch 59/100: Training Loss: 0.0008546559104494228
Epoch 60/100: Training Loss: 0.0007023120856588813
Epoch 61/100: Training Loss: 0.0013406968610301898
Epoch 62/100: Training Loss: 0.0008475364773136795
Epoch 63/100: Training Loss: 0.0004603286172933639
Epoch 64/100: Training Loss: 0.0006636914079356345
Epoch 65/100: Training Loss: 0.0010033814580577194
Epoch 66/100: Training Loss: 0.0011018462431658606
Epoch 67/100: Training Loss: 0.001099274416638028
Epoch 68/100: Training Loss: 0.0008646504134888862
Epoch 69/100: Training Loss: 0.0006014818598510354
Epoch 70/100: Training Loss: 0.0010915527677839729
Epoch 71/100: Training Loss: 0.0005059274044006493
Epoch 72/100: Training Loss: 0.0009543532209031901
Epoch 73/100: Training Loss: 0.0012329953491308128
Epoch 74/100: Training Loss: 0.0006384278653533595
Epoch 75/100: Training Loss: 0.0009681732411597185
Epoch 76/100: Training Loss: 0.0009326633943873606
Epoch 77/100: Training Loss: 0.001257058066926944
Epoch 78/100: Training Loss: 0.00035227958563786404
Epoch 79/100: Training Loss: 0.0009414552693154402
Epoch 80/100: Training Loss: 0.001177510733057739
Epoch 81/100: Training Loss: 0.0011897381323917656
Epoch 82/100: Training Loss: 0.0013158468493990078
Epoch 83/100: Training Loss: 0.0010441112670169514
Epoch 84/100: Training Loss: 0.001100586762853489
Epoch 85/100: Training Loss: 0.0008325763758580396
Epoch 86/100: Training Loss: 0.0008245035531414543
Epoch 87/100: Training Loss: 0.0008611954321527177
Epoch 88/100: Training Loss: 0.0007213240216492088
Epoch 89/100: Training Loss: 0.0011176951941411206
Epoch 90/100: Training Loss: 0.0012642799099539495
Epoch 91/100: Training Loss: 0.0022920715581079957
Epoch 92/100: Training Loss: 0.0017493128017255456
Epoch 93/100: Training Loss: 0.0007288602601950336
Epoch 94/100: Training Loss: 0.0007729273123346316
Epoch 95/100: Training Loss: 0.0009751292360816032
Epoch 96/100: Training Loss: 0.0009741385461418492
Epoch 97/100: Training Loss: 0.0009269329013338514
Epoch 98/100: Training Loss: 0.0009392651782673635
Epoch 99/100: Training Loss: 0.0012988706303250258
Epoch 0/100: Training Loss: 0.0022104291399573065
Epoch 1/100: Training Loss: 0.001884296441533763
Epoch 2/100: Training Loss: 0.001908867222488306
Epoch 3/100: Training Loss: 0.0027818174878503107
Epoch 4/100: Training Loss: 0.001519317744643825
Epoch 5/100: Training Loss: 0.0020834752328836234
Epoch 6/100: Training Loss: 0.0014931099240187626
Epoch 7/100: Training Loss: 0.0018654571976631311
Epoch 8/100: Training Loss: 0.0021696062224685767
Epoch 9/100: Training Loss: 0.0014533694762333183
Epoch 10/100: Training Loss: 0.0015779818129387632
Epoch 11/100: Training Loss: 0.0010891477962967696
Epoch 12/100: Training Loss: 0.0014297505662699413
Epoch 13/100: Training Loss: 0.00137915875122046
Epoch 14/100: Training Loss: 0.0015178865687862323
Epoch 15/100: Training Loss: 0.001833932604759362
Epoch 16/100: Training Loss: 0.0011901755811302526
Epoch 17/100: Training Loss: 0.0012931017928822024
Epoch 18/100: Training Loss: 0.0013266085249603174
Epoch 19/100: Training Loss: 0.0011621607337028357
Epoch 20/100: Training Loss: 0.0012713603343173956
Epoch 21/100: Training Loss: 0.0013895413488339468
Epoch 22/100: Training Loss: 0.0015121475336657968
Epoch 23/100: Training Loss: 0.001146837404579114
Epoch 24/100: Training Loss: 0.0010494661938612627
Epoch 25/100: Training Loss: 0.0008650142106280964
Epoch 26/100: Training Loss: 0.000916580295866462
Epoch 27/100: Training Loss: 0.0010501094115008215
Epoch 28/100: Training Loss: 0.001385513574454435
Epoch 29/100: Training Loss: 0.0009378505170724953
Epoch 30/100: Training Loss: 0.0010884345337084144
Epoch 31/100: Training Loss: 0.0010072446552811154
Epoch 32/100: Training Loss: 0.0008836799556282676
Epoch 33/100: Training Loss: 0.0007518335797224835
Epoch 34/100: Training Loss: 0.0007708304247279076
Epoch 35/100: Training Loss: 0.0007527017859136982
Epoch 36/100: Training Loss: 0.0008786013171930981
Epoch 37/100: Training Loss: 0.0020791641466177194
Epoch 38/100: Training Loss: 0.0014593856539695887
Epoch 39/100: Training Loss: 0.0012324746626957207
Epoch 40/100: Training Loss: 0.0010379924895657095
Epoch 41/100: Training Loss: 0.001106324469207958
Epoch 42/100: Training Loss: 0.0007801962790975146
Epoch 43/100: Training Loss: 0.0007753766551139248
Epoch 44/100: Training Loss: 0.0008668979262090792
Epoch 45/100: Training Loss: 0.0011524259094979354
Epoch 46/100: Training Loss: 0.0009134619668790489
Epoch 47/100: Training Loss: 0.0007322973506465838
Epoch 48/100: Training Loss: 0.0006384877547336991
Epoch 49/100: Training Loss: 0.0007130884251017479
Epoch 50/100: Training Loss: 0.0013451928355891233
Epoch 51/100: Training Loss: 0.0009919516030390551
Epoch 52/100: Training Loss: 0.001068347673507253
Epoch 53/100: Training Loss: 0.0004939892963998637
Epoch 54/100: Training Loss: 0.0011167548074843777
Epoch 55/100: Training Loss: 0.0007586597352270867
Epoch 56/100: Training Loss: 0.0005626824631053171
Epoch 57/100: Training Loss: 0.0015514383839953477
Epoch 58/100: Training Loss: 0.0006408536699926777
Epoch 59/100: Training Loss: 0.0007008437043542316
Epoch 60/100: Training Loss: 0.0007387203205922606
Epoch 61/100: Training Loss: 0.0009553262572379629
Epoch 62/100: Training Loss: 0.0007235159160225255
Epoch 63/100: Training Loss: 0.000449802418043659
Epoch 64/100: Training Loss: 0.0008557814701347594
Epoch 65/100: Training Loss: 0.0006922390905155498
Epoch 66/100: Training Loss: 0.0006769515906169916
Epoch 67/100: Training Loss: 0.0007205937698388555
Epoch 68/100: Training Loss: 0.0011176279016361114
Epoch 69/100: Training Loss: 0.0007725363229490389
Epoch 70/100: Training Loss: 0.0008187723007931072
Epoch 71/100: Training Loss: 0.0009029692715140664
Epoch 72/100: Training Loss: 0.0014738198488381258
Epoch 73/100: Training Loss: 0.0009622543480745546
Epoch 74/100: Training Loss: 0.0009164445719141869
Epoch 75/100: Training Loss: 0.0013219498714823633
Epoch 76/100: Training Loss: 0.0008003295037397154
Epoch 77/100: Training Loss: 0.0009882936052456024
Epoch 78/100: Training Loss: 0.0007057509319797442
Epoch 79/100: Training Loss: 0.0007990523698223625
Epoch 80/100: Training Loss: 0.0009070034049878454
Epoch 81/100: Training Loss: 0.0008426248837428488
Epoch 82/100: Training Loss: 0.0007227764577622626
Epoch 83/100: Training Loss: 0.0007807188637697013
Epoch 84/100: Training Loss: 0.0006036838623368816
Epoch 85/100: Training Loss: 0.0005958428143695661
Epoch 86/100: Training Loss: 0.0006428411716868164
Epoch 87/100: Training Loss: 0.0006614156114827296
Epoch 88/100: Training Loss: 0.0006903141357336834
Epoch 89/100: Training Loss: 0.0007694110653962299
Epoch 90/100: Training Loss: 0.0008072733499441937
Epoch 91/100: Training Loss: 0.0007503054988612035
Epoch 92/100: Training Loss: 0.00046126425835736997
Epoch 93/100: Training Loss: 0.0008015481719545498
Epoch 94/100: Training Loss: 0.0005494692617920554
Epoch 95/100: Training Loss: 0.0005011023229854122
Epoch 96/100: Training Loss: 0.0005940837180538542
Epoch 97/100: Training Loss: 0.0005414713245288582
Epoch 98/100: Training Loss: 0.0006076338564514355
Epoch 99/100: Training Loss: 0.0004996495546808668
Epoch 0/100: Training Loss: 0.002002104072813775
Epoch 1/100: Training Loss: 0.0018190069563069921
Epoch 2/100: Training Loss: 0.0014829182890570088
Epoch 3/100: Training Loss: 0.0014550397350529957
Epoch 4/100: Training Loss: 0.0022236323280698934
Epoch 5/100: Training Loss: 0.0014871214605440759
Epoch 6/100: Training Loss: 0.0014781386229642637
Epoch 7/100: Training Loss: 0.0023421134538711254
Epoch 8/100: Training Loss: 0.001866847656335041
Epoch 9/100: Training Loss: 0.0016387327081838232
Epoch 10/100: Training Loss: 0.0015297782648900512
Epoch 11/100: Training Loss: 0.0015146577623999043
Epoch 12/100: Training Loss: 0.0008832189687498056
Epoch 13/100: Training Loss: 0.001365719896972559
Epoch 14/100: Training Loss: 0.001366197113778181
Epoch 15/100: Training Loss: 0.0014926056573345402
Epoch 16/100: Training Loss: 0.001250740164404462
Epoch 17/100: Training Loss: 0.0011026312591163976
Epoch 18/100: Training Loss: 0.0013239848765597981
Epoch 19/100: Training Loss: 0.0011109674622298805
Epoch 20/100: Training Loss: 0.0011588269548051676
Epoch 21/100: Training Loss: 0.001511753079997506
Epoch 22/100: Training Loss: 0.001126211634866751
Epoch 23/100: Training Loss: 0.0009979270635896427
Epoch 24/100: Training Loss: 0.0008025921074448118
Epoch 25/100: Training Loss: 0.0011826284751770603
Epoch 26/100: Training Loss: 0.0011195567003480947
Epoch 27/100: Training Loss: 0.000904445720326369
Epoch 28/100: Training Loss: 0.00081222091510797
Epoch 29/100: Training Loss: 0.0004001433492466143
Epoch 30/100: Training Loss: 0.0008995159985912834
Epoch 31/100: Training Loss: 0.0014403541186812578
Epoch 32/100: Training Loss: 0.0008470839375902893
Epoch 33/100: Training Loss: 0.0010122673906338444
Epoch 34/100: Training Loss: 0.000828316162346275
Epoch 35/100: Training Loss: 0.0008454044723207024
Epoch 36/100: Training Loss: 0.0011201334796893368
Epoch 37/100: Training Loss: 0.0008994688273994786
Epoch 38/100: Training Loss: 0.0010958667963173738
Epoch 39/100: Training Loss: 0.0005557178312046513
Epoch 40/100: Training Loss: 0.0009701730339390458
Epoch 41/100: Training Loss: 0.0008165036227293076
Epoch 42/100: Training Loss: 0.0004568018351390863
Epoch 43/100: Training Loss: 0.0011593764944441
Epoch 44/100: Training Loss: 0.0012494879923049052
Epoch 45/100: Training Loss: 0.0009179629717662836
Epoch 46/100: Training Loss: 0.0010177620277283298
Epoch 47/100: Training Loss: 0.0012029265142550134
Epoch 48/100: Training Loss: 0.0007921344345542276
Epoch 49/100: Training Loss: 0.0009369954561731618
Epoch 50/100: Training Loss: 0.0007385828407706728
Epoch 51/100: Training Loss: 0.0004106720636604698
Epoch 52/100: Training Loss: 0.0006447134029333759
Epoch 53/100: Training Loss: 0.001187232269602976
Epoch 54/100: Training Loss: 0.0009072953538530192
Epoch 55/100: Training Loss: 0.0007837208782791332
Epoch 56/100: Training Loss: 0.0006166740682474367
Epoch 57/100: Training Loss: 0.0007796459801637443
Epoch 58/100: Training Loss: 0.0011302686420975217
Epoch 59/100: Training Loss: 0.0008797199483130388
Epoch 60/100: Training Loss: 0.0008417437220834623
Epoch 61/100: Training Loss: 0.0008134961507882282
Epoch 62/100: Training Loss: 0.0007127493525006969
Epoch 63/100: Training Loss: 0.0011668813646219338
Epoch 64/100: Training Loss: 0.000836394205214871
Epoch 65/100: Training Loss: 0.0006328485194285205
Epoch 66/100: Training Loss: 0.0008150189165856428
Epoch 67/100: Training Loss: 0.0007973065611663138
Epoch 68/100: Training Loss: 0.0008943728200948922
Epoch 69/100: Training Loss: 0.0008715594270426756
Epoch 70/100: Training Loss: 0.0010838778155624488
Epoch 71/100: Training Loss: 0.0011245256206791872
Epoch 72/100: Training Loss: 0.0004360422871674702
Epoch 73/100: Training Loss: 0.0007151787637904951
Epoch 74/100: Training Loss: 0.0014827503899859774
Epoch 75/100: Training Loss: 0.0010036362963876907
Epoch 76/100: Training Loss: 0.0009008828242113635
Epoch 77/100: Training Loss: 0.0008652896448305458
Epoch 78/100: Training Loss: 0.0010432309595642575
Epoch 79/100: Training Loss: 0.00039440018546049765
Epoch 80/100: Training Loss: 0.0008325466684475066
Epoch 81/100: Training Loss: 0.0006445120948894768
Epoch 82/100: Training Loss: 0.0005729218861859316
Epoch 83/100: Training Loss: 0.0011580818018336206
Epoch 84/100: Training Loss: 0.0007364513103369695
Epoch 85/100: Training Loss: 0.0010287652539599473
Epoch 86/100: Training Loss: 0.0011385249294293155
Epoch 87/100: Training Loss: 0.0007522252809469867
Epoch 88/100: Training Loss: 0.0007313678313972085
Epoch 89/100: Training Loss: 0.0007511200323985641
Epoch 90/100: Training Loss: 0.0016601791807041046
Epoch 91/100: Training Loss: 0.0007597326188330438
Epoch 92/100: Training Loss: 0.0005715108316415434
Epoch 93/100: Training Loss: 0.0010405404932180029
Epoch 94/100: Training Loss: 0.0027361686821955784
Epoch 95/100: Training Loss: 0.0016199908438761523
Epoch 96/100: Training Loss: 0.0006468309813244328
Epoch 97/100: Training Loss: 0.0014593642038904179
Epoch 98/100: Training Loss: 0.0009475096965291698
Epoch 99/100: Training Loss: 0.0007449949433089822
Epoch 0/100: Training Loss: 0.0027807611503348445
Epoch 1/100: Training Loss: 0.002283413086505915
Epoch 2/100: Training Loss: 0.002418470106377507
Epoch 3/100: Training Loss: 0.002829910508844237
Epoch 4/100: Training Loss: 0.0020004927322564534
Epoch 5/100: Training Loss: 0.002251988807261385
Epoch 6/100: Training Loss: 0.001257776523267986
Epoch 7/100: Training Loss: 0.00220608434929753
Epoch 8/100: Training Loss: 0.0030782139853925894
Epoch 9/100: Training Loss: 0.002167527841416416
Epoch 10/100: Training Loss: 0.001928606964894478
Epoch 11/100: Training Loss: 0.0024140836387280596
Epoch 12/100: Training Loss: 0.002118965845234347
Epoch 13/100: Training Loss: 0.0023231743187304365
Epoch 14/100: Training Loss: 0.0021597752902681464
Epoch 15/100: Training Loss: 0.002054524342745345
Epoch 16/100: Training Loss: 0.0026228254204554275
Epoch 17/100: Training Loss: 0.0024176439307383355
Epoch 18/100: Training Loss: 0.0020475999408999814
Epoch 19/100: Training Loss: 0.0021823279510270683
Epoch 20/100: Training Loss: 0.0021051365808145888
Epoch 21/100: Training Loss: 0.001779319237399575
Epoch 22/100: Training Loss: 0.0017181713849503473
Epoch 23/100: Training Loss: 0.0016320300418020085
Epoch 24/100: Training Loss: 0.0014519885873162982
Epoch 25/100: Training Loss: 0.001680476183922875
Epoch 26/100: Training Loss: 0.001545490018579344
Epoch 27/100: Training Loss: 0.0016464197280391163
Epoch 28/100: Training Loss: 0.0016893004344788608
Epoch 29/100: Training Loss: 0.0011704273965974516
Epoch 30/100: Training Loss: 0.0009403525796157635
Epoch 31/100: Training Loss: 0.0010890491948222483
Epoch 32/100: Training Loss: 0.0007598708500925279
Epoch 33/100: Training Loss: 0.0009626868544824865
Epoch 34/100: Training Loss: 0.0009898178230058278
Epoch 35/100: Training Loss: 0.000796596488810533
Epoch 36/100: Training Loss: 0.0008983830150389513
Epoch 37/100: Training Loss: 0.0007522741196960802
Epoch 38/100: Training Loss: 0.0009939590629362903
Epoch 39/100: Training Loss: 0.0008577540615536519
Epoch 40/100: Training Loss: 0.0006918959467616302
Epoch 41/100: Training Loss: 0.0007290272227186241
Epoch 42/100: Training Loss: 0.001227363549320903
Epoch 43/100: Training Loss: 0.0007714447596215254
Epoch 44/100: Training Loss: 0.0004954862081451921
Epoch 45/100: Training Loss: 0.0006368753333754886
Epoch 46/100: Training Loss: 0.0006483447374097559
Epoch 47/100: Training Loss: 0.0006153536158681705
Epoch 48/100: Training Loss: 0.0007760260278815465
Epoch 49/100: Training Loss: 0.0006765675860524967
Epoch 50/100: Training Loss: 0.0006593092289191997
Epoch 51/100: Training Loss: 0.0005507983810064808
Epoch 52/100: Training Loss: 0.0009043459465961583
Epoch 53/100: Training Loss: 0.0008116709376802507
Epoch 54/100: Training Loss: 0.0006103126813244346
Epoch 55/100: Training Loss: 0.0006218712456178981
Epoch 56/100: Training Loss: 0.0004230553148598071
Epoch 57/100: Training Loss: 0.0007103574986489403
Epoch 58/100: Training Loss: 0.0005015972631656571
Epoch 59/100: Training Loss: 0.0003819829778166007
Epoch 60/100: Training Loss: 0.0007165671381729328
Epoch 61/100: Training Loss: 0.0006766785059543635
Epoch 62/100: Training Loss: 0.0006706421047646479
Epoch 63/100: Training Loss: 0.0005093192225260451
Epoch 64/100: Training Loss: 0.0005210566994370214
Epoch 65/100: Training Loss: 0.0005542923579152846
Epoch 66/100: Training Loss: 0.00040284684853048513
Epoch 67/100: Training Loss: 0.0003238029975370066
Epoch 68/100: Training Loss: 0.00039523849818880197
Epoch 69/100: Training Loss: 0.0006061608822929938
Epoch 70/100: Training Loss: 0.0006392189110351714
Epoch 71/100: Training Loss: 0.00044372638329764864
Epoch 72/100: Training Loss: 0.0005960746218037132
Epoch 73/100: Training Loss: 0.00044647416729011284
Epoch 74/100: Training Loss: 0.0003094514770223605
Epoch 75/100: Training Loss: 0.0002769410659540568
Epoch 76/100: Training Loss: 0.00017577592308158117
Epoch 77/100: Training Loss: 0.0004494773433697934
Epoch 78/100: Training Loss: 0.0005420702478743547
Epoch 79/100: Training Loss: 0.0002853701890304389
Epoch 80/100: Training Loss: 0.0003177472051011016
Epoch 81/100: Training Loss: 0.00046311669199671965
Epoch 82/100: Training Loss: 0.0002558052243775879
Epoch 83/100: Training Loss: 0.0003443595194658696
Epoch 84/100: Training Loss: 0.0002882293863407034
Epoch 85/100: Training Loss: 0.0003515014930671414
Epoch 86/100: Training Loss: 0.00041942357622235026
Epoch 87/100: Training Loss: 0.0005972009423552759
Epoch 88/100: Training Loss: 0.0003131891027191617
Epoch 89/100: Training Loss: 0.0004205401271384283
Epoch 90/100: Training Loss: 0.0003432457563498162
Epoch 91/100: Training Loss: 0.00030326687835699673
Epoch 92/100: Training Loss: 0.0007096799892305539
Epoch 93/100: Training Loss: 0.0008091253061957706
Epoch 94/100: Training Loss: 0.0001958572854664152
Epoch 95/100: Training Loss: 0.00032165656421358224
Epoch 96/100: Training Loss: 0.00027949034871644534
Epoch 97/100: Training Loss: 0.0002579113949608329
Epoch 98/100: Training Loss: 0.00024217722431713382
Epoch 99/100: Training Loss: 0.00024552571753792417
Epoch 0/100: Training Loss: 0.002662575600163037
Epoch 1/100: Training Loss: 0.002401601794539698
Epoch 2/100: Training Loss: 0.002654160095366421
Epoch 3/100: Training Loss: 0.002871101660444247
Epoch 4/100: Training Loss: 0.0019666846619536544
Epoch 5/100: Training Loss: 0.0035239505452036067
Epoch 6/100: Training Loss: 0.0024176845882112616
Epoch 7/100: Training Loss: 0.0026363862666073223
Epoch 8/100: Training Loss: 0.0031299936455606624
Epoch 9/100: Training Loss: 0.003085779235852475
Epoch 10/100: Training Loss: 0.0027161195183431864
Epoch 11/100: Training Loss: 0.0025536011386391344
Epoch 12/100: Training Loss: 0.0024326023281804774
Epoch 13/100: Training Loss: 0.0029963395058714
Epoch 14/100: Training Loss: 0.002628829897634241
Epoch 15/100: Training Loss: 0.0026103056819233674
Epoch 16/100: Training Loss: 0.0026868190986431197
Epoch 17/100: Training Loss: 0.0018176558791406898
Epoch 18/100: Training Loss: 0.0020231934177954466
Epoch 19/100: Training Loss: 0.0020541823068201937
Epoch 20/100: Training Loss: 0.002125358936802441
Epoch 21/100: Training Loss: 0.002274081407003845
Epoch 22/100: Training Loss: 0.0020928779579945747
Epoch 23/100: Training Loss: 0.001832401712998649
Epoch 24/100: Training Loss: 0.0014211552229938128
Epoch 25/100: Training Loss: 0.0013715936648135154
Epoch 26/100: Training Loss: 0.001128850985836509
Epoch 27/100: Training Loss: 0.0012235815161900805
Epoch 28/100: Training Loss: 0.0012648476670119935
Epoch 29/100: Training Loss: 0.001562680234972215
Epoch 30/100: Training Loss: 0.0016318111624938764
Epoch 31/100: Training Loss: 0.001261060107622715
Epoch 32/100: Training Loss: 0.0007235405855620933
Epoch 33/100: Training Loss: 0.0005821287631988525
Epoch 34/100: Training Loss: 0.0008037170234894911
Epoch 35/100: Training Loss: 0.00040667360981568593
Epoch 36/100: Training Loss: 0.0007453725136668477
Epoch 37/100: Training Loss: 0.0009073362445199726
Epoch 38/100: Training Loss: 0.0007765974035326219
Epoch 39/100: Training Loss: 0.0007116650015313104
Epoch 40/100: Training Loss: 0.0008404594383492375
Epoch 41/100: Training Loss: 0.0005011747511017401
Epoch 42/100: Training Loss: 0.000731629054277938
Epoch 43/100: Training Loss: 0.0007145754647570731
Epoch 44/100: Training Loss: 0.0004434142483780716
Epoch 45/100: Training Loss: 0.0007579606396473006
Epoch 46/100: Training Loss: 0.0010581127065696463
Epoch 47/100: Training Loss: 0.000495482606208877
Epoch 48/100: Training Loss: 0.00045735339652623564
Epoch 49/100: Training Loss: 0.000861370504297168
Epoch 50/100: Training Loss: 0.00046020558733024346
Epoch 51/100: Training Loss: 0.0005679804560364477
Epoch 52/100: Training Loss: 0.0007466212013699361
Epoch 53/100: Training Loss: 0.0006742127585095285
Epoch 54/100: Training Loss: 0.0005109781362363045
Epoch 55/100: Training Loss: 0.00031035679657727676
Epoch 56/100: Training Loss: 0.00036116879408722684
Epoch 57/100: Training Loss: 0.0004730914898266066
Epoch 58/100: Training Loss: 0.0005061949128346727
Epoch 59/100: Training Loss: 0.00044745719985456655
Epoch 60/100: Training Loss: 0.0008186320496710721
Epoch 61/100: Training Loss: 0.0005051396441775442
Epoch 62/100: Training Loss: 0.0004331078256992315
Epoch 63/100: Training Loss: 0.00034945581527735223
Epoch 64/100: Training Loss: 0.00036832737113466326
Epoch 65/100: Training Loss: 0.00015814780004766603
Epoch 66/100: Training Loss: 7.583671526995715e-05
Epoch 67/100: Training Loss: 0.0001349096858738274
Epoch 68/100: Training Loss: 0.0002597011625766754
Epoch 69/100: Training Loss: 0.000445376711570664
Epoch 70/100: Training Loss: 0.0003046280895637361
Epoch 71/100: Training Loss: 0.00029705785560292123
Epoch 72/100: Training Loss: 0.00023644661765224886
Epoch 73/100: Training Loss: 0.00037982166800277914
Epoch 74/100: Training Loss: 0.0003129200923521787
Epoch 75/100: Training Loss: 0.0002962805281411733
Epoch 76/100: Training Loss: 0.0003510937588104349
Epoch 77/100: Training Loss: 0.0002490347929742952
Epoch 78/100: Training Loss: 0.00020925397213721118
Epoch 79/100: Training Loss: 0.00035224556429496666
Epoch 80/100: Training Loss: 0.0003069884433651602
Epoch 81/100: Training Loss: 0.0003597668760659679
Epoch 82/100: Training Loss: 0.0003921455450800081
Epoch 83/100: Training Loss: 0.00014532760821825622
Epoch 84/100: Training Loss: 0.0002740184400255317
Epoch 85/100: Training Loss: 0.00028743483866287383
Epoch 86/100: Training Loss: 0.0003258396458152114
Epoch 87/100: Training Loss: 0.00032099304313691244
Epoch 88/100: Training Loss: 0.00021798541115609227
Epoch 89/100: Training Loss: 0.00032043350946824283
Epoch 90/100: Training Loss: 0.00027163368680619245
Epoch 91/100: Training Loss: 0.00024551316010241477
Epoch 92/100: Training Loss: 0.00037073689383386774
Epoch 93/100: Training Loss: 0.0003152285634681878
Epoch 94/100: Training Loss: 0.0004442646507395814
Epoch 95/100: Training Loss: 0.0002751702208392667
Epoch 96/100: Training Loss: 0.0002328990064314659
Epoch 97/100: Training Loss: 0.00039435449420221596
Epoch 98/100: Training Loss: 0.00038069094352374805
Epoch 99/100: Training Loss: 0.00041783517164899816
Epoch 0/100: Training Loss: 0.002787769038156168
Epoch 1/100: Training Loss: 0.0020396405497923593
Epoch 2/100: Training Loss: 0.002207033878920094
Epoch 3/100: Training Loss: 0.0026211947794781616
Epoch 4/100: Training Loss: 0.0034973589789788454
Epoch 5/100: Training Loss: 0.0030920270657697263
Epoch 6/100: Training Loss: 0.0030921816036401204
Epoch 7/100: Training Loss: 0.0026262501217671577
Epoch 8/100: Training Loss: 0.0029677703285848856
Epoch 9/100: Training Loss: 0.0023317062696873747
Epoch 10/100: Training Loss: 0.002233775443588661
Epoch 11/100: Training Loss: 0.003063390783916246
Epoch 12/100: Training Loss: 0.0027732671491357665
Epoch 13/100: Training Loss: 0.0025063204449533626
Epoch 14/100: Training Loss: 0.002062857940496988
Epoch 15/100: Training Loss: 0.0024620789960520155
Epoch 16/100: Training Loss: 0.0021446518945378184
Epoch 17/100: Training Loss: 0.0019105696520268522
Epoch 18/100: Training Loss: 0.0019417466312054766
Epoch 19/100: Training Loss: 0.002085044683999573
Epoch 20/100: Training Loss: 0.002113968331292765
Epoch 21/100: Training Loss: 0.001905655229328484
Epoch 22/100: Training Loss: 0.001915128617886676
Epoch 23/100: Training Loss: 0.0021900400420687845
Epoch 24/100: Training Loss: 0.001685136201365894
Epoch 25/100: Training Loss: 0.001641375832999779
Epoch 26/100: Training Loss: 0.00123220593329297
Epoch 27/100: Training Loss: 0.0017313426298810946
Epoch 28/100: Training Loss: 0.0016626324874675826
Epoch 29/100: Training Loss: 0.0016433702202032733
Epoch 30/100: Training Loss: 0.0014173522690274069
Epoch 31/100: Training Loss: 0.0010374144015722717
Epoch 32/100: Training Loss: 0.0008380358779667229
Epoch 33/100: Training Loss: 0.0008568829850645255
Epoch 34/100: Training Loss: 0.0008592341119879919
Epoch 35/100: Training Loss: 0.0007532194059416158
Epoch 36/100: Training Loss: 0.0010375052887872354
Epoch 37/100: Training Loss: 0.0006083901848224615
Epoch 38/100: Training Loss: 0.0010034672866593923
Epoch 39/100: Training Loss: 0.0006437902122933344
Epoch 40/100: Training Loss: 0.0006383325879936976
Epoch 41/100: Training Loss: 0.0003477096162884441
Epoch 42/100: Training Loss: 0.0005626462844033905
Epoch 43/100: Training Loss: 0.0005192567674529474
Epoch 44/100: Training Loss: 0.0006239123593103017
Epoch 45/100: Training Loss: 0.0011980391496064647
Epoch 46/100: Training Loss: 0.0008100962994114452
Epoch 47/100: Training Loss: 0.0006902299772035207
Epoch 48/100: Training Loss: 0.0008224353983702249
Epoch 49/100: Training Loss: 0.0005488157864437988
Epoch 50/100: Training Loss: 0.00032459715048208934
Epoch 51/100: Training Loss: 0.00048364886384926094
Epoch 52/100: Training Loss: 0.0007501781381518635
Epoch 53/100: Training Loss: 0.0005329431879599363
Epoch 54/100: Training Loss: 0.0006677392401442622
Epoch 55/100: Training Loss: 0.0006458403554973223
Epoch 56/100: Training Loss: 0.0005095596641104742
Epoch 57/100: Training Loss: 0.00031203386799389163
Epoch 58/100: Training Loss: 0.00044753728126058516
Epoch 59/100: Training Loss: 0.00036110038196803716
Epoch 60/100: Training Loss: 0.00040527119837849344
Epoch 61/100: Training Loss: 0.0004171840599830577
Epoch 62/100: Training Loss: 0.0005682247956067521
Epoch 63/100: Training Loss: 0.0009977052740703356
Epoch 64/100: Training Loss: 0.0006302816682303978
Epoch 65/100: Training Loss: 0.00041016265256515403
Epoch 66/100: Training Loss: 0.00046804428890051433
Epoch 67/100: Training Loss: 0.0005664739585080683
Epoch 68/100: Training Loss: 0.0004649442059314804
Epoch 69/100: Training Loss: 0.0003737385156533576
Epoch 70/100: Training Loss: 0.0006049897102330694
Epoch 71/100: Training Loss: 0.0002103484673610586
Epoch 72/100: Training Loss: 0.0002403417170442493
Epoch 73/100: Training Loss: 0.0002235166777838145
Epoch 74/100: Training Loss: 0.00030295624835601707
Epoch 75/100: Training Loss: 0.00022941743893339144
Epoch 76/100: Training Loss: 0.00025718688767477376
Epoch 77/100: Training Loss: 0.0003795911440786147
Epoch 78/100: Training Loss: 0.00026677714179683203
Epoch 79/100: Training Loss: 0.0004810704202051984
Epoch 80/100: Training Loss: 0.0004977753126858086
Epoch 81/100: Training Loss: 0.00030203807531603127
Epoch 82/100: Training Loss: 0.00032228515144215516
Epoch 83/100: Training Loss: 0.00021231157199436465
Epoch 84/100: Training Loss: 0.000297660513824185
Epoch 85/100: Training Loss: 0.00032576898865352405
Epoch 86/100: Training Loss: 0.00024173334734329326
Epoch 87/100: Training Loss: 0.00027762563122029336
Epoch 88/100: Training Loss: 0.00019577752478075345
Epoch 89/100: Training Loss: 0.00017782316746695943
Epoch 90/100: Training Loss: 0.00017164492153174041
Epoch 91/100: Training Loss: 0.00018100650203938516
Epoch 92/100: Training Loss: 0.00027064633685232
Epoch 93/100: Training Loss: 0.00023601778986438222
Epoch 94/100: Training Loss: 0.0002002020471340773
Epoch 95/100: Training Loss: 0.00021006899657628395
Epoch 96/100: Training Loss: 0.00026918910690490775
Epoch 97/100: Training Loss: 0.0003723514987932925
Epoch 98/100: Training Loss: 0.00036576163295089014
Epoch 99/100: Training Loss: 0.0002451031314616172
Epoch 0/100: Training Loss: 0.0003661267678527271
Epoch 1/100: Training Loss: 0.0003024415715652354
Epoch 2/100: Training Loss: 0.00013972265536294263
Epoch 3/100: Training Loss: 0.00013008652364506442
Epoch 4/100: Training Loss: 7.283643216771237e-05
Epoch 5/100: Training Loss: 6.211421397678992e-05
Epoch 6/100: Training Loss: 4.250229993725524e-05
Epoch 7/100: Training Loss: 0.00033300727167550253
Epoch 8/100: Training Loss: 1.9790904651231627e-05
Epoch 9/100: Training Loss: 0.0004893736365963431
Epoch 10/100: Training Loss: 0.00021170848870978636
Epoch 11/100: Training Loss: 0.0001853639369501787
Epoch 12/100: Training Loss: 4.915727083297337e-05
Epoch 13/100: Training Loss: 1.4405194943880334e-05
Epoch 14/100: Training Loss: 0.00039408640826449673
Epoch 15/100: Training Loss: 5.058439119773752e-05
Epoch 16/100: Training Loss: 0.0006323251654120053
Epoch 17/100: Training Loss: 0.0004762521123184877
Epoch 18/100: Training Loss: 2.987589139272185e-05
Epoch 19/100: Training Loss: 0.00032025481409886304
Epoch 20/100: Training Loss: 1.1872246806674144e-05
Epoch 21/100: Training Loss: 0.0010121324483086081
Epoch 22/100: Training Loss: 0.0002450480399762883
Epoch 23/100: Training Loss: 0.0006233040024252499
Epoch 24/100: Training Loss: 0.0006150228573995479
Epoch 25/100: Training Loss: 0.00015479032607639538
Epoch 26/100: Training Loss: 0.0005055386792210972
Epoch 27/100: Training Loss: 0.0006124903612277087
Epoch 28/100: Training Loss: 0.0006033669061520521
Epoch 29/100: Training Loss: 0.0004989190574954538
Epoch 30/100: Training Loss: 0.00012668770025758182
Epoch 31/100: Training Loss: 0.0005658607710810269
Epoch 32/100: Training Loss: 0.0005093849757138421
Epoch 33/100: Training Loss: 8.796063963981235e-05
Epoch 34/100: Training Loss: 6.639654474223361e-05
Epoch 35/100: Training Loss: 0.0001810440793633461
Epoch 36/100: Training Loss: 0.0001641944160356241
Epoch 37/100: Training Loss: 0.00010930601069155861
Epoch 38/100: Training Loss: 0.0002126485328463947
Epoch 39/100: Training Loss: 0.0005448672263061299
Epoch 40/100: Training Loss: 1.3518443002420314e-05
Epoch 41/100: Training Loss: 0.0004948439405244939
Epoch 42/100: Training Loss: 0.000525414987522013
Epoch 43/100: Training Loss: 0.0005285604911692002
Epoch 44/100: Training Loss: 0.0005264964612091289
Epoch 45/100: Training Loss: 5.393079139620942e-06
Epoch 46/100: Training Loss: 3.9924568880130265e-05
Epoch 47/100: Training Loss: 0.00037021851714919593
Epoch 48/100: Training Loss: 1.1738031068542862e-06
Epoch 49/100: Training Loss: 0.00046061281772220836
Epoch 50/100: Training Loss: 2.204195788020597e-05
Epoch 51/100: Training Loss: 3.8852183805669056e-05
Epoch 52/100: Training Loss: 0.0005179211059037377
Epoch 53/100: Training Loss: 0.0004553626127102796
Epoch 54/100: Training Loss: 1.5344258452601292e-06
Epoch 55/100: Training Loss: 0.0005280455683960634
Epoch 56/100: Training Loss: 0.0006187094923327951
Epoch 57/100: Training Loss: 0.0004174637882148518
Epoch 58/100: Training Loss: 0.0003351727610125261
Epoch 59/100: Training Loss: 0.0003666306243223303
Epoch 60/100: Training Loss: 0.0003466033102834926
Epoch 61/100: Training Loss: 0.0005297848406959982
Epoch 62/100: Training Loss: 3.899014007081004e-05
Epoch 63/100: Training Loss: 7.730475900804296e-06
Epoch 64/100: Training Loss: 0.0004775247591383317
Epoch 65/100: Training Loss: 3.897541421739494e-05
Epoch 66/100: Training Loss: 0.00036434599143617293
Epoch 67/100: Training Loss: 0.00048130733125350053
Epoch 68/100: Training Loss: 0.00042984362910775576
Epoch 69/100: Training Loss: 0.00021409539177137262
Epoch 70/100: Training Loss: 2.8861436636789756e-06
Epoch 71/100: Training Loss: 5.741568610948675e-05
Epoch 72/100: Training Loss: 0.0005436065880691304
Epoch 73/100: Training Loss: 0.0004666299504392287
Epoch 74/100: Training Loss: 0.0004369723884498372
Epoch 75/100: Training Loss: 0.00044884475715020126
Epoch 76/100: Training Loss: 0.0005322675056317274
Epoch 77/100: Training Loss: 1.166913630988668e-05
Epoch 78/100: Training Loss: 0.00046663069549728845
Epoch 79/100: Training Loss: 0.0005132878527921789
Epoch 80/100: Training Loss: 0.00030112750828266145
Epoch 81/100: Training Loss: 0.0005374207654420067
Epoch 82/100: Training Loss: 0.0005185387152082779
Epoch 83/100: Training Loss: 8.941210998112665e-06
Epoch 84/100: Training Loss: 0.0004642979625393363
Epoch 85/100: Training Loss: 0.0002911820131189683
Epoch 86/100: Training Loss: 8.73331692727173e-05
Epoch 87/100: Training Loss: 0.0004975151051493253
Epoch 88/100: Training Loss: 0.000456017825533362
Epoch 89/100: Training Loss: 0.00022181539851076463
Epoch 90/100: Training Loss: 1.2608681914999204e-05
Epoch 91/100: Training Loss: 0.0005109652000315049
Epoch 92/100: Training Loss: 0.0032059122534359203
Epoch 93/100: Training Loss: 0.0001313222133938004
Epoch 94/100: Training Loss: 7.320387864156681e-06
Epoch 95/100: Training Loss: 0.0002733895883840673
Epoch 96/100: Training Loss: 1.0112371049163973e-05
Epoch 97/100: Training Loss: 0.0003828671048669254
Epoch 98/100: Training Loss: 0.00038369613535263957
Epoch 99/100: Training Loss: 2.158505787305972e-06
Epoch 0/100: Training Loss: 0.00040786003365236173
Epoch 1/100: Training Loss: 0.0003003344816320083
Epoch 2/100: Training Loss: 0.00019787525867714603
Epoch 3/100: Training Loss: 0.0003773563924957724
Epoch 4/100: Training Loss: 4.988581709125463e-05
Epoch 5/100: Training Loss: 0.0006794395253938787
Epoch 6/100: Training Loss: 0.000341215094222742
Epoch 7/100: Training Loss: 0.00022203810513019562
Epoch 8/100: Training Loss: 0.0006296280990628635
Epoch 9/100: Training Loss: 0.0006454640013330123
Epoch 10/100: Training Loss: 0.00048771364723934846
Epoch 11/100: Training Loss: 0.0005754477837506463
Epoch 12/100: Training Loss: 0.0004670570878421559
Epoch 13/100: Training Loss: 0.0006735761814257678
Epoch 14/100: Training Loss: 1.456636853296967e-05
Epoch 15/100: Training Loss: 0.0003292397760293063
Epoch 16/100: Training Loss: 0.0001253550872206688
Epoch 17/100: Training Loss: 0.0007563789977746852
Epoch 18/100: Training Loss: 1.6578632023404627e-05
Epoch 19/100: Training Loss: 0.0006404531352660236
Epoch 20/100: Training Loss: 0.0006614351535544676
Epoch 21/100: Training Loss: 2.245652242837583e-06
Epoch 22/100: Training Loss: 2.6109496898510877e-05
Epoch 23/100: Training Loss: 0.0006155190660673029
Epoch 24/100: Training Loss: 0.00044118012575542224
Epoch 25/100: Training Loss: 0.0004939651226296144
Epoch 26/100: Training Loss: 0.0005624082596863018
Epoch 27/100: Training Loss: 2.4829451542566805e-05
Epoch 28/100: Training Loss: 0.00038240587010103115
Epoch 29/100: Training Loss: 1.5419407966820634e-05
Epoch 30/100: Training Loss: 0.0006603773464174832
Epoch 31/100: Training Loss: 0.0005449209581403171
Epoch 32/100: Training Loss: 0.0005735490251989925
Epoch 33/100: Training Loss: 0.000564800071365693
Epoch 34/100: Training Loss: 0.00025616058093660016
Epoch 35/100: Training Loss: 4.62959410951418e-05
Epoch 36/100: Training Loss: 0.0006567454075112062
Epoch 37/100: Training Loss: 9.839472306125305e-06
Epoch 38/100: Training Loss: 0.000360375818084268
Epoch 39/100: Training Loss: 0.0005842765026232775
Epoch 40/100: Training Loss: 0.00013605793390203925
Epoch 41/100: Training Loss: 0.0005723524181281819
Epoch 42/100: Training Loss: 3.473331658717464e-05
Epoch 43/100: Training Loss: 6.892988441155895e-07
Epoch 44/100: Training Loss: 0.0006046650602537043
Epoch 45/100: Training Loss: 0.00040176208404933705
Epoch 46/100: Training Loss: 0.0004989585455726174
Epoch 47/100: Training Loss: 1.7921773114186876e-05
Epoch 48/100: Training Loss: 8.499014930909171e-06
Epoch 49/100: Training Loss: 0.000352897823733442
Epoch 50/100: Training Loss: 0.0005818888983305763
Epoch 51/100: Training Loss: 0.0006015666705720565
Epoch 52/100: Training Loss: 1.8310837228508556e-05
Epoch 53/100: Training Loss: 0.0005774238530327292
Epoch 54/100: Training Loss: 9.654625592862858e-05
Epoch 55/100: Training Loss: 5.1389546955333034e-05
Epoch 56/100: Training Loss: 4.49298120870748e-06
Epoch 57/100: Training Loss: 0.0004176411120330586
Epoch 58/100: Training Loss: 0.00011733726543538711
Epoch 59/100: Training Loss: 0.0003244959037093555
Epoch 60/100: Training Loss: 0.0001354795606697307
Epoch 61/100: Training Loss: 0.0006976284963243148
Epoch 62/100: Training Loss: 6.834520574878244e-05
Epoch 63/100: Training Loss: 0.0003800637143499711
Epoch 64/100: Training Loss: 0.0005370508222018971
Epoch 65/100: Training Loss: 2.780069925767534e-05
Epoch 66/100: Training Loss: 1.0084439589477638e-05
Epoch 67/100: Training Loss: 0.0004882819074041703
Epoch 68/100: Training Loss: 8.974120240001118e-06
Epoch 69/100: Training Loss: 1.7552794066860396e-05
Epoch 70/100: Training Loss: 0.000611938799128813
Epoch 71/100: Training Loss: 0.0005623496192343095
Epoch 72/100: Training Loss: 8.960742612971979e-05
Epoch 73/100: Training Loss: 0.0005596555331174065
Epoch 74/100: Training Loss: 0.0003336732878404505
Epoch 75/100: Training Loss: 4.669937359936097e-05
Epoch 76/100: Training Loss: 0.00014055289108963573
Epoch 77/100: Training Loss: 4.266552791437682e-05
Epoch 78/100: Training Loss: 1.6904170693391386e-06
Epoch 79/100: Training Loss: 0.0005357222083736869
Epoch 80/100: Training Loss: 3.2982161706861325e-05
Epoch 81/100: Training Loss: 0.0006014687611776239
Epoch 82/100: Training Loss: 2.3696544196675805e-05
Epoch 83/100: Training Loss: 3.1430337249356157e-05
Epoch 84/100: Training Loss: 0.000174123136436238
Epoch 85/100: Training Loss: 0.0007306288270389332
Epoch 86/100: Training Loss: 5.618411266957136e-06
Epoch 87/100: Training Loss: 0.00019430215744411245
Epoch 88/100: Training Loss: 4.09777310457738e-06
Epoch 89/100: Training Loss: 4.82993885217344e-06
Epoch 90/100: Training Loss: 2.44242769172963e-05
Epoch 91/100: Training Loss: 0.0002778846770524979
Epoch 92/100: Training Loss: 0.0005484661635230569
Epoch 93/100: Training Loss: 0.000394765506772434
Epoch 94/100: Training Loss: 0.0004912704229354858
Epoch 95/100: Training Loss: 0.00047922449953415817
Epoch 96/100: Training Loss: 0.00042985958211562214
Epoch 97/100: Training Loss: 0.000309226442785824
Epoch 98/100: Training Loss: 0.00018153094193514656
Epoch 99/100: Training Loss: 0.00011715093518004698
Epoch 0/100: Training Loss: 0.0002910942058352863
Epoch 1/100: Training Loss: 9.709729210418814e-05
Epoch 2/100: Training Loss: 4.593933768132154e-05
Epoch 3/100: Training Loss: 0.00015447057564468944
Epoch 4/100: Training Loss: 0.0005486165337702807
Epoch 5/100: Training Loss: 0.0001489874523352174
Epoch 6/100: Training Loss: 1.956979908487376e-05
Epoch 7/100: Training Loss: 0.00022613329484182245
Epoch 8/100: Training Loss: 9.256477522499421e-05
Epoch 9/100: Training Loss: 0.0002635672031080022
Epoch 10/100: Training Loss: 0.00023690970504985136
Epoch 11/100: Training Loss: 3.214174717226449e-05
Epoch 12/100: Training Loss: 0.0007380907149875866
Epoch 13/100: Training Loss: 2.5884472929379518e-05
Epoch 14/100: Training Loss: 0.0006455030073137844
Epoch 15/100: Training Loss: 0.0006794519284192254
Epoch 16/100: Training Loss: 4.85620680539047e-05
Epoch 17/100: Training Loss: 0.0010588331257595735
Epoch 18/100: Training Loss: 7.274446680265314e-05
Epoch 19/100: Training Loss: 0.00015410551910891252
Epoch 20/100: Training Loss: 0.0007443229941760792
Epoch 21/100: Training Loss: 0.000913262104286867
Epoch 22/100: Training Loss: 0.0005117763929507311
Epoch 23/100: Training Loss: 0.0005481823840562035
Epoch 24/100: Training Loss: 3.639107898754232e-05
Epoch 25/100: Training Loss: 0.00044123543535961826
Epoch 26/100: Training Loss: 0.0001428294159910258
Epoch 27/100: Training Loss: 0.0004140400711227866
Epoch 28/100: Training Loss: 0.00038717787931947145
Epoch 29/100: Training Loss: 4.916375722078716e-05
Epoch 30/100: Training Loss: 0.0006800241768360138
Epoch 31/100: Training Loss: 0.0002730801044141545
Epoch 32/100: Training Loss: 0.0005897141554776361
Epoch 33/100: Training Loss: 9.38620966146974e-05
Epoch 34/100: Training Loss: 1.019882449113271e-06
Epoch 35/100: Training Loss: 0.0006889264373218312
Epoch 36/100: Training Loss: 0.0008799724719103644
Epoch 37/100: Training Loss: 0.00018407726550803466
Epoch 38/100: Training Loss: 0.0005059520549633924
Epoch 39/100: Training Loss: 0.0006435644977232989
Epoch 40/100: Training Loss: 3.838979104972061e-06
Epoch 41/100: Training Loss: 0.0005950866376652437
Epoch 42/100: Training Loss: 0.0005688556415193221
Epoch 43/100: Training Loss: 0.0005538215970291811
Epoch 44/100: Training Loss: 9.89711021675783e-06
Epoch 45/100: Training Loss: 3.065056893426706e-06
Epoch 46/100: Training Loss: 0.00045634499367545634
Epoch 47/100: Training Loss: 0.000396467131726882
Epoch 48/100: Training Loss: 3.0458353542010575e-06
Epoch 49/100: Training Loss: 5.991885657696163e-05
Epoch 50/100: Training Loss: 0.0006896483985816732
Epoch 51/100: Training Loss: 8.599729249801706e-06
Epoch 52/100: Training Loss: 0.0005687257384552675
Epoch 53/100: Training Loss: 0.0005818533108514897
Epoch 54/100: Training Loss: 0.00046750609489048226
Epoch 55/100: Training Loss: 0.00045210675281636857
Epoch 56/100: Training Loss: 5.2081349798861675e-05
Epoch 57/100: Training Loss: 0.0002961722147815368
Epoch 58/100: Training Loss: 8.536426569609081e-05
Epoch 59/100: Training Loss: 0.000483007772880442
Epoch 60/100: Training Loss: 0.0002519236767993254
Epoch 61/100: Training Loss: 5.117558312657125e-06
Epoch 62/100: Training Loss: 4.6458046006805754e-05
Epoch 63/100: Training Loss: 0.0003288240336319979
Epoch 64/100: Training Loss: 0.000644425960148082
Epoch 65/100: Training Loss: 0.0003218750085900812
Epoch 66/100: Training Loss: 7.716493461938465e-05
Epoch 67/100: Training Loss: 1.1313523795893964e-05
Epoch 68/100: Training Loss: 7.02675705885186e-05
Epoch 69/100: Training Loss: 0.0006243470398818745
Epoch 70/100: Training Loss: 1.3804753988926464e-06
Epoch 71/100: Training Loss: 0.000552911136080237
Epoch 72/100: Training Loss: 7.180115755866555e-05
Epoch 73/100: Training Loss: 0.0005394338246654061
Epoch 74/100: Training Loss: 0.0004232444307383369
Epoch 75/100: Training Loss: 0.00048738573403919446
Epoch 76/100: Training Loss: 1.3249648070675047e-06
Epoch 77/100: Training Loss: 0.0004598324789720423
Epoch 78/100: Training Loss: 4.420115831581985e-05
Epoch 79/100: Training Loss: 0.0005912626052604002
Epoch 80/100: Training Loss: 1.0534559431321481e-05
Epoch 81/100: Training Loss: 0.00024909157963360056
Epoch 82/100: Training Loss: 0.0005425888387595906
Epoch 83/100: Training Loss: 0.0005139839999815997
Epoch 84/100: Training Loss: 0.00030144512215081384
Epoch 85/100: Training Loss: 5.32291699474787e-06
Epoch 86/100: Training Loss: 0.0002486426383256912
Epoch 87/100: Training Loss: 0.0001926940372761558
Epoch 88/100: Training Loss: 0.0001503108288435375
Epoch 89/100: Training Loss: 0.0005704064579570995
Epoch 90/100: Training Loss: 0.00019690365914036247
Epoch 91/100: Training Loss: 1.1392764281481505e-05
Epoch 92/100: Training Loss: 0.00031116972951328054
Epoch 93/100: Training Loss: 0.0005279353874571184
Epoch 94/100: Training Loss: 2.6337276829187484e-06
Epoch 95/100: Training Loss: 0.00037574478808571313
Epoch 96/100: Training Loss: 9.263830725103616e-06
Epoch 97/100: Training Loss: 0.00031509530894896563
Epoch 98/100: Training Loss: 1.3504181611899505e-06
Epoch 99/100: Training Loss: 0.00014890444410197874
dataset: capitals layer_num_from_end:-1 Avg_acc:tensor(0.8066) Avg_AUC:0.8868739270524229 Avg_threshold:0.668887178103129
dataset: inventions layer_num_from_end:-1 Avg_acc:tensor(0.5400) Avg_AUC:0.6886089722129228 Avg_threshold:0.8640434145927429
dataset: elements layer_num_from_end:-1 Avg_acc:tensor(0.5814) Avg_AUC:0.6330489073881375 Avg_threshold:0.4979040026664734
dataset: animals layer_num_from_end:-1 Avg_acc:tensor(0.5648) Avg_AUC:0.6703586913160327 Avg_threshold:0.7245890696843466
dataset: companies layer_num_from_end:-1 Avg_acc:tensor(0.6850) Avg_AUC:0.8261925925925926 Avg_threshold:0.49264801541964215
dataset: facts layer_num_from_end:-1 Avg_acc:tensor(0.6443) Avg_AUC:0.7868483631651643 Avg_threshold:0.9375978906949362
