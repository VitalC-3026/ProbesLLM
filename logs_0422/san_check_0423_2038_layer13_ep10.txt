2024-04-24 00:45:44.645988: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-04-24 00:45:45.705302: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
================layer -13================
Epoch 0/10: Training Loss: 0.002434969781995653
Epoch 1/10: Training Loss: 0.0017308170770431732
Epoch 2/10: Training Loss: 0.002217646870579753
Epoch 3/10: Training Loss: 0.00151502054471236
Epoch 4/10: Training Loss: 0.0017044887050882086
Epoch 5/10: Training Loss: 0.0012652522825694584
Epoch 6/10: Training Loss: 0.0013177974240763204
Epoch 7/10: Training Loss: 0.00113950471778016
Epoch 8/10: Training Loss: 0.0008381064850967248
Epoch 9/10: Training Loss: 0.0011464560573751276
Epoch 0/10: Training Loss: 0.002336794679815119
Epoch 1/10: Training Loss: 0.0025691865207432035
Epoch 2/10: Training Loss: 0.0020329700066493107
Epoch 3/10: Training Loss: 0.0019992823367352254
Epoch 4/10: Training Loss: 0.0014368382158812944
Epoch 5/10: Training Loss: 0.00176294283433394
Epoch 6/10: Training Loss: 0.0010474707696821305
Epoch 7/10: Training Loss: 0.0009027458987869583
Epoch 8/10: Training Loss: 0.0015180754911649477
Epoch 9/10: Training Loss: 0.001443195384699148
Epoch 0/10: Training Loss: 0.0025046034292741256
Epoch 1/10: Training Loss: 0.00255930652985206
Epoch 2/10: Training Loss: 0.0019696748756862187
Epoch 3/10: Training Loss: 0.0012887412017875619
Epoch 4/10: Training Loss: 0.002177875150333751
Epoch 5/10: Training Loss: 0.0015414145651397172
Epoch 6/10: Training Loss: 0.0013452647449253322
Epoch 7/10: Training Loss: 0.0016788215070337683
Epoch 8/10: Training Loss: 0.0009380583162907954
Epoch 9/10: Training Loss: 0.0012064301050626314
Epoch 0/10: Training Loss: 0.0023602303917422615
Epoch 1/10: Training Loss: 0.0017147460963828432
Epoch 2/10: Training Loss: 0.0022950501529717007
Epoch 3/10: Training Loss: 0.0020921062106735136
Epoch 4/10: Training Loss: 0.0016976699141636948
Epoch 5/10: Training Loss: 0.0014839361598886595
Epoch 6/10: Training Loss: 0.0010487729786363847
Epoch 7/10: Training Loss: 0.0009274600656485997
Epoch 8/10: Training Loss: 0.000760170762524283
Epoch 9/10: Training Loss: 0.00048015166096892093
Epoch 0/10: Training Loss: 0.0024313180724535983
Epoch 1/10: Training Loss: 0.0014979005591269651
Epoch 2/10: Training Loss: 0.0018440673322034028
Epoch 3/10: Training Loss: 0.002173983794779865
Epoch 4/10: Training Loss: 0.001738234714496355
Epoch 5/10: Training Loss: 0.0012162416442040285
Epoch 6/10: Training Loss: 0.0009240832614021067
Epoch 7/10: Training Loss: 0.000719964138569276
Epoch 8/10: Training Loss: 0.0006967671444079627
Epoch 9/10: Training Loss: 0.0006330042528959871
Epoch 0/10: Training Loss: 0.002375901111064513
Epoch 1/10: Training Loss: 0.00148978948227467
Epoch 2/10: Training Loss: 0.001999345293805643
Epoch 3/10: Training Loss: 0.0022688049114554937
Epoch 4/10: Training Loss: 0.0014202882724305603
Epoch 5/10: Training Loss: 0.001364656844022084
Epoch 6/10: Training Loss: 0.0011279206334447569
Epoch 7/10: Training Loss: 0.0006418032514537039
Epoch 8/10: Training Loss: 0.0005725117548843103
Epoch 9/10: Training Loss: 0.0005356710107048597
Epoch 0/10: Training Loss: 0.0026942690834403037
Epoch 1/10: Training Loss: 0.0017841186374425888
Epoch 2/10: Training Loss: 0.0018773237243294716
Epoch 3/10: Training Loss: 0.001667972467839718
Epoch 4/10: Training Loss: 0.0019923433661460876
Epoch 5/10: Training Loss: 0.0019151320680975913
Epoch 6/10: Training Loss: 0.0015178756788372994
Epoch 7/10: Training Loss: 0.001768278516829014
Epoch 8/10: Training Loss: 0.0013763546943664552
Epoch 9/10: Training Loss: 0.0009924673475325107
Epoch 0/10: Training Loss: 0.002997547946870327
Epoch 1/10: Training Loss: 0.0015171362087130547
Epoch 2/10: Training Loss: 0.0009449703618884086
Epoch 3/10: Training Loss: 0.001257552020251751
Epoch 4/10: Training Loss: 0.0019388781860470773
Epoch 5/10: Training Loss: 0.0015334338881075383
Epoch 6/10: Training Loss: 0.0017995068803429604
Epoch 7/10: Training Loss: 0.0009042123332619667
Epoch 8/10: Training Loss: 0.001139067579060793
Epoch 9/10: Training Loss: 0.0011825442314147949
Epoch 0/10: Training Loss: 0.0028266875073313714
Epoch 1/10: Training Loss: 0.0012955856509506702
Epoch 2/10: Training Loss: 0.0010218781419098378
Epoch 3/10: Training Loss: 0.0018133655190467834
Epoch 4/10: Training Loss: 0.0015243342146277427
Epoch 5/10: Training Loss: 0.0018294230103492737
Epoch 6/10: Training Loss: 0.0014981135725975036
Epoch 7/10: Training Loss: 0.001770859770476818
Epoch 8/10: Training Loss: 0.001427055336534977
Epoch 9/10: Training Loss: 0.0013499426655471324
Epoch 0/10: Training Loss: 0.0012479072353642458
Epoch 1/10: Training Loss: 0.0008283260331791677
Epoch 2/10: Training Loss: 0.0005731505288439951
Epoch 3/10: Training Loss: 0.001254502375414417
Epoch 4/10: Training Loss: 0.0012017358450373268
Epoch 5/10: Training Loss: 0.0013053692450189288
Epoch 6/10: Training Loss: 0.0010167631753690684
Epoch 7/10: Training Loss: 0.0014950062628764256
Epoch 8/10: Training Loss: 0.0012791247884179377
Epoch 9/10: Training Loss: 0.0013273515898710603
Epoch 0/10: Training Loss: 0.0012420681631489165
Epoch 1/10: Training Loss: 0.0008203439461957118
Epoch 2/10: Training Loss: 0.0006475910354571738
Epoch 3/10: Training Loss: 0.0013752301597291497
Epoch 4/10: Training Loss: 0.001340619697692288
Epoch 5/10: Training Loss: 0.0009952122048967203
Epoch 6/10: Training Loss: 0.0008761149113345298
Epoch 7/10: Training Loss: 0.0009823077043909936
Epoch 8/10: Training Loss: 0.0010804616531748682
Epoch 9/10: Training Loss: 0.0009770895455293595
Epoch 0/10: Training Loss: 0.0013633395076557329
Epoch 1/10: Training Loss: 0.0009416599941861098
Epoch 2/10: Training Loss: 0.0008717210619312943
Epoch 3/10: Training Loss: 0.0008215212328418805
Epoch 4/10: Training Loss: 0.0008318829498473246
Epoch 5/10: Training Loss: 0.001365670352984386
Epoch 6/10: Training Loss: 0.0008245446499745557
Epoch 7/10: Training Loss: 0.0010685811566699083
Epoch 8/10: Training Loss: 0.0012257702791007461
Epoch 9/10: Training Loss: 0.0011799712279799638
Epoch 0/10: Training Loss: 0.002638225918574049
Epoch 1/10: Training Loss: 0.0021341062144727896
Epoch 2/10: Training Loss: 0.0010878588782241012
Epoch 3/10: Training Loss: 0.0018542677361444132
Epoch 4/10: Training Loss: 0.0016270429882782184
Epoch 5/10: Training Loss: 0.001768672111018604
Epoch 6/10: Training Loss: 0.001995485152629827
Epoch 7/10: Training Loss: 0.0020870436106296564
Epoch 8/10: Training Loss: 0.0018505355380228814
Epoch 9/10: Training Loss: 0.0016738857259813522
Epoch 0/10: Training Loss: 0.0023545449143213943
Epoch 1/10: Training Loss: 0.0024957552256173647
Epoch 2/10: Training Loss: 0.00246690164338674
Epoch 3/10: Training Loss: 0.0021927449482166215
Epoch 4/10: Training Loss: 0.0022193732245868406
Epoch 5/10: Training Loss: 0.002018105510054835
Epoch 6/10: Training Loss: 0.0017994523837866372
Epoch 7/10: Training Loss: 0.002215061756159296
Epoch 8/10: Training Loss: 0.0021540001528152566
Epoch 9/10: Training Loss: 0.002066288562799921
Epoch 0/10: Training Loss: 0.0020696662909147757
Epoch 1/10: Training Loss: 0.002024527811846196
Epoch 2/10: Training Loss: 0.001344704470097624
Epoch 3/10: Training Loss: 0.00189682821564327
Epoch 4/10: Training Loss: 0.0020170422974011754
Epoch 5/10: Training Loss: 0.0024268417958392212
Epoch 6/10: Training Loss: 0.001927600396389993
Epoch 7/10: Training Loss: 0.0018838113112165438
Epoch 8/10: Training Loss: 0.002152345235774059
Epoch 9/10: Training Loss: 0.0019019425704779214
Epoch 0/10: Training Loss: 6.160686230834793e-05
Epoch 1/10: Training Loss: 0.00013329475460683598
Epoch 2/10: Training Loss: 0.00013489329858737834
Epoch 3/10: Training Loss: 0.0007128423189415651
Epoch 4/10: Training Loss: 0.0001753390273627113
Epoch 5/10: Training Loss: 0.00044187557171372806
Epoch 6/10: Training Loss: 0.0005019890473169439
Epoch 7/10: Training Loss: 0.0001071593669407508
Epoch 8/10: Training Loss: 0.0006251609062447268
Epoch 9/10: Training Loss: 0.0008970613865291371
Epoch 0/10: Training Loss: 0.00016679453718311646
Epoch 1/10: Training Loss: 0.0002990316818742191
Epoch 2/10: Training Loss: 0.0004826373037169961
Epoch 3/10: Training Loss: 8.361238767119015e-05
Epoch 4/10: Training Loss: 0.0004430005217299742
Epoch 5/10: Training Loss: 0.00022904270273797653
Epoch 6/10: Training Loss: 9.518071789951885e-05
Epoch 7/10: Training Loss: 0.0004919336560894461
Epoch 8/10: Training Loss: 0.00015559086904806248
Epoch 9/10: Training Loss: 0.0007690113256959354
Epoch 0/10: Training Loss: 0.00011104145909056944
Epoch 1/10: Training Loss: 0.00010217138949562521
Epoch 2/10: Training Loss: 0.00011651939986383214
Epoch 3/10: Training Loss: 0.0001997743240174125
Epoch 4/10: Training Loss: 0.0002412158338462605
Epoch 5/10: Training Loss: 0.0007848845685229582
Epoch 6/10: Training Loss: 0.00016743121120859594
Epoch 7/10: Training Loss: 0.00045378519331707676
Epoch 8/10: Training Loss: 0.0005102556856239544
Epoch 9/10: Training Loss: 0.0004955710295368644
dataset: capitals layer_num_from_end:-13 Avg_acc:tensor(0.8466) Avg_AUC:0.9281397182377725 Avg_threshold:0.42353739341100055
dataset: inventions layer_num_from_end:-13 Avg_acc:tensor(0.7371) Avg_AUC:0.8253299017966745 Avg_threshold:0.46236398816108704
dataset: elements layer_num_from_end:-13 Avg_acc:tensor(0.5975) Avg_AUC:0.6774440205033337 Avg_threshold:0.4344756503899892
dataset: animals layer_num_from_end:-13 Avg_acc:tensor(0.5784) Avg_AUC:0.7033684229864786 Avg_threshold:0.47761977712313336
dataset: companies layer_num_from_end:-13 Avg_acc:tensor(0.8244) Avg_AUC:0.9041370370370371 Avg_threshold:0.5581526458263397
dataset: facts layer_num_from_end:-13 Avg_acc:tensor(0.6389) Avg_AUC:0.7310987939111738 Avg_threshold:0.7396570046742758


