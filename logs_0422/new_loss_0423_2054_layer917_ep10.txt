2024-04-24 00:54:56.598240: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-04-24 00:54:57.683091: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
================layer -9================
Epoch 0/10: Training Loss: 0.003875689489858134
Epoch 1/10: Training Loss: 0.004050670803843678
Epoch 2/10: Training Loss: 0.0035580142394646065
Epoch 3/10: Training Loss: 0.004466016392607789
Epoch 4/10: Training Loss: 0.004041911005140185
Epoch 5/10: Training Loss: 0.0036551077049095312
Epoch 6/10: Training Loss: 0.003472535343436928
Epoch 7/10: Training Loss: 0.003623565593799511
Epoch 8/10: Training Loss: 0.003958828799374454
Epoch 9/10: Training Loss: 0.0022058065954621855
Epoch 0/10: Training Loss: 0.0031049372432948824
Epoch 1/10: Training Loss: 0.0032682366721279972
Epoch 2/10: Training Loss: 0.004847767886581955
Epoch 3/10: Training Loss: 0.00429949893818035
Epoch 4/10: Training Loss: 0.0030340285567970545
Epoch 5/10: Training Loss: 0.002870511341761876
Epoch 6/10: Training Loss: 0.004475880752910267
Epoch 7/10: Training Loss: 0.004020304529816954
Epoch 8/10: Training Loss: 0.003286691187144993
Epoch 9/10: Training Loss: 0.0022727155185245968
Epoch 0/10: Training Loss: 0.0034244443986799333
Epoch 1/10: Training Loss: 0.002740471513121278
Epoch 2/10: Training Loss: 0.0042385746549059465
Epoch 3/10: Training Loss: 0.0037415377743594295
Epoch 4/10: Training Loss: 0.004404416034271667
Epoch 5/10: Training Loss: 0.0034813864247782247
Epoch 6/10: Training Loss: 0.003168978057541214
Epoch 7/10: Training Loss: 0.0037395233040923006
Epoch 8/10: Training Loss: 0.0029390160437230465
Epoch 9/10: Training Loss: 0.0028215288282274365
Epoch 0/10: Training Loss: 0.004053075255060488
Epoch 1/10: Training Loss: 0.004097336640387225
Epoch 2/10: Training Loss: 0.0038534775833410717
Epoch 3/10: Training Loss: 0.0035645610715713968
Epoch 4/10: Training Loss: 0.0029849347892714423
Epoch 5/10: Training Loss: 0.0017740057289966046
Epoch 6/10: Training Loss: 0.0014623344127385894
Epoch 7/10: Training Loss: 0.0012754362785011714
Epoch 8/10: Training Loss: 0.0009170405886655936
Epoch 9/10: Training Loss: 0.0010006435992527592
Epoch 0/10: Training Loss: 0.0042412105513496634
Epoch 1/10: Training Loss: 0.003368138901295106
Epoch 2/10: Training Loss: 0.0034722444469943367
Epoch 3/10: Training Loss: 0.0028396852177344947
Epoch 4/10: Training Loss: 0.002673446766437928
Epoch 5/10: Training Loss: 0.0020933409044347658
Epoch 6/10: Training Loss: 0.001971642663873778
Epoch 7/10: Training Loss: 0.001972044903807845
Epoch 8/10: Training Loss: 0.0011364934634577276
Epoch 9/10: Training Loss: 0.0016737633321914204
Epoch 0/10: Training Loss: 0.004008705264951554
Epoch 1/10: Training Loss: 0.0034111518801355654
Epoch 2/10: Training Loss: 0.0034988679768849004
Epoch 3/10: Training Loss: 0.00336208482461473
Epoch 4/10: Training Loss: 0.002091578727850885
Epoch 5/10: Training Loss: 0.0023558779728193226
Epoch 6/10: Training Loss: 0.0012847410023578106
Epoch 7/10: Training Loss: 0.0011816898737948365
Epoch 8/10: Training Loss: 0.0008565328238200556
Epoch 9/10: Training Loss: 0.0014680035275184305
Epoch 0/10: Training Loss: 0.0031570717692375183
Epoch 1/10: Training Loss: 0.0031357303261756895
Epoch 2/10: Training Loss: 0.0028055042028427126
Epoch 3/10: Training Loss: 0.0037042241543531417
Epoch 4/10: Training Loss: 0.002497134171426296
Epoch 5/10: Training Loss: 0.0025678515434265138
Epoch 6/10: Training Loss: 0.003844834491610527
Epoch 7/10: Training Loss: 0.0014847913756966592
Epoch 8/10: Training Loss: 0.0028727509081363676
Epoch 9/10: Training Loss: 0.002516016736626625
Epoch 0/10: Training Loss: 0.004065734893083572
Epoch 1/10: Training Loss: 0.002452700212597847
Epoch 2/10: Training Loss: 0.004049484431743622
Epoch 3/10: Training Loss: 0.0034047972410917284
Epoch 4/10: Training Loss: 0.0033715397119522096
Epoch 5/10: Training Loss: 0.0025596989318728445
Epoch 6/10: Training Loss: 0.0015387191437184812
Epoch 7/10: Training Loss: 0.0030830245465040206
Epoch 8/10: Training Loss: 0.002426760271191597
Epoch 9/10: Training Loss: 0.0017008282244205474
Epoch 0/10: Training Loss: 0.0037589512765407564
Epoch 1/10: Training Loss: 0.0030526498332619665
Epoch 2/10: Training Loss: 0.003711599111557007
Epoch 3/10: Training Loss: 0.0039753556251525875
Epoch 4/10: Training Loss: 0.0030188942328095436
Epoch 5/10: Training Loss: 0.004050847887992859
Epoch 6/10: Training Loss: 0.002335604652762413
Epoch 7/10: Training Loss: 0.002462887391448021
Epoch 8/10: Training Loss: 0.0027546804398298264
Epoch 9/10: Training Loss: 0.003073614463210106
Epoch 0/10: Training Loss: 0.0026188359898366748
Epoch 1/10: Training Loss: 0.002588990387643219
Epoch 2/10: Training Loss: 0.002507625871403202
Epoch 3/10: Training Loss: 0.0026943068595448876
Epoch 4/10: Training Loss: 0.0024926083482754457
Epoch 5/10: Training Loss: 0.0026489473452233963
Epoch 6/10: Training Loss: 0.0024857391977006465
Epoch 7/10: Training Loss: 0.0024049128316769933
Epoch 8/10: Training Loss: 0.0022036894491523694
Epoch 9/10: Training Loss: 0.0019429941086252784
Epoch 0/10: Training Loss: 0.003716205335726404
Epoch 1/10: Training Loss: 0.0022331225644251344
Epoch 2/10: Training Loss: 0.0022453399980144136
Epoch 3/10: Training Loss: 0.0024687046078360007
Epoch 4/10: Training Loss: 0.0028234794261349236
Epoch 5/10: Training Loss: 0.0023932278535927936
Epoch 6/10: Training Loss: 0.002626581556478124
Epoch 7/10: Training Loss: 0.00238711704873735
Epoch 8/10: Training Loss: 0.002458903440244638
Epoch 9/10: Training Loss: 0.002331753824926486
Epoch 0/10: Training Loss: 0.003052813991619523
Epoch 1/10: Training Loss: 0.002542465165921837
Epoch 2/10: Training Loss: 0.0031090538213207463
Epoch 3/10: Training Loss: 0.002812880619316344
Epoch 4/10: Training Loss: 0.0021723776486269226
Epoch 5/10: Training Loss: 0.002635038962030107
Epoch 6/10: Training Loss: 0.0023059670332890407
Epoch 7/10: Training Loss: 0.002129120621711585
Epoch 8/10: Training Loss: 0.0027036678259539757
Epoch 9/10: Training Loss: 0.002322284848826706
Epoch 0/10: Training Loss: 0.0025600362692447688
Epoch 1/10: Training Loss: 0.002910346779602253
Epoch 2/10: Training Loss: 0.0023686007553378476
Epoch 3/10: Training Loss: 0.002407471075752713
Epoch 4/10: Training Loss: 0.003019143808756443
Epoch 5/10: Training Loss: 0.0032218023641219992
Epoch 6/10: Training Loss: 0.0027388330326964523
Epoch 7/10: Training Loss: 0.0032201671837181443
Epoch 8/10: Training Loss: 0.002929024151618907
Epoch 9/10: Training Loss: 0.0028750874743556343
Epoch 0/10: Training Loss: 0.003378740604350109
Epoch 1/10: Training Loss: 0.0031713130853034013
Epoch 2/10: Training Loss: 0.0030498494770353204
Epoch 3/10: Training Loss: 0.002531162358277681
Epoch 4/10: Training Loss: 0.002876836535156957
Epoch 5/10: Training Loss: 0.0029099528363208896
Epoch 6/10: Training Loss: 0.0013390509103307661
Epoch 7/10: Training Loss: 0.003765535670400455
Epoch 8/10: Training Loss: 0.0023814684507862624
Epoch 9/10: Training Loss: 0.00250690228102223
Epoch 0/10: Training Loss: 0.0029935029563524867
Epoch 1/10: Training Loss: 0.0024478184861063167
Epoch 2/10: Training Loss: 0.001583755115010091
Epoch 3/10: Training Loss: 0.002660985024559577
Epoch 4/10: Training Loss: 0.0021273505608767076
Epoch 5/10: Training Loss: 0.0034876390798202414
Epoch 6/10: Training Loss: 0.0034914861451711084
Epoch 7/10: Training Loss: 0.0034856804159303376
Epoch 8/10: Training Loss: 0.002722093995833239
Epoch 9/10: Training Loss: 0.002096246409889878
Epoch 0/10: Training Loss: 0.0015849152032066795
Epoch 1/10: Training Loss: 0.0011220195714165182
Epoch 2/10: Training Loss: 0.0009310930967330932
Epoch 3/10: Training Loss: 0.0014429825193741743
Epoch 4/10: Training Loss: 0.0008392313823980444
Epoch 5/10: Training Loss: 0.0004933188066763036
Epoch 6/10: Training Loss: 0.0014421806615941665
Epoch 7/10: Training Loss: 0.0001216652529204593
Epoch 8/10: Training Loss: 0.00015757028013467788
Epoch 9/10: Training Loss: 0.0006922174902523265
Epoch 0/10: Training Loss: 0.0014456400976461522
Epoch 1/10: Training Loss: 0.001747055965311387
Epoch 2/10: Training Loss: 0.0011482491212732651
Epoch 3/10: Training Loss: 0.0013233079629785874
Epoch 4/10: Training Loss: 0.0003741727155797622
Epoch 5/10: Training Loss: 0.0005145775044665617
Epoch 6/10: Training Loss: 0.0018355818355784697
Epoch 7/10: Training Loss: 0.0017348026528077967
Epoch 8/10: Training Loss: 0.0016883576617521398
Epoch 9/10: Training Loss: 0.00017477379125707289
Epoch 0/10: Training Loss: 0.0018136532867656034
Epoch 1/10: Training Loss: 0.0010812058168299058
Epoch 2/10: Training Loss: 0.000500996586154489
Epoch 3/10: Training Loss: 0.0014769193004159366
Epoch 4/10: Training Loss: 0.0003410046810612959
Epoch 5/10: Training Loss: 0.0007375076413154602
Epoch 6/10: Training Loss: 0.0010453816722421085
Epoch 7/10: Training Loss: 0.0008689890013021582
Epoch 8/10: Training Loss: 0.0015261401148403391
Epoch 9/10: Training Loss: 0.0012987238519331988
dataset: capitals layer_num_from_end:-9 Avg_acc:tensor(0.7878) Avg_AUC:0.9288462877346685 Avg_threshold:0.3832988440990448
dataset: inventions layer_num_from_end:-9 Avg_acc:tensor(0.6739) Avg_AUC:0.7753616365361009 Avg_threshold:0.24397729088862738
dataset: elements layer_num_from_end:-9 Avg_acc:tensor(0.6036) Avg_AUC:0.659664701121517 Avg_threshold:0.40806318322817486
dataset: animals layer_num_from_end:-9 Avg_acc:tensor(0.5946) Avg_AUC:0.7100300768455531 Avg_threshold:0.4052310287952423
dataset: companies layer_num_from_end:-9 Avg_acc:tensor(0.7939) Avg_AUC:0.9051018518518519 Avg_threshold:0.5665978988011678
dataset: facts layer_num_from_end:-9 Avg_acc:tensor(0.6607) Avg_AUC:0.7346052800205048 Avg_threshold:0.8037484685579935


================layer -17================
Epoch 0/10: Training Loss: 0.004812360643506884
Epoch 1/10: Training Loss: 0.00546337924637161
Epoch 2/10: Training Loss: 0.005395940550557384
Epoch 3/10: Training Loss: 0.0059304491623298275
Epoch 4/10: Training Loss: 0.006265258455609942
Epoch 5/10: Training Loss: 0.005231140376804592
Epoch 6/10: Training Loss: 0.005417409476700363
Epoch 7/10: Training Loss: 0.004208443464932742
Epoch 8/10: Training Loss: 0.004915733437438111
Epoch 9/10: Training Loss: 0.005297695006523932
Epoch 0/10: Training Loss: 0.005326062232464343
Epoch 1/10: Training Loss: 0.004634275303020344
Epoch 2/10: Training Loss: 0.005309329166278972
Epoch 3/10: Training Loss: 0.006146239234017326
Epoch 4/10: Training Loss: 0.005786032526643126
Epoch 5/10: Training Loss: 0.0058144389332591235
Epoch 6/10: Training Loss: 0.005283320700372016
Epoch 7/10: Training Loss: 0.00411612170559543
Epoch 8/10: Training Loss: 0.005036296127559421
Epoch 9/10: Training Loss: 0.005232555049282687
Epoch 0/10: Training Loss: 0.005737325111469189
Epoch 1/10: Training Loss: 0.005109866182287256
Epoch 2/10: Training Loss: 0.005867160700417899
Epoch 3/10: Training Loss: 0.006035717217238633
Epoch 4/10: Training Loss: 0.006267838544778891
Epoch 5/10: Training Loss: 0.00537894155595686
Epoch 6/10: Training Loss: 0.0044689353529389925
Epoch 7/10: Training Loss: 0.004528360766964359
Epoch 8/10: Training Loss: 0.0038693234637067033
Epoch 9/10: Training Loss: 0.005153649753623909
Epoch 0/10: Training Loss: 0.004882767522261918
Epoch 1/10: Training Loss: 0.004219333452681091
Epoch 2/10: Training Loss: 0.0034517963971097045
Epoch 3/10: Training Loss: 0.0027054681734073383
Epoch 4/10: Training Loss: 0.002276147801451888
Epoch 5/10: Training Loss: 0.00223890093206628
Epoch 6/10: Training Loss: 0.0027296007410880247
Epoch 7/10: Training Loss: 0.0026812350457431347
Epoch 8/10: Training Loss: 0.0017700813299307795
Epoch 9/10: Training Loss: 0.0019439762355359786
Epoch 0/10: Training Loss: 0.004914447573796372
Epoch 1/10: Training Loss: 0.004697742637681084
Epoch 2/10: Training Loss: 0.004268775688358611
Epoch 3/10: Training Loss: 0.004259273318425278
Epoch 4/10: Training Loss: 0.0039777082899596795
Epoch 5/10: Training Loss: 0.003794776515726663
Epoch 6/10: Training Loss: 0.002721225374315414
Epoch 7/10: Training Loss: 0.0024838935743811672
Epoch 8/10: Training Loss: 0.0023429159737803455
Epoch 9/10: Training Loss: 0.0023184894052751225
Epoch 0/10: Training Loss: 0.004675669904135487
Epoch 1/10: Training Loss: 0.0038227281687449825
Epoch 2/10: Training Loss: 0.0034962475665507873
Epoch 3/10: Training Loss: 0.0032084379459451314
Epoch 4/10: Training Loss: 0.0027197438515037115
Epoch 5/10: Training Loss: 0.0025393280880582846
Epoch 6/10: Training Loss: 0.0025304020182486692
Epoch 7/10: Training Loss: 0.0029497400748949108
Epoch 8/10: Training Loss: 0.001751725293375963
Epoch 9/10: Training Loss: 0.0015186329195104493
Epoch 0/10: Training Loss: 0.003585122898221016
Epoch 1/10: Training Loss: 0.0034192562103271484
Epoch 2/10: Training Loss: 0.002165651507675648
Epoch 3/10: Training Loss: 0.0016632078215479851
Epoch 4/10: Training Loss: 0.0028297377750277517
Epoch 5/10: Training Loss: 0.001609925553202629
Epoch 6/10: Training Loss: 0.002802724950015545
Epoch 7/10: Training Loss: 0.0021991316229104997
Epoch 8/10: Training Loss: 0.0022890537977218626
Epoch 9/10: Training Loss: 0.0024292390793561936
Epoch 0/10: Training Loss: 0.004061605036258698
Epoch 1/10: Training Loss: 0.0038488008081912994
Epoch 2/10: Training Loss: 0.003531915694475174
Epoch 3/10: Training Loss: 0.0028190579265356063
Epoch 4/10: Training Loss: 0.003419923782348633
Epoch 5/10: Training Loss: 0.0022886330261826515
Epoch 6/10: Training Loss: 0.0023269789293408395
Epoch 7/10: Training Loss: 0.0027764122933149338
Epoch 8/10: Training Loss: 0.0023037251085042953
Epoch 9/10: Training Loss: 0.00352742038667202
Epoch 0/10: Training Loss: 0.004323207587003708
Epoch 1/10: Training Loss: 0.0031718265265226362
Epoch 2/10: Training Loss: 0.0037386827170848846
Epoch 3/10: Training Loss: 0.003478321433067322
Epoch 4/10: Training Loss: 0.0031107664108276367
Epoch 5/10: Training Loss: 0.0024268286302685737
Epoch 6/10: Training Loss: 0.002732297033071518
Epoch 7/10: Training Loss: 0.002309122495353222
Epoch 8/10: Training Loss: 0.0020440567284822466
Epoch 9/10: Training Loss: 0.002025175280869007
Epoch 0/10: Training Loss: 0.004201849554754366
Epoch 1/10: Training Loss: 0.002814355549538971
Epoch 2/10: Training Loss: 0.003736163020893267
Epoch 3/10: Training Loss: 0.0036647448873823613
Epoch 4/10: Training Loss: 0.0035026388563168275
Epoch 5/10: Training Loss: 0.0033674354006530374
Epoch 6/10: Training Loss: 0.0033293343653344803
Epoch 7/10: Training Loss: 0.0041417168204192145
Epoch 8/10: Training Loss: 0.004244970667893719
Epoch 9/10: Training Loss: 0.003787782161858431
Epoch 0/10: Training Loss: 0.004243345017645769
Epoch 1/10: Training Loss: 0.004090237010056805
Epoch 2/10: Training Loss: 0.003690581412831689
Epoch 3/10: Training Loss: 0.003042135079195545
Epoch 4/10: Training Loss: 0.003952132668464807
Epoch 5/10: Training Loss: 0.00415010095401934
Epoch 6/10: Training Loss: 0.004334020007188153
Epoch 7/10: Training Loss: 0.004282257739146044
Epoch 8/10: Training Loss: 0.0037950653179435975
Epoch 9/10: Training Loss: 0.002393106366418729
Epoch 0/10: Training Loss: 0.004088626545705613
Epoch 1/10: Training Loss: 0.0034596286002238083
Epoch 2/10: Training Loss: 0.0038376317662038623
Epoch 3/10: Training Loss: 0.004050260896136047
Epoch 4/10: Training Loss: 0.004176700191133341
Epoch 5/10: Training Loss: 0.004040567358587957
Epoch 6/10: Training Loss: 0.0041424780134942125
Epoch 7/10: Training Loss: 0.00383472860239114
Epoch 8/10: Training Loss: 0.004064262292946979
Epoch 9/10: Training Loss: 0.003719222014117393
Epoch 0/10: Training Loss: 0.004790226355293728
Epoch 1/10: Training Loss: 0.004152146001525273
Epoch 2/10: Training Loss: 0.004110579064350255
Epoch 3/10: Training Loss: 0.00465020951845788
Epoch 4/10: Training Loss: 0.004214355092964425
Epoch 5/10: Training Loss: 0.003792758019554694
Epoch 6/10: Training Loss: 0.004247899087059577
Epoch 7/10: Training Loss: 0.0037321665429121613
Epoch 8/10: Training Loss: 0.0034753392074281806
Epoch 9/10: Training Loss: 0.0034791906148392633
Epoch 0/10: Training Loss: 0.004518389701843262
Epoch 1/10: Training Loss: 0.004381143494157602
Epoch 2/10: Training Loss: 0.00428333424574492
Epoch 3/10: Training Loss: 0.004400743553970033
Epoch 4/10: Training Loss: 0.004461540291640932
Epoch 5/10: Training Loss: 0.004196295280330229
Epoch 6/10: Training Loss: 0.004166898743206302
Epoch 7/10: Training Loss: 0.0038602683717841343
Epoch 8/10: Training Loss: 0.0038745805917196714
Epoch 9/10: Training Loss: 0.003874621643925345
Epoch 0/10: Training Loss: 0.004425872635367691
Epoch 1/10: Training Loss: 0.004141487427894643
Epoch 2/10: Training Loss: 0.004123794716715023
Epoch 3/10: Training Loss: 0.004506430089078991
Epoch 4/10: Training Loss: 0.003640283417228042
Epoch 5/10: Training Loss: 0.0047369019085208314
Epoch 6/10: Training Loss: 0.0041866330121526655
Epoch 7/10: Training Loss: 0.003775626223608358
Epoch 8/10: Training Loss: 0.003407222150966821
Epoch 9/10: Training Loss: 0.00353926420211792
Epoch 0/10: Training Loss: 0.003574972994187299
Epoch 1/10: Training Loss: 0.0034355160068063177
Epoch 2/10: Training Loss: 0.002729917098494137
Epoch 3/10: Training Loss: 0.003245494646184585
Epoch 4/10: Training Loss: 0.00327835994608262
Epoch 5/10: Training Loss: 0.0034903144135194665
Epoch 6/10: Training Loss: 0.0020443937357734233
Epoch 7/10: Training Loss: 0.0012482595794341144
Epoch 8/10: Training Loss: 0.001864271830109989
Epoch 9/10: Training Loss: 0.002112624224494485
Epoch 0/10: Training Loss: 0.0034548882175894345
Epoch 1/10: Training Loss: 0.00324748719439787
Epoch 2/10: Training Loss: 0.0024989561123006485
Epoch 3/10: Training Loss: 0.003297711119932287
Epoch 4/10: Training Loss: 0.002691455798990586
Epoch 5/10: Training Loss: 0.002357184185701258
Epoch 6/10: Training Loss: 0.002576525597011342
Epoch 7/10: Training Loss: 0.001940443936516257
Epoch 8/10: Training Loss: 0.0019399567561991075
Epoch 9/10: Training Loss: 0.0019126348635729621
Epoch 0/10: Training Loss: 0.003552415090448716
Epoch 1/10: Training Loss: 0.0038165278294507196
Epoch 2/10: Training Loss: 0.0031689110924215877
Epoch 3/10: Training Loss: 0.0034690986661350025
Epoch 4/10: Training Loss: 0.0028556222424787636
Epoch 5/10: Training Loss: 0.0028295978027231554
Epoch 6/10: Training Loss: 0.0018050328773610731
Epoch 7/10: Training Loss: 0.002020403918098001
Epoch 8/10: Training Loss: 0.002250036597251892
Epoch 9/10: Training Loss: 0.003136869037852568
dataset: capitals layer_num_from_end:-17 Avg_acc:tensor(0.7631) Avg_AUC:0.8422349172658238 Avg_threshold:0.33455835779507953
dataset: inventions layer_num_from_end:-17 Avg_acc:tensor(0.6115) Avg_AUC:0.7230638321615891 Avg_threshold:0.3534396290779114
dataset: elements layer_num_from_end:-17 Avg_acc:tensor(0.5620) Avg_AUC:0.6118040621266427 Avg_threshold:0.24275105694929758
dataset: animals layer_num_from_end:-17 Avg_acc:tensor(0.5397) Avg_AUC:0.632263978122113 Avg_threshold:0.3626984159151713
dataset: companies layer_num_from_end:-17 Avg_acc:tensor(0.6683) Avg_AUC:0.8524361111111111 Avg_threshold:0.6688002447287241
dataset: facts layer_num_from_end:-17 Avg_acc:tensor(0.6035) Avg_AUC:0.656880544519914 Avg_threshold:0.6914066871007284


