2024-04-24 00:45:56.579378: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-04-24 00:45:57.640129: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
================layer -13================
Epoch 0/10: Training Loss: 0.005059677404123586
Epoch 1/10: Training Loss: 0.0040173222134996965
Epoch 2/10: Training Loss: 0.005510866225182594
Epoch 3/10: Training Loss: 0.005503716168703733
Epoch 4/10: Training Loss: 0.005456638919723617
Epoch 5/10: Training Loss: 0.005707704937541401
Epoch 6/10: Training Loss: 0.004944048144600608
Epoch 7/10: Training Loss: 0.004844436278709998
Epoch 8/10: Training Loss: 0.004937111080943288
Epoch 9/10: Training Loss: 0.00468611008637435
Epoch 0/10: Training Loss: 0.0048023737393892724
Epoch 1/10: Training Loss: 0.004156534071568843
Epoch 2/10: Training Loss: 0.004169998885868313
Epoch 3/10: Training Loss: 0.005633108265750058
Epoch 4/10: Training Loss: 0.003929082330290254
Epoch 5/10: Training Loss: 0.005336741884271582
Epoch 6/10: Training Loss: 0.004814754416058947
Epoch 7/10: Training Loss: 0.00549834591525418
Epoch 8/10: Training Loss: 0.005356077130857881
Epoch 9/10: Training Loss: 0.004717256222571527
Epoch 0/10: Training Loss: 0.0046442246937251595
Epoch 1/10: Training Loss: 0.004469473045189064
Epoch 2/10: Training Loss: 0.005143558228766168
Epoch 3/10: Training Loss: 0.005784030144031231
Epoch 4/10: Training Loss: 0.005644446903175408
Epoch 5/10: Training Loss: 0.005230679378642903
Epoch 6/10: Training Loss: 0.0034199375789482275
Epoch 7/10: Training Loss: 0.0038233449409058044
Epoch 8/10: Training Loss: 0.0036910092914021098
Epoch 9/10: Training Loss: 0.004808827296837227
Epoch 0/10: Training Loss: 0.004324745912493372
Epoch 1/10: Training Loss: 0.0038132587093517092
Epoch 2/10: Training Loss: 0.003648543650387255
Epoch 3/10: Training Loss: 0.002703216543958231
Epoch 4/10: Training Loss: 0.0022114946798312885
Epoch 5/10: Training Loss: 0.0023100560793847392
Epoch 6/10: Training Loss: 0.0020419763275450723
Epoch 7/10: Training Loss: 0.0013217916883573942
Epoch 8/10: Training Loss: 0.001692264723631502
Epoch 9/10: Training Loss: 0.0013575482587872838
Epoch 0/10: Training Loss: 0.003974639930608082
Epoch 1/10: Training Loss: 0.00412662190162331
Epoch 2/10: Training Loss: 0.003813355612608553
Epoch 3/10: Training Loss: 0.0029065117148533923
Epoch 4/10: Training Loss: 0.002086799751761501
Epoch 5/10: Training Loss: 0.0017924579374629297
Epoch 6/10: Training Loss: 0.002563574745611179
Epoch 7/10: Training Loss: 0.0012670073406827961
Epoch 8/10: Training Loss: 0.0014633705462414794
Epoch 9/10: Training Loss: 0.0010712519745153884
Epoch 0/10: Training Loss: 0.004187431437837566
Epoch 1/10: Training Loss: 0.0034942268593911013
Epoch 2/10: Training Loss: 0.0037237347269350766
Epoch 3/10: Training Loss: 0.0030372764435282512
Epoch 4/10: Training Loss: 0.0031498367069689045
Epoch 5/10: Training Loss: 0.002510773257975198
Epoch 6/10: Training Loss: 0.0011566456841544872
Epoch 7/10: Training Loss: 0.0010390842070608782
Epoch 8/10: Training Loss: 0.0008495917166668945
Epoch 9/10: Training Loss: 0.0006492686417936547
Epoch 0/10: Training Loss: 0.0046819765120744705
Epoch 1/10: Training Loss: 0.003117785044014454
Epoch 2/10: Training Loss: 0.0033438540995121002
Epoch 3/10: Training Loss: 0.0029530111700296403
Epoch 4/10: Training Loss: 0.0026219736784696577
Epoch 5/10: Training Loss: 0.0021265473216772078
Epoch 6/10: Training Loss: 0.002602989412844181
Epoch 7/10: Training Loss: 0.0028125112876296043
Epoch 8/10: Training Loss: 0.002348020300269127
Epoch 9/10: Training Loss: 0.002496695891022682
Epoch 0/10: Training Loss: 0.004468416050076485
Epoch 1/10: Training Loss: 0.002927442081272602
Epoch 2/10: Training Loss: 0.0024575969204306603
Epoch 3/10: Training Loss: 0.0032848726958036424
Epoch 4/10: Training Loss: 0.0028776735067367553
Epoch 5/10: Training Loss: 0.002724974974989891
Epoch 6/10: Training Loss: 0.002400357276201248
Epoch 7/10: Training Loss: 0.0023576550185680388
Epoch 8/10: Training Loss: 0.002321721613407135
Epoch 9/10: Training Loss: 0.0029998887330293655
Epoch 0/10: Training Loss: 0.004116104543209076
Epoch 1/10: Training Loss: 0.003245030343532562
Epoch 2/10: Training Loss: 0.003312765434384346
Epoch 3/10: Training Loss: 0.0027206208556890487
Epoch 4/10: Training Loss: 0.0030795739963650703
Epoch 5/10: Training Loss: 0.003184227645397186
Epoch 6/10: Training Loss: 0.0022111576050519942
Epoch 7/10: Training Loss: 0.0019678939133882523
Epoch 8/10: Training Loss: 0.002552817016839981
Epoch 9/10: Training Loss: 0.0021073056384921076
Epoch 0/10: Training Loss: 0.003734010040380393
Epoch 1/10: Training Loss: 0.002457054177666925
Epoch 2/10: Training Loss: 0.002229909038847419
Epoch 3/10: Training Loss: 0.0028176311474696846
Epoch 4/10: Training Loss: 0.0024109330906230174
Epoch 5/10: Training Loss: 0.0024791891407814756
Epoch 6/10: Training Loss: 0.0025342395351191237
Epoch 7/10: Training Loss: 0.0018538145502661444
Epoch 8/10: Training Loss: 0.0021695714847297424
Epoch 9/10: Training Loss: 0.001976781969617127
Epoch 0/10: Training Loss: 0.0036840066788302863
Epoch 1/10: Training Loss: 0.0028096426064801064
Epoch 2/10: Training Loss: 0.0025582435024771723
Epoch 3/10: Training Loss: 0.0026511836583447304
Epoch 4/10: Training Loss: 0.0021259101333132215
Epoch 5/10: Training Loss: 0.002594276977952119
Epoch 6/10: Training Loss: 0.0028666455274934223
Epoch 7/10: Training Loss: 0.0031219409529570563
Epoch 8/10: Training Loss: 0.00207688265545353
Epoch 9/10: Training Loss: 0.0028175130771223907
Epoch 0/10: Training Loss: 0.004026267558905729
Epoch 1/10: Training Loss: 0.0028269266246990035
Epoch 2/10: Training Loss: 0.002831663105897843
Epoch 3/10: Training Loss: 0.002854898097408805
Epoch 4/10: Training Loss: 0.0019077924406452544
Epoch 5/10: Training Loss: 0.0023436219828903296
Epoch 6/10: Training Loss: 0.0021299393313705542
Epoch 7/10: Training Loss: 0.002493823030192381
Epoch 8/10: Training Loss: 0.002838528459998453
Epoch 9/10: Training Loss: 0.002715490426227545
Epoch 0/10: Training Loss: 0.0038843198327828715
Epoch 1/10: Training Loss: 0.0035348822738950616
Epoch 2/10: Training Loss: 0.00442968417477134
Epoch 3/10: Training Loss: 0.0040305402894683234
Epoch 4/10: Training Loss: 0.0032634206165541085
Epoch 5/10: Training Loss: 0.0033566107023630712
Epoch 6/10: Training Loss: 0.003249204908775178
Epoch 7/10: Training Loss: 0.0038375953175374213
Epoch 8/10: Training Loss: 0.0035789012908935547
Epoch 9/10: Training Loss: 0.0031362732514640355
Epoch 0/10: Training Loss: 0.004241798097724157
Epoch 1/10: Training Loss: 0.0034435292742899712
Epoch 2/10: Training Loss: 0.0032277665785606334
Epoch 3/10: Training Loss: 0.004118351746868614
Epoch 4/10: Training Loss: 0.003975209810875899
Epoch 5/10: Training Loss: 0.003945525513579514
Epoch 6/10: Training Loss: 0.0037415406561845184
Epoch 7/10: Training Loss: 0.003168126210471652
Epoch 8/10: Training Loss: 0.003693708520851388
Epoch 9/10: Training Loss: 0.0030110916554533093
Epoch 0/10: Training Loss: 0.004096698287307032
Epoch 1/10: Training Loss: 0.0035672736483693913
Epoch 2/10: Training Loss: 0.003955151861077113
Epoch 3/10: Training Loss: 0.003802130553896064
Epoch 4/10: Training Loss: 0.003689485669925513
Epoch 5/10: Training Loss: 0.004020904073652053
Epoch 6/10: Training Loss: 0.0036690503556207317
Epoch 7/10: Training Loss: 0.0034357814599346643
Epoch 8/10: Training Loss: 0.003766265136516647
Epoch 9/10: Training Loss: 0.002706262646921423
Epoch 0/10: Training Loss: 0.005412678157582003
Epoch 1/10: Training Loss: 0.004102726543650907
Epoch 2/10: Training Loss: 0.0033999898854423973
Epoch 3/10: Training Loss: 0.0027929081636316635
Epoch 4/10: Training Loss: 0.002919944594888126
Epoch 5/10: Training Loss: 0.002843773365020752
Epoch 6/10: Training Loss: 0.002435829709557926
Epoch 7/10: Training Loss: 0.003159456743913538
Epoch 8/10: Training Loss: 0.002432928716435152
Epoch 9/10: Training Loss: 0.00272253576446982
Epoch 0/10: Training Loss: 0.004642619104946361
Epoch 1/10: Training Loss: 0.002622663273530848
Epoch 2/10: Training Loss: 0.002984019588021671
Epoch 3/10: Training Loss: 0.0029540573849397545
Epoch 4/10: Training Loss: 0.002667672493878533
Epoch 5/10: Training Loss: 0.0030016352148617017
Epoch 6/10: Training Loss: 0.0026518131003660313
Epoch 7/10: Training Loss: 0.0028967327931348017
Epoch 8/10: Training Loss: 0.0030678153038024903
Epoch 9/10: Training Loss: 0.002433609261232264
Epoch 0/10: Training Loss: 0.004602491855621338
Epoch 1/10: Training Loss: 0.0035158834036658794
Epoch 2/10: Training Loss: 0.002914653455509859
Epoch 3/10: Training Loss: 0.00306169215370627
Epoch 4/10: Training Loss: 0.0030805812162511487
Epoch 5/10: Training Loss: 0.002910460444057689
Epoch 6/10: Training Loss: 0.0028707462198594037
Epoch 7/10: Training Loss: 0.002342938324984382
Epoch 8/10: Training Loss: 0.0022680524517508112
Epoch 9/10: Training Loss: 0.002465728801839492
dataset: capitals layer_num_from_end:-13 Avg_acc:tensor(0.8388) Avg_AUC:0.9332428246973795 Avg_threshold:0.3336430291334788
dataset: inventions layer_num_from_end:-13 Avg_acc:tensor(0.7234) Avg_AUC:0.8223517185581967 Avg_threshold:0.22279511392116547
dataset: elements layer_num_from_end:-13 Avg_acc:tensor(0.5993) Avg_AUC:0.6807569275831503 Avg_threshold:0.3177640736103058
dataset: animals layer_num_from_end:-13 Avg_acc:tensor(0.5747) Avg_AUC:0.7140245758797347 Avg_threshold:0.43643702069918316
dataset: companies layer_num_from_end:-13 Avg_acc:tensor(0.7869) Avg_AUC:0.9025324074074074 Avg_threshold:0.46668591101964313
dataset: facts layer_num_from_end:-13 Avg_acc:tensor(0.6563) Avg_AUC:0.740447050279807 Avg_threshold:0.6993454992771149


