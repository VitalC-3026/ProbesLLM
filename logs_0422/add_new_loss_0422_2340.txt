2024-04-23 03:40:19.273160: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-04-23 03:40:20.351501: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Epoch 0/100: Training Loss: 0.004595895747204761
Epoch 1/100: Training Loss: 0.004292494350379998
Epoch 2/100: Training Loss: 0.0047208154118144425
Epoch 3/100: Training Loss: 0.004319560277712095
Epoch 4/100: Training Loss: 0.004059303056943667
Epoch 5/100: Training Loss: 0.003427971909929822
Epoch 6/100: Training Loss: 0.0032704426692082332
Epoch 7/100: Training Loss: 0.003434717446774036
Epoch 8/100: Training Loss: 0.0028336938027735356
Epoch 9/100: Training Loss: 0.003435761153281152
Epoch 10/100: Training Loss: 0.002711958401686662
Epoch 11/100: Training Loss: 0.002667770519123211
Epoch 12/100: Training Loss: 0.0038677316445570728
Epoch 13/100: Training Loss: 0.0019028928730037663
Epoch 14/100: Training Loss: 0.0031751419280792448
Epoch 15/100: Training Loss: 0.004907521334561435
Epoch 16/100: Training Loss: 0.003673745202017831
Epoch 17/100: Training Loss: 0.002131981866343038
Epoch 18/100: Training Loss: 0.0036196329376914286
Epoch 19/100: Training Loss: 0.0020509478929159526
Epoch 20/100: Training Loss: 0.004106263597528418
Epoch 21/100: Training Loss: 0.0026714128630978243
Epoch 22/100: Training Loss: 0.002773129439854122
Epoch 23/100: Training Loss: 0.0028678862364975724
Epoch 24/100: Training Loss: 0.002161753344369101
Epoch 25/100: Training Loss: 0.0022692822076223946
Epoch 26/100: Training Loss: 0.0025123647459737073
Epoch 27/100: Training Loss: 0.002030910103471129
Epoch 28/100: Training Loss: 0.0018419746335569796
Epoch 29/100: Training Loss: 0.0020030530599447396
Epoch 30/100: Training Loss: 0.0015878738848479478
Epoch 31/100: Training Loss: 0.0018551107886787895
Epoch 32/100: Training Loss: 0.001469362448979091
Epoch 33/100: Training Loss: 0.0015019206525562526
Epoch 34/100: Training Loss: 0.0016665927596859165
Epoch 35/100: Training Loss: 0.001645460620626703
Epoch 36/100: Training Loss: 0.0010659561290607587
Epoch 37/100: Training Loss: 0.00118689780885523
Epoch 38/100: Training Loss: 0.0013618270208785583
Epoch 39/100: Training Loss: 0.0011821313129438387
Epoch 40/100: Training Loss: 0.0013086441513541695
Epoch 41/100: Training Loss: 0.0017011801262835522
Epoch 42/100: Training Loss: 0.0014581764703030353
Epoch 43/100: Training Loss: 0.001289572228084911
Epoch 44/100: Training Loss: 0.0010902232431865238
Epoch 45/100: Training Loss: 0.0012572618214400498
Epoch 46/100: Training Loss: 0.001803152211062558
Epoch 47/100: Training Loss: 0.0012449457512035236
Epoch 48/100: Training Loss: 0.0015914215074552523
Epoch 49/100: Training Loss: 0.001620009854123309
Epoch 50/100: Training Loss: 0.0011739862131905723
Epoch 51/100: Training Loss: 0.0012566163823321149
Epoch 52/100: Training Loss: 0.001460281910596194
Epoch 53/100: Training Loss: 0.0010265804879315249
Epoch 54/100: Training Loss: 0.0008717021950474986
Epoch 55/100: Training Loss: 0.000640255014796357
Epoch 56/100: Training Loss: 0.0006331579460130705
Epoch 57/100: Training Loss: 0.000649309502198146
Epoch 58/100: Training Loss: 0.0009183948273425336
Epoch 59/100: Training Loss: 0.0007923335045367688
Epoch 60/100: Training Loss: 0.0009280007619124192
Epoch 61/100: Training Loss: 0.0010258172983889813
Epoch 62/100: Training Loss: 0.0010658323347985327
Epoch 63/100: Training Loss: 0.0010218088860278363
Epoch 64/100: Training Loss: 0.000871167420507311
Epoch 65/100: Training Loss: 0.000672952070102825
Epoch 66/100: Training Loss: 0.000422791714643265
Epoch 67/100: Training Loss: 0.0009909291992654333
Epoch 68/100: Training Loss: 0.0008798410425652991
Epoch 69/100: Training Loss: 0.0005141552928444389
Epoch 70/100: Training Loss: 0.0009378021830445403
Epoch 71/100: Training Loss: 0.000857476848405558
Epoch 72/100: Training Loss: 0.0012579210154660097
Epoch 73/100: Training Loss: 0.0008680221292522404
Epoch 74/100: Training Loss: 0.0008986421398349575
Epoch 75/100: Training Loss: 0.0008657776288219265
Epoch 76/100: Training Loss: 0.001225043739472236
Epoch 77/100: Training Loss: 0.0010527578683999868
Epoch 78/100: Training Loss: 0.0007293756295751024
Epoch 79/100: Training Loss: 0.00030890860028200216
Epoch 80/100: Training Loss: 0.0007124700537928335
Epoch 81/100: Training Loss: 0.0009669898064820083
Epoch 82/100: Training Loss: 0.0005640244775718742
Epoch 83/100: Training Loss: 0.0010801159835361934
Epoch 84/100: Training Loss: 0.0007620929004429104
Epoch 85/100: Training Loss: 0.0007457142318045343
Epoch 86/100: Training Loss: 0.0007297809307391827
Epoch 87/100: Training Loss: 0.0007219437535826142
Epoch 88/100: Training Loss: 0.00046072212549356313
Epoch 89/100: Training Loss: 0.0007959507353655942
Epoch 90/100: Training Loss: 0.0006146470045709944
Epoch 91/100: Training Loss: 0.0008359231523700527
Epoch 92/100: Training Loss: 0.0005580748503024762
Epoch 93/100: Training Loss: 0.0006940913992328244
Epoch 94/100: Training Loss: 0.0008112566246019377
Epoch 95/100: Training Loss: 0.00045220725811444794
Epoch 96/100: Training Loss: 0.0006749032364858614
Epoch 97/100: Training Loss: 0.00034340721118700256
Epoch 98/100: Training Loss: 0.0007851744865204071
Epoch 99/100: Training Loss: 0.0003851754265231686
Epoch 0/100: Training Loss: 0.004661340396721047
Epoch 1/100: Training Loss: 0.0041195929467261255
Epoch 2/100: Training Loss: 0.004356561424015286
Epoch 3/100: Training Loss: 0.003571108921424492
Epoch 4/100: Training Loss: 0.004190439527684992
Epoch 5/100: Training Loss: 0.003511310874165355
Epoch 6/100: Training Loss: 0.003450345326136876
Epoch 7/100: Training Loss: 0.003521996778208059
Epoch 8/100: Training Loss: 0.0022135581169928703
Epoch 9/100: Training Loss: 0.005049682997323416
Epoch 10/100: Training Loss: 0.004829911502091201
Epoch 11/100: Training Loss: 0.0030111874733771476
Epoch 12/100: Training Loss: 0.005215961199540358
Epoch 13/100: Training Loss: 0.004045789058391864
Epoch 14/100: Training Loss: 0.003364229535723066
Epoch 15/100: Training Loss: 0.004930523308840665
Epoch 16/100: Training Loss: 0.002424560226760544
Epoch 17/100: Training Loss: 0.002976274365311736
Epoch 18/100: Training Loss: 0.002698640931736339
Epoch 19/100: Training Loss: 0.0028733169282232963
Epoch 20/100: Training Loss: 0.003622395592135983
Epoch 21/100: Training Loss: 0.0021984798091274874
Epoch 22/100: Training Loss: 0.00313847215025575
Epoch 23/100: Training Loss: 0.0018453935643176099
Epoch 24/100: Training Loss: 0.0031715152146932963
Epoch 25/100: Training Loss: 0.0015218513203667594
Epoch 26/100: Training Loss: 0.0022687220073246456
Epoch 27/100: Training Loss: 0.002104550391644031
Epoch 28/100: Training Loss: 0.0031291160966966537
Epoch 29/100: Training Loss: 0.001924585212360729
Epoch 30/100: Training Loss: 0.0019531083273720909
Epoch 31/100: Training Loss: 0.0017357644709673796
Epoch 32/100: Training Loss: 0.0019467716867273505
Epoch 33/100: Training Loss: 0.0014200085526579745
Epoch 34/100: Training Loss: 0.0011997412551533091
Epoch 35/100: Training Loss: 0.0017023757621124908
Epoch 36/100: Training Loss: 0.0009001226691932945
Epoch 37/100: Training Loss: 0.0016706916210534688
Epoch 38/100: Training Loss: 0.002015796366271439
Epoch 39/100: Training Loss: 0.0019759219009559473
Epoch 40/100: Training Loss: 0.0014801654782328573
Epoch 41/100: Training Loss: 0.0013582656433532288
Epoch 42/100: Training Loss: 0.0010326044125990434
Epoch 43/100: Training Loss: 0.0016961164407796794
Epoch 44/100: Training Loss: 0.0010236044238497327
Epoch 45/100: Training Loss: 0.0011071282458472085
Epoch 46/100: Training Loss: 0.0007552690022475236
Epoch 47/100: Training Loss: 0.0012314490505031773
Epoch 48/100: Training Loss: 0.0012534954747953615
Epoch 49/100: Training Loss: 0.0014132757286925415
Epoch 50/100: Training Loss: 0.0010181943645010461
Epoch 51/100: Training Loss: 0.0013339977997999925
Epoch 52/100: Training Loss: 0.002400656054903577
Epoch 53/100: Training Loss: 0.0010879390931629635
Epoch 54/100: Training Loss: 0.0007674594025511842
Epoch 55/100: Training Loss: 0.001493691147624196
Epoch 56/100: Training Loss: 0.0010665894805134593
Epoch 57/100: Training Loss: 0.0011890845282094462
Epoch 58/100: Training Loss: 0.0007664336191190706
Epoch 59/100: Training Loss: 0.0013405339909600212
Epoch 60/100: Training Loss: 0.0014026550563065322
Epoch 61/100: Training Loss: 0.0012377779800575096
Epoch 62/100: Training Loss: 0.0014812280456502954
Epoch 63/100: Training Loss: 0.0010447554238192686
Epoch 64/100: Training Loss: 0.0012794438150379207
Epoch 65/100: Training Loss: 0.001331880376055524
Epoch 66/100: Training Loss: 0.0015022124235446637
Epoch 67/100: Training Loss: 0.0012545199035764573
Epoch 68/100: Training Loss: 0.0013392024732136226
Epoch 69/100: Training Loss: 0.0009841556315655475
Epoch 70/100: Training Loss: 0.000852092735834055
Epoch 71/100: Training Loss: 0.0008525525356506134
Epoch 72/100: Training Loss: 0.0011585501732526125
Epoch 73/100: Training Loss: 0.0009371372578027365
Epoch 74/100: Training Loss: 0.0007945316863226724
Epoch 75/100: Training Loss: 0.001158076566416067
Epoch 76/100: Training Loss: 0.0007113908137474861
Epoch 77/100: Training Loss: 0.0006510144868097105
Epoch 78/100: Training Loss: 0.0007465439555528281
Epoch 79/100: Training Loss: 0.0008603254710877692
Epoch 80/100: Training Loss: 0.0005724223239438517
Epoch 81/100: Training Loss: 0.001961283750467367
Epoch 82/100: Training Loss: 0.0006939779211591173
Epoch 83/100: Training Loss: 0.0006383907544862974
Epoch 84/100: Training Loss: 0.0010758141329238464
Epoch 85/100: Training Loss: 0.0008564375705652304
Epoch 86/100: Training Loss: 0.00083699416030537
Epoch 87/100: Training Loss: 0.0011189364261560506
Epoch 88/100: Training Loss: 0.0010520164574776495
Epoch 89/100: Training Loss: 0.0011260317755745841
Epoch 90/100: Training Loss: 0.0011560083269239305
Epoch 91/100: Training Loss: 0.0009017463747438018
Epoch 92/100: Training Loss: 0.0004923072207224119
Epoch 93/100: Training Loss: 0.0007859607051302503
Epoch 94/100: Training Loss: 0.0007169917746857329
Epoch 95/100: Training Loss: 0.0006431304699891097
Epoch 96/100: Training Loss: 0.000694433917532434
Epoch 97/100: Training Loss: 0.0006034447700827272
Epoch 98/100: Training Loss: 0.00023292315068778458
Epoch 99/100: Training Loss: 0.0013424280014905062
Epoch 0/100: Training Loss: 0.003908448702805526
Epoch 1/100: Training Loss: 0.004714508990307788
Epoch 2/100: Training Loss: 0.0039529200200434335
Epoch 3/100: Training Loss: 0.004724254141320715
Epoch 4/100: Training Loss: 0.004166551403232388
Epoch 5/100: Training Loss: 0.004196006934959572
Epoch 6/100: Training Loss: 0.004237519277559294
Epoch 7/100: Training Loss: 0.003963065730941879
Epoch 8/100: Training Loss: 0.003500481585522632
Epoch 9/100: Training Loss: 0.0020620026788511474
Epoch 10/100: Training Loss: 0.002329881374652569
Epoch 11/100: Training Loss: 0.0022128695374602205
Epoch 12/100: Training Loss: 0.002784408055818998
Epoch 13/100: Training Loss: 0.003965306115317178
Epoch 14/100: Training Loss: 0.0021385875615206633
Epoch 15/100: Training Loss: 0.0042630308991545565
Epoch 16/100: Training Loss: 0.0042992513496558984
Epoch 17/100: Training Loss: 0.0017801227269472776
Epoch 18/100: Training Loss: 0.0035554983399131083
Epoch 19/100: Training Loss: 0.00433176089000035
Epoch 20/100: Training Loss: 0.003109413843888503
Epoch 21/100: Training Loss: 0.002751729288301268
Epoch 22/100: Training Loss: 0.0028781594929995235
Epoch 23/100: Training Loss: 0.003097123616225236
Epoch 24/100: Training Loss: 0.0024927904555847595
Epoch 25/100: Training Loss: 0.002996801913201392
Epoch 26/100: Training Loss: 0.0022226988852440894
Epoch 27/100: Training Loss: 0.002093379314129169
Epoch 28/100: Training Loss: 0.002867483384125716
Epoch 29/100: Training Loss: 0.0016256018535240548
Epoch 30/100: Training Loss: 0.0019471689120872872
Epoch 31/100: Training Loss: 0.002341336303657585
Epoch 32/100: Training Loss: 0.0016639547331349833
Epoch 33/100: Training Loss: 0.0018236420371315696
Epoch 34/100: Training Loss: 0.001761311209285176
Epoch 35/100: Training Loss: 0.0016883791951866416
Epoch 36/100: Training Loss: 0.001360471432025616
Epoch 37/100: Training Loss: 0.002221701028463724
Epoch 38/100: Training Loss: 0.0021794184104545968
Epoch 39/100: Training Loss: 0.00150681000489455
Epoch 40/100: Training Loss: 0.0014839247390106842
Epoch 41/100: Training Loss: 0.0012969324638793518
Epoch 42/100: Training Loss: 0.001246767756822226
Epoch 43/100: Training Loss: 0.0013910125602375376
Epoch 44/100: Training Loss: 0.0015498310119122058
Epoch 45/100: Training Loss: 0.0014294164997714382
Epoch 46/100: Training Loss: 0.0013850390702694446
Epoch 47/100: Training Loss: 0.002519191561878978
Epoch 48/100: Training Loss: 0.0014022072920432459
Epoch 49/100: Training Loss: 0.0013668273503963763
Epoch 50/100: Training Loss: 0.0012742731121036556
Epoch 51/100: Training Loss: 0.0011642295997459572
Epoch 52/100: Training Loss: 0.003139889115220183
Epoch 53/100: Training Loss: 0.0022276639938354492
Epoch 54/100: Training Loss: 0.0013717727644460185
Epoch 55/100: Training Loss: 0.0015368778388816994
Epoch 56/100: Training Loss: 0.0018346001218248914
Epoch 57/100: Training Loss: 0.0015327693907530992
Epoch 58/100: Training Loss: 0.0016648359857239089
Epoch 59/100: Training Loss: 0.0015148951873912677
Epoch 60/100: Training Loss: 0.00171192230044545
Epoch 61/100: Training Loss: 0.0015691181162854175
Epoch 62/100: Training Loss: 0.0018019484473275138
Epoch 63/100: Training Loss: 0.0016367779328272892
Epoch 64/100: Training Loss: 0.0017172771317141872
Epoch 65/100: Training Loss: 0.001978724152891786
Epoch 66/100: Training Loss: 0.0017612017951645217
Epoch 67/100: Training Loss: 0.0020926284206497087
Epoch 68/100: Training Loss: 0.0016830940763433496
Epoch 69/100: Training Loss: 0.001235111193223433
Epoch 70/100: Training Loss: 0.0009386282075535167
Epoch 71/100: Training Loss: 0.0014942507226983984
Epoch 72/100: Training Loss: 0.0023116569419007202
Epoch 73/100: Training Loss: 0.001711259771893908
Epoch 74/100: Training Loss: 0.0012343427934846678
Epoch 75/100: Training Loss: 0.0013791458173231645
Epoch 76/100: Training Loss: 0.001341376792300831
Epoch 77/100: Training Loss: 0.0016020816731286216
Epoch 78/100: Training Loss: 0.0007734115828167309
Epoch 79/100: Training Loss: 0.0010274481940102745
Epoch 80/100: Training Loss: 0.0011815706958303918
Epoch 81/100: Training Loss: 0.001524582401022211
Epoch 82/100: Training Loss: 0.0012450755892933665
Epoch 83/100: Training Loss: 0.0007636006270255242
Epoch 84/100: Training Loss: 0.001261999348660449
Epoch 85/100: Training Loss: 0.0012540863110468937
Epoch 86/100: Training Loss: 0.002332282441479343
Epoch 87/100: Training Loss: 0.0010166070261201658
Epoch 88/100: Training Loss: 0.000914832511981884
Epoch 89/100: Training Loss: 0.0011888903962982283
Epoch 90/100: Training Loss: 0.000936085944409137
Epoch 91/100: Training Loss: 0.001048327846960588
Epoch 92/100: Training Loss: 0.0009723918421285136
Epoch 93/100: Training Loss: 0.0008409084765227525
Epoch 94/100: Training Loss: 0.0005013526944847373
Epoch 95/100: Training Loss: 0.0009423187979451426
Epoch 96/100: Training Loss: 0.0012219533636853412
Epoch 97/100: Training Loss: 0.0010128773592568779
Epoch 98/100: Training Loss: 0.0008741957949591683
Epoch 99/100: Training Loss: 0.0011708467573552698
Epoch 0/100: Training Loss: 0.00341053681870911
Epoch 1/100: Training Loss: 0.0030230356871716085
Epoch 2/100: Training Loss: 0.0036396066103976197
Epoch 3/100: Training Loss: 0.003048830968470661
Epoch 4/100: Training Loss: 0.0013888490163475459
Epoch 5/100: Training Loss: 0.0006946938718023476
Epoch 6/100: Training Loss: 0.0011179630010405932
Epoch 7/100: Training Loss: 0.0007640121538946234
Epoch 8/100: Training Loss: 0.0010957145617783435
Epoch 9/100: Training Loss: 0.000820523207904371
Epoch 10/100: Training Loss: 0.0016006169143629953
Epoch 11/100: Training Loss: 0.002499664488014268
Epoch 12/100: Training Loss: 0.001795220411628302
Epoch 13/100: Training Loss: 0.0018780995731704805
Epoch 14/100: Training Loss: 0.001488343246875365
Epoch 15/100: Training Loss: 0.001193864762417378
Epoch 16/100: Training Loss: 0.0008197899427882002
Epoch 17/100: Training Loss: 0.0008885509397354594
Epoch 18/100: Training Loss: 0.0007277875224505465
Epoch 19/100: Training Loss: 0.0005091755119569463
Epoch 20/100: Training Loss: 0.0005013228285532057
Epoch 21/100: Training Loss: 0.0006704804904621803
Epoch 22/100: Training Loss: 0.0005425157174010949
Epoch 23/100: Training Loss: 0.0006743832691315493
Epoch 24/100: Training Loss: 0.0005491496595136958
Epoch 25/100: Training Loss: 0.0006349254056719915
Epoch 26/100: Training Loss: 0.000342151595770947
Epoch 27/100: Training Loss: 0.00045229805393453025
Epoch 28/100: Training Loss: 0.0003573985004717587
Epoch 29/100: Training Loss: 0.000524839694880269
Epoch 30/100: Training Loss: 0.000354679312808382
Epoch 31/100: Training Loss: 0.00035687069768554596
Epoch 32/100: Training Loss: 0.0003692736289252533
Epoch 33/100: Training Loss: 0.0017852666187871454
Epoch 34/100: Training Loss: 0.0003503462745368115
Epoch 35/100: Training Loss: 0.00033710519868903363
Epoch 36/100: Training Loss: 0.0003317860783609145
Epoch 37/100: Training Loss: 0.0007703264980959746
Epoch 38/100: Training Loss: 0.0003700855593739843
Epoch 39/100: Training Loss: 0.0003417044923715065
Epoch 40/100: Training Loss: 0.00036247929363894315
Epoch 41/100: Training Loss: 0.00010412450354157781
Epoch 42/100: Training Loss: 0.00017019041293969185
Epoch 43/100: Training Loss: 2.471747825299304e-05
Epoch 44/100: Training Loss: 4.194812003545966e-05
Epoch 45/100: Training Loss: 0.00027541797950955254
Epoch 46/100: Training Loss: 6.5888429597660075e-06
Epoch 47/100: Training Loss: 1.8262974289388745e-05
Epoch 48/100: Training Loss: 0.00042927178319977836
Epoch 49/100: Training Loss: 0.00022625002118707434
Epoch 50/100: Training Loss: 0.00031922243398391394
Epoch 51/100: Training Loss: 0.00043475920437303786
Epoch 52/100: Training Loss: 0.0003908116576130405
Epoch 53/100: Training Loss: 8.828041537956226e-05
Epoch 54/100: Training Loss: 0.00010108833481197708
Epoch 55/100: Training Loss: 0.00032751536442458264
Epoch 56/100: Training Loss: 0.00032058835852365555
Epoch 57/100: Training Loss: 7.619753914193872e-05
Epoch 58/100: Training Loss: 9.155625397442309e-05
Epoch 59/100: Training Loss: 0.0001221375962707894
Epoch 60/100: Training Loss: 0.0012563357323956636
Epoch 61/100: Training Loss: 0.00031383017912232805
Epoch 62/100: Training Loss: 0.0002537973545080314
Epoch 63/100: Training Loss: 6.547502236315078e-05
Epoch 64/100: Training Loss: 2.7906141055325057e-05
Epoch 65/100: Training Loss: 0.0001015348211387915
Epoch 66/100: Training Loss: 5.8229098473589845e-05
Epoch 67/100: Training Loss: 3.1134799305646696e-06
Epoch 68/100: Training Loss: 6.013012782561999e-06
Epoch 69/100: Training Loss: 9.324856422430166e-05
Epoch 70/100: Training Loss: 1.6225171932504954e-05
Epoch 71/100: Training Loss: 0.0003019729914840745
Epoch 72/100: Training Loss: 0.00042369362949593666
Epoch 73/100: Training Loss: 0.00027774717727321787
Epoch 74/100: Training Loss: 0.000423161667183133
Epoch 75/100: Training Loss: 2.478961254315782e-06
Epoch 76/100: Training Loss: 5.39055848322763e-05
Epoch 77/100: Training Loss: 3.317597270743247e-05
Epoch 78/100: Training Loss: 9.648696136620878e-05
Epoch 79/100: Training Loss: 2.9288997953654797e-05
Epoch 80/100: Training Loss: 9.711408999068605e-06
Epoch 81/100: Training Loss: 2.4682162272418205e-05
Epoch 82/100: Training Loss: 1.102707425898021e-05
Epoch 83/100: Training Loss: 2.1270894805437948e-05
Epoch 84/100: Training Loss: 1.7352073676198537e-05
Epoch 85/100: Training Loss: 7.840881889765979e-06
Epoch 86/100: Training Loss: 0.0005049849946074691
Epoch 87/100: Training Loss: 4.401559330111632e-05
Epoch 88/100: Training Loss: 2.0662668315362344e-05
Epoch 89/100: Training Loss: 1.8001018389145288e-05
Epoch 90/100: Training Loss: 0.00011234597925759532
Epoch 91/100: Training Loss: 0.00021132378856097262
Epoch 92/100: Training Loss: 7.005716942570692e-06
Epoch 93/100: Training Loss: 6.923444609820523e-07
Epoch 94/100: Training Loss: 3.366505704317356e-05
Epoch 95/100: Training Loss: 9.700239556013075e-06
Epoch 96/100: Training Loss: 1.6065298342517365e-06
Epoch 97/100: Training Loss: 2.3827675238617357e-05
Epoch 98/100: Training Loss: 8.190630655164368e-06
Epoch 99/100: Training Loss: 0.00012765237433047383
Epoch 0/100: Training Loss: 0.004367356651399764
Epoch 1/100: Training Loss: 0.0032066750380159155
Epoch 2/100: Training Loss: 0.003602819940063851
Epoch 3/100: Training Loss: 0.0033480751733838414
Epoch 4/100: Training Loss: 0.002129479054293018
Epoch 5/100: Training Loss: 0.0015983563259335384
Epoch 6/100: Training Loss: 0.001093862155463798
Epoch 7/100: Training Loss: 0.0009322649130791975
Epoch 8/100: Training Loss: 0.0008642885582578695
Epoch 9/100: Training Loss: 0.0004667976151214787
Epoch 10/100: Training Loss: 0.0010648305796406752
Epoch 11/100: Training Loss: 0.0015053061619858068
Epoch 12/100: Training Loss: 0.001041847046898918
Epoch 13/100: Training Loss: 0.0014398130902483419
Epoch 14/100: Training Loss: 0.0030888787076517117
Epoch 15/100: Training Loss: 0.002035392574006063
Epoch 16/100: Training Loss: 0.0013392774239639562
Epoch 17/100: Training Loss: 0.0012413315429277948
Epoch 18/100: Training Loss: 0.0013400257730776547
Epoch 19/100: Training Loss: 0.0006564748616306328
Epoch 20/100: Training Loss: 0.0004965960065280001
Epoch 21/100: Training Loss: 0.0004733984181486024
Epoch 22/100: Training Loss: 0.00040648425100771194
Epoch 23/100: Training Loss: 0.000399760353418947
Epoch 24/100: Training Loss: 0.0004101089356135737
Epoch 25/100: Training Loss: 0.0004898868272640954
Epoch 26/100: Training Loss: 0.00042717117656228
Epoch 27/100: Training Loss: 0.0003966179910612984
Epoch 28/100: Training Loss: 0.0004330321780743043
Epoch 29/100: Training Loss: 0.0003859450115016633
Epoch 30/100: Training Loss: 0.0005617911921688384
Epoch 31/100: Training Loss: 0.0004323825148717026
Epoch 32/100: Training Loss: 0.0009020313711985489
Epoch 33/100: Training Loss: 0.00017754316467083304
Epoch 34/100: Training Loss: 0.00036793650110806425
Epoch 35/100: Training Loss: 0.0003559162006056382
Epoch 36/100: Training Loss: 0.00020151306515091036
Epoch 37/100: Training Loss: 0.0001816240145019227
Epoch 38/100: Training Loss: 0.00039819394883933975
Epoch 39/100: Training Loss: 0.00032537700208418207
Epoch 40/100: Training Loss: 0.00013330030295015112
Epoch 41/100: Training Loss: 0.00038680347379731254
Epoch 42/100: Training Loss: 0.0005298044326846585
Epoch 43/100: Training Loss: 0.0002759166656096289
Epoch 44/100: Training Loss: 0.00048331952899511606
Epoch 45/100: Training Loss: 0.00018020711290690064
Epoch 46/100: Training Loss: 0.00029561499510806033
Epoch 47/100: Training Loss: 0.00029865096957405654
Epoch 48/100: Training Loss: 0.00015710360524844537
Epoch 49/100: Training Loss: 0.00047890979088157233
Epoch 50/100: Training Loss: 6.983594317560547e-05
Epoch 51/100: Training Loss: 0.00010395621412370834
Epoch 52/100: Training Loss: 0.00043233474887953216
Epoch 53/100: Training Loss: 0.0003868793508757843
Epoch 54/100: Training Loss: 0.0006385852405630006
Epoch 55/100: Training Loss: 0.0005452076167416719
Epoch 56/100: Training Loss: 0.000391688357832973
Epoch 57/100: Training Loss: 0.0004598264313914293
Epoch 58/100: Training Loss: 0.0005818219638309595
Epoch 59/100: Training Loss: 0.0005157120023037027
Epoch 60/100: Training Loss: 0.00031991229832538067
Epoch 61/100: Training Loss: 0.0003085096646671646
Epoch 62/100: Training Loss: 0.0002074876994443086
Epoch 63/100: Training Loss: 0.0003206062993388966
Epoch 64/100: Training Loss: 0.00044707569973600423
Epoch 65/100: Training Loss: 5.566381329407721e-05
Epoch 66/100: Training Loss: 4.1162429457427536e-05
Epoch 67/100: Training Loss: 5.473734730957476e-05
Epoch 68/100: Training Loss: 0.00033001542274205964
Epoch 69/100: Training Loss: 0.00044879056741854896
Epoch 70/100: Training Loss: 1.684479141162217e-05
Epoch 71/100: Training Loss: 0.0001232960472809025
Epoch 72/100: Training Loss: 9.237065271365862e-05
Epoch 73/100: Training Loss: 1.2075211395698091e-05
Epoch 74/100: Training Loss: 4.7863655340817805e-05
Epoch 75/100: Training Loss: 2.494394390678113e-05
Epoch 76/100: Training Loss: 0.0002107085442981837
Epoch 77/100: Training Loss: 5.5211910440877904e-05
Epoch 78/100: Training Loss: 0.0001634354331742035
Epoch 79/100: Training Loss: 0.0010609789494356494
Epoch 80/100: Training Loss: 2.794862193472546e-05
Epoch 81/100: Training Loss: 0.0014028768597936338
Epoch 82/100: Training Loss: 3.810125214556244e-05
Epoch 83/100: Training Loss: 1.561322872258403e-05
Epoch 84/100: Training Loss: 1.748002768477048e-05
Epoch 85/100: Training Loss: 1.4578292226316008e-05
Epoch 86/100: Training Loss: 0.00013594859582514852
Epoch 87/100: Training Loss: 9.933390591773519e-05
Epoch 88/100: Training Loss: 2.809138854314213e-05
Epoch 89/100: Training Loss: 1.6312212026192365e-05
Epoch 90/100: Training Loss: 1.3991461872505996e-05
Epoch 91/100: Training Loss: 3.8866673601916965e-05
Epoch 92/100: Training Loss: 6.250425761828393e-05
Epoch 93/100: Training Loss: 7.070738113730963e-06
Epoch 94/100: Training Loss: 1.9039577330274084e-05
Epoch 95/100: Training Loss: 2.937038047696275e-06
Epoch 96/100: Training Loss: 2.3528045054776535e-05
Epoch 97/100: Training Loss: 2.8765897489398534e-05
Epoch 98/100: Training Loss: 0.000135702258147345
Epoch 99/100: Training Loss: 2.49150614796972e-05
Epoch 0/100: Training Loss: 0.004048940594210946
Epoch 1/100: Training Loss: 0.002867020163799356
Epoch 2/100: Training Loss: 0.003171411760014259
Epoch 3/100: Training Loss: 0.0026160824152589575
Epoch 4/100: Training Loss: 0.0014867201173232377
Epoch 5/100: Training Loss: 0.0008029551959476588
Epoch 6/100: Training Loss: 0.0025377277216296987
Epoch 7/100: Training Loss: 0.0020163627855616846
Epoch 8/100: Training Loss: 0.0010329217998528042
Epoch 9/100: Training Loss: 0.000735942937113756
Epoch 10/100: Training Loss: 0.0018024455550258145
Epoch 11/100: Training Loss: 0.003996791283777155
Epoch 12/100: Training Loss: 0.0006326911456745826
Epoch 13/100: Training Loss: 0.0013095522585090684
Epoch 14/100: Training Loss: 0.00216279347981412
Epoch 15/100: Training Loss: 0.0011660478597769709
Epoch 16/100: Training Loss: 0.0017316045205285945
Epoch 17/100: Training Loss: 0.0011317801256121303
Epoch 18/100: Training Loss: 0.0005459144620076279
Epoch 19/100: Training Loss: 0.00047946351071808237
Epoch 20/100: Training Loss: 0.000992119860795378
Epoch 21/100: Training Loss: 0.0005172523527057624
Epoch 22/100: Training Loss: 0.0004790044086842449
Epoch 23/100: Training Loss: 0.0004120942639426951
Epoch 24/100: Training Loss: 0.0004215756991158234
Epoch 25/100: Training Loss: 0.0005010268622380824
Epoch 26/100: Training Loss: 0.00031020480888021505
Epoch 27/100: Training Loss: 0.00011186072020077267
Epoch 28/100: Training Loss: 0.00043662907155744867
Epoch 29/100: Training Loss: 0.0008567388803680982
Epoch 30/100: Training Loss: 0.0003673595610571785
Epoch 31/100: Training Loss: 0.00017782239003415488
Epoch 32/100: Training Loss: 0.00031347250554459227
Epoch 33/100: Training Loss: 0.0003388392413320717
Epoch 34/100: Training Loss: 0.00015334277613762698
Epoch 35/100: Training Loss: 0.00018228326518477108
Epoch 36/100: Training Loss: 0.00023937792134431243
Epoch 37/100: Training Loss: 0.00016152735319605635
Epoch 38/100: Training Loss: 0.0002059271684819204
Epoch 39/100: Training Loss: 0.00015591606040673753
Epoch 40/100: Training Loss: 0.0002468434488115135
Epoch 41/100: Training Loss: 5.72691391582138e-05
Epoch 42/100: Training Loss: 0.00016321474971946762
Epoch 43/100: Training Loss: 4.485452924769349e-05
Epoch 44/100: Training Loss: 7.746961721979036e-05
Epoch 45/100: Training Loss: 8.013597089637277e-05
Epoch 46/100: Training Loss: 0.00024226093036265462
Epoch 47/100: Training Loss: 0.0015037121948288993
Epoch 48/100: Training Loss: 5.171734816823269e-05
Epoch 49/100: Training Loss: 7.629773773801473e-06
Epoch 50/100: Training Loss: 1.4698045739641218e-05
Epoch 51/100: Training Loss: 0.00010306669250587744
Epoch 52/100: Training Loss: 1.461429098632438e-05
Epoch 53/100: Training Loss: 2.6317127620332812e-05
Epoch 54/100: Training Loss: 2.9250353780443684e-05
Epoch 55/100: Training Loss: 9.89792634235935e-06
Epoch 56/100: Training Loss: 5.552301788988289e-05
Epoch 57/100: Training Loss: 1.2548756071486356e-05
Epoch 58/100: Training Loss: 0.0001800662946481646
Epoch 59/100: Training Loss: 2.3118424532146543e-05
Epoch 60/100: Training Loss: 0.00015813984159677308
Epoch 61/100: Training Loss: 6.289628157220735e-05
Epoch 62/100: Training Loss: 5.568277685005972e-05
Epoch 63/100: Training Loss: 3.2866165110487145e-05
Epoch 64/100: Training Loss: 4.4091890331791957e-05
Epoch 65/100: Training Loss: 0.00025715978324778973
Epoch 66/100: Training Loss: 8.119786718140351e-05
Epoch 67/100: Training Loss: 5.030042371866893e-05
Epoch 68/100: Training Loss: 0.00011606349901187639
Epoch 69/100: Training Loss: 2.26239722359217e-05
Epoch 70/100: Training Loss: 5.008543675845386e-05
Epoch 71/100: Training Loss: 6.138920738287498e-05
Epoch 72/100: Training Loss: 1.547656570521235e-05
Epoch 73/100: Training Loss: 0.00017370237894584795
Epoch 74/100: Training Loss: 5.753605163719025e-05
Epoch 75/100: Training Loss: 5.188854011289912e-05
Epoch 76/100: Training Loss: 3.8782711729315894e-05
Epoch 77/100: Training Loss: 5.41186477257613e-06
Epoch 78/100: Training Loss: 2.649713284764553e-05
Epoch 79/100: Training Loss: 6.120927911108942e-07
Epoch 80/100: Training Loss: 2.906610046519092e-05
Epoch 81/100: Training Loss: 9.791257054162171e-05
Epoch 82/100: Training Loss: 1.2055037977413897e-05
Epoch 83/100: Training Loss: 3.3030447841513375e-06
Epoch 84/100: Training Loss: 7.39325854950156e-06
Epoch 85/100: Training Loss: 5.312510723236697e-06
Epoch 86/100: Training Loss: 1.2278967669827897e-06
Epoch 87/100: Training Loss: 6.983151653655833e-06
Epoch 88/100: Training Loss: 1.7686028167765381e-06
Epoch 89/100: Training Loss: 1.3544515598039686e-06
Epoch 90/100: Training Loss: 1.3558926457893446e-06
Epoch 91/100: Training Loss: 7.23433645836369e-07
Epoch 92/100: Training Loss: 2.272971591517969e-05
Epoch 93/100: Training Loss: 3.0865212131085936e-06
Epoch 94/100: Training Loss: 5.904065753601812e-05
Epoch 95/100: Training Loss: 3.794184957339537e-06
Epoch 96/100: Training Loss: 2.698612005158436e-05
Epoch 97/100: Training Loss: 1.668783356121347e-05
Epoch 98/100: Training Loss: 6.776604447171001e-06
Epoch 99/100: Training Loss: 1.2168249378175092e-05
Epoch 0/100: Training Loss: 0.003800494223833084
Epoch 1/100: Training Loss: 0.0029458113014698027
Epoch 2/100: Training Loss: 0.002228705957531929
Epoch 3/100: Training Loss: 0.0037037841975688934
Epoch 4/100: Training Loss: 0.0032039448618888856
Epoch 5/100: Training Loss: 0.001601068489253521
Epoch 6/100: Training Loss: 0.002318095974624157
Epoch 7/100: Training Loss: 0.0016731962561607361
Epoch 8/100: Training Loss: 0.0007726331241428852
Epoch 9/100: Training Loss: 0.0013843834400177001
Epoch 10/100: Training Loss: 0.0015491707250475883
Epoch 11/100: Training Loss: 0.0017708979547023773
Epoch 12/100: Training Loss: 0.0020065758377313613
Epoch 13/100: Training Loss: 0.0013604046776890754
Epoch 14/100: Training Loss: 0.001409953273832798
Epoch 15/100: Training Loss: 0.0017988026142120362
Epoch 16/100: Training Loss: 0.001698365807533264
Epoch 17/100: Training Loss: 0.001454940065741539
Epoch 18/100: Training Loss: 0.0014701662585139275
Epoch 19/100: Training Loss: 0.0006593511905521154
Epoch 20/100: Training Loss: 0.0015343012288212775
Epoch 21/100: Training Loss: 0.0009069913998246193
Epoch 22/100: Training Loss: 0.0011152319610118866
Epoch 23/100: Training Loss: 0.0007165707647800446
Epoch 24/100: Training Loss: 0.0007657365873456001
Epoch 25/100: Training Loss: 0.0007657090201973916
Epoch 26/100: Training Loss: 0.0011675285175442695
Epoch 27/100: Training Loss: 0.0008250811137259006
Epoch 28/100: Training Loss: 0.0013183779083192348
Epoch 29/100: Training Loss: 0.0009064911864697933
Epoch 30/100: Training Loss: 0.0009503890760242939
Epoch 31/100: Training Loss: 0.0009608529508113861
Epoch 32/100: Training Loss: 0.0009903016500175
Epoch 33/100: Training Loss: 0.0011178449727594852
Epoch 34/100: Training Loss: 0.0006497731432318687
Epoch 35/100: Training Loss: 0.0010548874735832215
Epoch 36/100: Training Loss: 0.0010799916461110115
Epoch 37/100: Training Loss: 0.0007714317180216312
Epoch 38/100: Training Loss: 0.0006461494602262973
Epoch 39/100: Training Loss: 0.001227998174726963
Epoch 40/100: Training Loss: 0.0011053234338760376
Epoch 41/100: Training Loss: 0.0008318064734339714
Epoch 42/100: Training Loss: 0.0004693314433097839
Epoch 43/100: Training Loss: 0.0005572880618274212
Epoch 44/100: Training Loss: 0.0004831105936318636
Epoch 45/100: Training Loss: 0.000670541264116764
Epoch 46/100: Training Loss: 0.00038257306441664697
Epoch 47/100: Training Loss: 0.0007346156053245068
Epoch 48/100: Training Loss: 0.0008005265146493912
Epoch 49/100: Training Loss: 0.0006605167873203754
Epoch 50/100: Training Loss: 0.0005617768503725529
Epoch 51/100: Training Loss: 0.0005357385613024235
Epoch 52/100: Training Loss: 0.000503619946539402
Epoch 53/100: Training Loss: 0.00047085597179830074
Epoch 54/100: Training Loss: 0.0005030155181884766
Epoch 55/100: Training Loss: 0.0004327856469899416
Epoch 56/100: Training Loss: 0.0005793976131826639
Epoch 57/100: Training Loss: 0.00013561156811192632
Epoch 58/100: Training Loss: 0.00044917194172739984
Epoch 59/100: Training Loss: 0.0006050887517631054
Epoch 60/100: Training Loss: 0.00015127584338188172
Epoch 61/100: Training Loss: 0.0001659116940572858
Epoch 62/100: Training Loss: 0.00043766144663095476
Epoch 63/100: Training Loss: 0.0002449316903948784
Epoch 64/100: Training Loss: 0.0005137357860803604
Epoch 65/100: Training Loss: 0.00037954137660562993
Epoch 66/100: Training Loss: 0.0002649764996021986
Epoch 67/100: Training Loss: 0.0005488486029207706
Epoch 68/100: Training Loss: 0.00018905221950262784
Epoch 69/100: Training Loss: 0.0006257146596908569
Epoch 70/100: Training Loss: 0.00013157905777916312
Epoch 71/100: Training Loss: 0.00021082391031086446
Epoch 72/100: Training Loss: 5.689641693606973e-05
Epoch 73/100: Training Loss: 9.907088242471218e-05
Epoch 74/100: Training Loss: 0.0003236726392060518
Epoch 75/100: Training Loss: 0.0002683124504983425
Epoch 76/100: Training Loss: 8.831603918224573e-05
Epoch 77/100: Training Loss: 1.9092884031124413e-05
Epoch 78/100: Training Loss: 0.0006198585033416748
Epoch 79/100: Training Loss: 1.8612417625263335e-05
Epoch 80/100: Training Loss: 0.0002943727420642972
Epoch 81/100: Training Loss: 0.0002558393869549036
Epoch 82/100: Training Loss: 0.00011935574002563953
Epoch 83/100: Training Loss: 5.8277021162211894e-05
Epoch 84/100: Training Loss: 0.00043291226029396057
Epoch 85/100: Training Loss: 0.00037691830657422544
Epoch 86/100: Training Loss: 0.00010516853071749211
Epoch 87/100: Training Loss: 4.806437937077135e-05
Epoch 88/100: Training Loss: 0.00013909594854339958
Epoch 89/100: Training Loss: 1.5350365720223637e-05
Epoch 90/100: Training Loss: 8.649007068015636e-05
Epoch 91/100: Training Loss: 2.5199324591085313e-05
Epoch 92/100: Training Loss: 3.340660477988422e-05
Epoch 93/100: Training Loss: 6.548153469339013e-05
Epoch 94/100: Training Loss: 4.702351870946586e-05
Epoch 95/100: Training Loss: 1.683694572420791e-05
Epoch 96/100: Training Loss: 3.762954438570887e-05
Epoch 97/100: Training Loss: 1.3904039224144072e-05
Epoch 98/100: Training Loss: 5.5126840015873314e-05
Epoch 99/100: Training Loss: 0.00010775650152936578
Epoch 0/100: Training Loss: 0.0035555806010961533
Epoch 1/100: Training Loss: 0.0027115916833281516
Epoch 2/100: Training Loss: 0.0016062425449490548
Epoch 3/100: Training Loss: 0.003106476739048958
Epoch 4/100: Training Loss: 0.0027945211157202722
Epoch 5/100: Training Loss: 0.002638578787446022
Epoch 6/100: Training Loss: 0.0017459077760577201
Epoch 7/100: Training Loss: 0.002624548599123955
Epoch 8/100: Training Loss: 0.001953950896859169
Epoch 9/100: Training Loss: 0.0016016963869333266
Epoch 10/100: Training Loss: 0.0012464811094105243
Epoch 11/100: Training Loss: 0.0016351956874132156
Epoch 12/100: Training Loss: 0.0022471874952316286
Epoch 13/100: Training Loss: 0.0015557825565338135
Epoch 14/100: Training Loss: 0.0023058421909809114
Epoch 15/100: Training Loss: 0.0012525677680969237
Epoch 16/100: Training Loss: 0.0015128658153116703
Epoch 17/100: Training Loss: 0.0019831979647278787
Epoch 18/100: Training Loss: 0.0012170987203717232
Epoch 19/100: Training Loss: 0.0012973286211490631
Epoch 20/100: Training Loss: 0.0016335170716047288
Epoch 21/100: Training Loss: 0.0012538062408566474
Epoch 22/100: Training Loss: 0.0012782485224306583
Epoch 23/100: Training Loss: 0.0014072783291339875
Epoch 24/100: Training Loss: 0.0008375229313969612
Epoch 25/100: Training Loss: 0.001339728944003582
Epoch 26/100: Training Loss: 0.0014438042417168617
Epoch 27/100: Training Loss: 0.0012180026620626449
Epoch 28/100: Training Loss: 0.0011097893118858337
Epoch 29/100: Training Loss: 0.0013829449191689492
Epoch 30/100: Training Loss: 0.0010083540342748164
Epoch 31/100: Training Loss: 0.0007537024095654488
Epoch 32/100: Training Loss: 0.0011017147451639175
Epoch 33/100: Training Loss: 0.0007804988417774439
Epoch 34/100: Training Loss: 0.0004521692171692848
Epoch 35/100: Training Loss: 0.000742923840880394
Epoch 36/100: Training Loss: 0.0006768457591533661
Epoch 37/100: Training Loss: 0.0006010031327605247
Epoch 38/100: Training Loss: 0.0007399981841444969
Epoch 39/100: Training Loss: 0.0005989308003336192
Epoch 40/100: Training Loss: 0.000944165512919426
Epoch 41/100: Training Loss: 0.0007635141257196665
Epoch 42/100: Training Loss: 0.0007006966508924962
Epoch 43/100: Training Loss: 0.0004732399247586727
Epoch 44/100: Training Loss: 0.0007026740815490485
Epoch 45/100: Training Loss: 0.0008858846500515938
Epoch 46/100: Training Loss: 0.0009848945774137974
Epoch 47/100: Training Loss: 0.0008179628290235996
Epoch 48/100: Training Loss: 0.00017366865649819373
Epoch 49/100: Training Loss: 0.0005572973284870386
Epoch 50/100: Training Loss: 0.0003001183969900012
Epoch 51/100: Training Loss: 0.0002565666800364852
Epoch 52/100: Training Loss: 0.0005088209174573422
Epoch 53/100: Training Loss: 0.0005575116723775863
Epoch 54/100: Training Loss: 9.586822707206011e-05
Epoch 55/100: Training Loss: 0.00014016227796673775
Epoch 56/100: Training Loss: 0.00024713349994271995
Epoch 57/100: Training Loss: 0.00031716562807559967
Epoch 58/100: Training Loss: 0.0001573575777001679
Epoch 59/100: Training Loss: 0.00016789460787549614
Epoch 60/100: Training Loss: 0.0001509584835730493
Epoch 61/100: Training Loss: 6.910818628966809e-05
Epoch 62/100: Training Loss: 0.00015830036718398333
Epoch 63/100: Training Loss: 2.7641435735858977e-05
Epoch 64/100: Training Loss: 0.00015061250887811185
Epoch 65/100: Training Loss: 0.0005717756226658821
Epoch 66/100: Training Loss: 5.860290257260203e-05
Epoch 67/100: Training Loss: 9.246288100257516e-05
Epoch 68/100: Training Loss: 5.2974431309849027e-05
Epoch 69/100: Training Loss: 5.9843360213562845e-05
Epoch 70/100: Training Loss: 3.478848084341735e-05
Epoch 71/100: Training Loss: 0.0017453774809837342
Epoch 72/100: Training Loss: 0.0002854154212400317
Epoch 73/100: Training Loss: 0.00012018566485494375
Epoch 74/100: Training Loss: 9.251502342522144e-05
Epoch 75/100: Training Loss: 6.454169633798301e-05
Epoch 76/100: Training Loss: 0.00017000832594931125
Epoch 77/100: Training Loss: 9.394405788043514e-06
Epoch 78/100: Training Loss: 7.17179267667234e-05
Epoch 79/100: Training Loss: 2.115498064085841e-05
Epoch 80/100: Training Loss: 0.00021319077350199223
Epoch 81/100: Training Loss: 6.547155207954347e-05
Epoch 82/100: Training Loss: 5.7773583102971314e-05
Epoch 83/100: Training Loss: 3.2939473749138414e-05
Epoch 84/100: Training Loss: 4.498774069361389e-05
Epoch 85/100: Training Loss: 7.096779299899935e-05
Epoch 86/100: Training Loss: 1.6511493595317005e-05
Epoch 87/100: Training Loss: 8.16111802123487e-05
Epoch 88/100: Training Loss: 9.152191923931242e-05
Epoch 89/100: Training Loss: 3.583234502002597e-05
Epoch 90/100: Training Loss: 2.741731295827776e-05
Epoch 91/100: Training Loss: 8.564455492887646e-06
Epoch 92/100: Training Loss: 6.095989374443889e-05
Epoch 93/100: Training Loss: 8.723327191546559e-05
Epoch 94/100: Training Loss: 0.0006892571225762367
Epoch 95/100: Training Loss: 1.8640428606886417e-05
Epoch 96/100: Training Loss: 1.6743180458433927e-05
Epoch 97/100: Training Loss: 1.8760509556159376e-05
Epoch 98/100: Training Loss: 7.420729962177574e-05
Epoch 99/100: Training Loss: 8.730650879442691e-05
Epoch 0/100: Training Loss: 0.0031421948224306107
Epoch 1/100: Training Loss: 0.003674636036157608
Epoch 2/100: Training Loss: 0.003207264095544815
Epoch 3/100: Training Loss: 0.002844158373773098
Epoch 4/100: Training Loss: 0.002818540297448635
Epoch 5/100: Training Loss: 0.0019108880311250686
Epoch 6/100: Training Loss: 0.0027649050578475
Epoch 7/100: Training Loss: 0.0020424295216798782
Epoch 8/100: Training Loss: 0.0020573969930410387
Epoch 9/100: Training Loss: 0.0012190199457108974
Epoch 10/100: Training Loss: 0.001434178650379181
Epoch 11/100: Training Loss: 0.0016644200310111045
Epoch 12/100: Training Loss: 0.0013839565217494965
Epoch 13/100: Training Loss: 0.0015253029763698579
Epoch 14/100: Training Loss: 0.001484810933470726
Epoch 15/100: Training Loss: 0.001272950414568186
Epoch 16/100: Training Loss: 0.001552082784473896
Epoch 17/100: Training Loss: 0.0015625994652509688
Epoch 18/100: Training Loss: 0.0010616995394229888
Epoch 19/100: Training Loss: 0.0010748429223895073
Epoch 20/100: Training Loss: 0.0006235882639884949
Epoch 21/100: Training Loss: 0.0008238950744271278
Epoch 22/100: Training Loss: 0.0007305840495973826
Epoch 23/100: Training Loss: 0.001027605775743723
Epoch 24/100: Training Loss: 0.0008032487705349923
Epoch 25/100: Training Loss: 0.0008507657796144486
Epoch 26/100: Training Loss: 0.0007131104823201895
Epoch 27/100: Training Loss: 0.0007017520256340504
Epoch 28/100: Training Loss: 0.0007805656641721725
Epoch 29/100: Training Loss: 0.0007423770613968373
Epoch 30/100: Training Loss: 0.0015663757920265197
Epoch 31/100: Training Loss: 0.0015798414126038552
Epoch 32/100: Training Loss: 0.0008577628992497921
Epoch 33/100: Training Loss: 0.0008459761738777161
Epoch 34/100: Training Loss: 0.000973321869969368
Epoch 35/100: Training Loss: 0.0007896922528743744
Epoch 36/100: Training Loss: 0.0006076592952013015
Epoch 37/100: Training Loss: 0.0005932808388024568
Epoch 38/100: Training Loss: 0.0005495883058756589
Epoch 39/100: Training Loss: 9.994430001825094e-05
Epoch 40/100: Training Loss: 0.0009686971083283424
Epoch 41/100: Training Loss: 0.0005159971304237842
Epoch 42/100: Training Loss: 0.0006112763658165932
Epoch 43/100: Training Loss: 0.0007333291694521904
Epoch 44/100: Training Loss: 0.0008449654094874858
Epoch 45/100: Training Loss: 0.0006601533386856318
Epoch 46/100: Training Loss: 0.0005902013275772333
Epoch 47/100: Training Loss: 0.0005171206779778003
Epoch 48/100: Training Loss: 0.0006050417199730873
Epoch 49/100: Training Loss: 0.0005143043585121631
Epoch 50/100: Training Loss: 0.0005243428982794285
Epoch 51/100: Training Loss: 0.0002524837851524353
Epoch 52/100: Training Loss: 0.0004324364010244608
Epoch 53/100: Training Loss: 0.0005167923867702484
Epoch 54/100: Training Loss: 0.00032889354042708874
Epoch 55/100: Training Loss: 0.000614351499825716
Epoch 56/100: Training Loss: 0.000756143219769001
Epoch 57/100: Training Loss: 0.0007910632528364658
Epoch 58/100: Training Loss: 0.0010192370042204856
Epoch 59/100: Training Loss: 0.0007887153886258602
Epoch 60/100: Training Loss: 0.0001634343876503408
Epoch 61/100: Training Loss: 0.0005758948158472776
Epoch 62/100: Training Loss: 0.0006976306438446044
Epoch 63/100: Training Loss: 0.0007765798829495907
Epoch 64/100: Training Loss: 0.0008411198854446411
Epoch 65/100: Training Loss: 0.0008642766624689102
Epoch 66/100: Training Loss: 0.0008934594690799714
Epoch 67/100: Training Loss: 0.0005966332275420428
Epoch 68/100: Training Loss: 0.0009880295023322106
Epoch 69/100: Training Loss: 0.0003563854144886136
Epoch 70/100: Training Loss: 0.00035165115259587764
Epoch 71/100: Training Loss: 0.00067105982452631
Epoch 72/100: Training Loss: 0.000605516042560339
Epoch 73/100: Training Loss: 0.0005091113969683647
Epoch 74/100: Training Loss: 0.00041159563697874547
Epoch 75/100: Training Loss: 0.0006557337008416652
Epoch 76/100: Training Loss: 0.000797177292406559
Epoch 77/100: Training Loss: 0.00018470732029527426
Epoch 78/100: Training Loss: 0.00041588610038161277
Epoch 79/100: Training Loss: 0.0001376461354084313
Epoch 80/100: Training Loss: 0.0007406328804790974
Epoch 81/100: Training Loss: 6.647813133895398e-05
Epoch 82/100: Training Loss: 0.0007203074637800455
Epoch 83/100: Training Loss: 1.8142051703762264e-05
Epoch 84/100: Training Loss: 9.388876787852495e-06
Epoch 85/100: Training Loss: 0.0008828281424939632
Epoch 86/100: Training Loss: 7.38785820431076e-06
Epoch 87/100: Training Loss: 3.396774118300527e-05
Epoch 88/100: Training Loss: 0.00021625487133860588
Epoch 89/100: Training Loss: 2.4439606931991875e-05
Epoch 90/100: Training Loss: 0.0001221565529704094
Epoch 91/100: Training Loss: 7.912574801594019e-05
Epoch 92/100: Training Loss: 9.918615687638521e-05
Epoch 93/100: Training Loss: 0.0002998175797984004
Epoch 94/100: Training Loss: 0.00011676237918436527
Epoch 95/100: Training Loss: 2.3052575124893336e-05
Epoch 96/100: Training Loss: 0.0001417765044607222
Epoch 97/100: Training Loss: 1.7737687448970975e-05
Epoch 98/100: Training Loss: 0.0005894771777093411
Epoch 99/100: Training Loss: 0.00018249507993459702
Epoch 0/100: Training Loss: 0.0030108500438131344
Epoch 1/100: Training Loss: 0.0016588039079289527
Epoch 2/100: Training Loss: 0.0007176502210319422
Epoch 3/100: Training Loss: 0.003191716731733577
Epoch 4/100: Training Loss: 0.003651002410111154
Epoch 5/100: Training Loss: 0.0013067551479218112
Epoch 6/100: Training Loss: 0.0013378914564278475
Epoch 7/100: Training Loss: 0.0016885096100485249
Epoch 8/100: Training Loss: 0.0020363945869883156
Epoch 9/100: Training Loss: 0.0022471139005794645
Epoch 10/100: Training Loss: 0.0021811670558467794
Epoch 11/100: Training Loss: 0.002073362754408721
Epoch 12/100: Training Loss: 0.0016163809663930517
Epoch 13/100: Training Loss: 0.0016205314618007393
Epoch 14/100: Training Loss: 0.00150817955375477
Epoch 15/100: Training Loss: 0.001863915449494769
Epoch 16/100: Training Loss: 0.0022172079344463954
Epoch 17/100: Training Loss: 0.0017089228721181298
Epoch 18/100: Training Loss: 0.0017468629369310513
Epoch 19/100: Training Loss: 0.0013228802923943587
Epoch 20/100: Training Loss: 0.0011365251366499882
Epoch 21/100: Training Loss: 0.0021078848535088218
Epoch 22/100: Training Loss: 0.0013457517715016747
Epoch 23/100: Training Loss: 0.0010781494105697438
Epoch 24/100: Training Loss: 0.0015435165660396502
Epoch 25/100: Training Loss: 0.0008095509497223387
Epoch 26/100: Training Loss: 0.000987767983394064
Epoch 27/100: Training Loss: 0.0006346746235136773
Epoch 28/100: Training Loss: 0.0011042876608052831
Epoch 29/100: Training Loss: 0.0006061575025509877
Epoch 30/100: Training Loss: 0.00029384086181403725
Epoch 31/100: Training Loss: 0.00045633202145813377
Epoch 32/100: Training Loss: 0.00033373373803819063
Epoch 33/100: Training Loss: 0.00025035277198834024
Epoch 34/100: Training Loss: 0.0003961393265587509
Epoch 35/100: Training Loss: 0.00028339042595237684
Epoch 36/100: Training Loss: 0.0002812020670456491
Epoch 37/100: Training Loss: 0.00031879069698843984
Epoch 38/100: Training Loss: 0.0004336237432850394
Epoch 39/100: Training Loss: 0.00047031025977651024
Epoch 40/100: Training Loss: 0.0008813807159472423
Epoch 41/100: Training Loss: 0.0005199222048376776
Epoch 42/100: Training Loss: 0.001154242617309473
Epoch 43/100: Training Loss: 0.0005122426019352713
Epoch 44/100: Training Loss: 0.0005336131449717625
Epoch 45/100: Training Loss: 0.0003814173352186847
Epoch 46/100: Training Loss: 0.00045501037388090876
Epoch 47/100: Training Loss: 0.0007272683511114424
Epoch 48/100: Training Loss: 0.0006931398040170123
Epoch 49/100: Training Loss: 0.0006632786362793794
Epoch 50/100: Training Loss: 0.0007775333846450611
Epoch 51/100: Training Loss: 0.0008175819163109846
Epoch 52/100: Training Loss: 0.0003999500612544406
Epoch 53/100: Training Loss: 0.0003828578361660052
Epoch 54/100: Training Loss: 0.000682825400571155
Epoch 55/100: Training Loss: 0.0005454752285769031
Epoch 56/100: Training Loss: 0.0007652368894807852
Epoch 57/100: Training Loss: 0.00036328753373425477
Epoch 58/100: Training Loss: 0.0006713270191933699
Epoch 59/100: Training Loss: 0.0009350612474854585
Epoch 60/100: Training Loss: 0.0007089759893478102
Epoch 61/100: Training Loss: 0.0005028205597476594
Epoch 62/100: Training Loss: 0.0005222990824158784
Epoch 63/100: Training Loss: 9.753369388117152e-05
Epoch 64/100: Training Loss: 0.00031876620972991746
Epoch 65/100: Training Loss: 0.0003118455220179953
Epoch 66/100: Training Loss: 0.0005167955236070475
Epoch 67/100: Training Loss: 0.0005489999701263039
Epoch 68/100: Training Loss: 0.00045912181306037174
Epoch 69/100: Training Loss: 0.000453873472228931
Epoch 70/100: Training Loss: 0.0002512705696236556
Epoch 71/100: Training Loss: 0.00045355281252769906
Epoch 72/100: Training Loss: 0.00023718379959938632
Epoch 73/100: Training Loss: 0.0002842652047895322
Epoch 74/100: Training Loss: 0.0006532166509111975
Epoch 75/100: Training Loss: 0.0006705908828480229
Epoch 76/100: Training Loss: 0.0006036424807682159
Epoch 77/100: Training Loss: 0.0003400646672127353
Epoch 78/100: Training Loss: 0.0014746628559319077
Epoch 79/100: Training Loss: 0.00019127982342319124
Epoch 80/100: Training Loss: 0.0003313231430235942
Epoch 81/100: Training Loss: 0.0005040214794456579
Epoch 82/100: Training Loss: 0.0006094589165061902
Epoch 83/100: Training Loss: 0.00036320170968960803
Epoch 84/100: Training Loss: 0.00040688066725518294
Epoch 85/100: Training Loss: 0.0001553129760702704
Epoch 86/100: Training Loss: 0.00027099868674187143
Epoch 87/100: Training Loss: 0.00031147954190612596
Epoch 88/100: Training Loss: 0.0007281175274757823
Epoch 89/100: Training Loss: 0.0010465883715137555
Epoch 90/100: Training Loss: 0.00032623354226920257
Epoch 91/100: Training Loss: 0.00022937616060493858
Epoch 92/100: Training Loss: 0.00030889974278249557
Epoch 93/100: Training Loss: 0.0021094520380542537
Epoch 94/100: Training Loss: 0.00023407989721389334
Epoch 95/100: Training Loss: 0.00025928841464838403
Epoch 96/100: Training Loss: 0.00032116653053623854
Epoch 97/100: Training Loss: 0.000951375361460789
Epoch 98/100: Training Loss: 0.0003165633530373786
Epoch 99/100: Training Loss: 0.0003831514222606732
Epoch 0/100: Training Loss: 0.0027767529912814974
Epoch 1/100: Training Loss: 0.0016736767853900885
Epoch 2/100: Training Loss: 0.0016494588867114607
Epoch 3/100: Training Loss: 0.0024504557157018383
Epoch 4/100: Training Loss: 0.002433681943614012
Epoch 5/100: Training Loss: 0.0028135514563056314
Epoch 6/100: Training Loss: 0.0014082326251230422
Epoch 7/100: Training Loss: 0.002180745647211743
Epoch 8/100: Training Loss: 0.002275625611566434
Epoch 9/100: Training Loss: 0.0006729940984659134
Epoch 10/100: Training Loss: 0.002750107437182384
Epoch 11/100: Training Loss: 0.001485348981656846
Epoch 12/100: Training Loss: 0.001355387412818374
Epoch 13/100: Training Loss: 0.0012429079432396373
Epoch 14/100: Training Loss: 0.001460356602243557
Epoch 15/100: Training Loss: 0.0012771630552923603
Epoch 16/100: Training Loss: 0.001247215233031352
Epoch 17/100: Training Loss: 0.002253143840534672
Epoch 18/100: Training Loss: 0.0020417553983676207
Epoch 19/100: Training Loss: 0.0013655410830382328
Epoch 20/100: Training Loss: 0.001733208348037331
Epoch 21/100: Training Loss: 0.001397105444009137
Epoch 22/100: Training Loss: 0.0017543723628779125
Epoch 23/100: Training Loss: 0.0013616842449091042
Epoch 24/100: Training Loss: 0.001071926989373128
Epoch 25/100: Training Loss: 0.0010569145914855276
Epoch 26/100: Training Loss: 0.0015430366917020956
Epoch 27/100: Training Loss: 0.0008731655254485501
Epoch 28/100: Training Loss: 0.0009013745625307605
Epoch 29/100: Training Loss: 0.0008175347451191799
Epoch 30/100: Training Loss: 0.001188960234830334
Epoch 31/100: Training Loss: 0.0007164670498507797
Epoch 32/100: Training Loss: 0.0007282294285525182
Epoch 33/100: Training Loss: 0.0008724739976749299
Epoch 34/100: Training Loss: 0.0008120695306996631
Epoch 35/100: Training Loss: 0.0006303903973026641
Epoch 36/100: Training Loss: 0.0005738524969216365
Epoch 37/100: Training Loss: 0.00042297600940534264
Epoch 38/100: Training Loss: 0.0003970552022290078
Epoch 39/100: Training Loss: 0.0004035506373757769
Epoch 40/100: Training Loss: 0.00047151369463865927
Epoch 41/100: Training Loss: 0.0004099009997525792
Epoch 42/100: Training Loss: 0.0003020458255603815
Epoch 43/100: Training Loss: 0.0003797993965589317
Epoch 44/100: Training Loss: 0.0001619596651215462
Epoch 45/100: Training Loss: 0.0002446809107330954
Epoch 46/100: Training Loss: 0.00026336729906167194
Epoch 47/100: Training Loss: 0.00039771949030031824
Epoch 48/100: Training Loss: 0.0005613396501844856
Epoch 49/100: Training Loss: 0.00045352286783752926
Epoch 50/100: Training Loss: 0.0010342534370483107
Epoch 51/100: Training Loss: 0.0005955438894830691
Epoch 52/100: Training Loss: 0.00032472956901902607
Epoch 53/100: Training Loss: 0.0005786772936013094
Epoch 54/100: Training Loss: 0.0005476022032415791
Epoch 55/100: Training Loss: 0.00046856226814780263
Epoch 56/100: Training Loss: 0.0006918891030512039
Epoch 57/100: Training Loss: 0.00041241412329825626
Epoch 58/100: Training Loss: 0.000309199759155322
Epoch 59/100: Training Loss: 0.0003833789022485162
Epoch 60/100: Training Loss: 0.00034499291781407254
Epoch 61/100: Training Loss: 0.0003036294535846467
Epoch 62/100: Training Loss: 0.00023486783170396355
Epoch 63/100: Training Loss: 0.0004381021591508464
Epoch 64/100: Training Loss: 0.00032545891916675935
Epoch 65/100: Training Loss: 0.00069576853020176
Epoch 66/100: Training Loss: 0.0003187814905385303
Epoch 67/100: Training Loss: 0.0003083849883383247
Epoch 68/100: Training Loss: 0.0003800672140850383
Epoch 69/100: Training Loss: 0.0001813997484885963
Epoch 70/100: Training Loss: 0.00024281518094858547
Epoch 71/100: Training Loss: 0.0002047187488549834
Epoch 72/100: Training Loss: 0.0005214927112980254
Epoch 73/100: Training Loss: 0.00044218740265840177
Epoch 74/100: Training Loss: 0.0002968661534558436
Epoch 75/100: Training Loss: 0.0005070639738611355
Epoch 76/100: Training Loss: 0.00037385250447662013
Epoch 77/100: Training Loss: 0.0003763656992061882
Epoch 78/100: Training Loss: 0.00024873450113709565
Epoch 79/100: Training Loss: 0.0002677553113858411
Epoch 80/100: Training Loss: 0.0003745628959813695
Epoch 81/100: Training Loss: 0.0003566635167522795
Epoch 82/100: Training Loss: 0.0002593587917886722
Epoch 83/100: Training Loss: 0.0003878723379153355
Epoch 84/100: Training Loss: 0.0007474202259330992
Epoch 85/100: Training Loss: 0.0005084229692531999
Epoch 86/100: Training Loss: 0.0003967325968347537
Epoch 87/100: Training Loss: 0.00024481319413063635
Epoch 88/100: Training Loss: 0.0002668630213114866
Epoch 89/100: Training Loss: 0.00026027673179176964
Epoch 90/100: Training Loss: 0.0004186900273250167
Epoch 91/100: Training Loss: 0.00026390924575222526
Epoch 92/100: Training Loss: 0.0006139991199894316
Epoch 93/100: Training Loss: 0.00020550646979338043
Epoch 94/100: Training Loss: 0.0024995642482854756
Epoch 95/100: Training Loss: 0.0001591940167223572
Epoch 96/100: Training Loss: 0.0002757875592845261
Epoch 97/100: Training Loss: 0.00022062348427286574
Epoch 98/100: Training Loss: 0.00021934888924762702
Epoch 99/100: Training Loss: 0.0002571415084942131
Epoch 0/100: Training Loss: 0.0018640202321823995
Epoch 1/100: Training Loss: 0.0015010114308375462
Epoch 2/100: Training Loss: 0.0029683925543621086
Epoch 3/100: Training Loss: 0.0027662967420687343
Epoch 4/100: Training Loss: 0.0022941470905474036
Epoch 5/100: Training Loss: 0.002034003567543759
Epoch 6/100: Training Loss: 0.002375583549973312
Epoch 7/100: Training Loss: 0.0016084806934283796
Epoch 8/100: Training Loss: 0.0022228358277849333
Epoch 9/100: Training Loss: 0.0022161162582932004
Epoch 10/100: Training Loss: 0.00238361176411817
Epoch 11/100: Training Loss: 0.0014742531214550043
Epoch 12/100: Training Loss: 0.0019743036312662113
Epoch 13/100: Training Loss: 0.0015266736032097203
Epoch 14/100: Training Loss: 0.0015998852860396075
Epoch 15/100: Training Loss: 0.0024031210856832515
Epoch 16/100: Training Loss: 0.0022818068410180936
Epoch 17/100: Training Loss: 0.0019856607838041464
Epoch 18/100: Training Loss: 0.0018547315886066219
Epoch 19/100: Training Loss: 0.0021725536152056068
Epoch 20/100: Training Loss: 0.0020890258679724045
Epoch 21/100: Training Loss: 0.001824599542435567
Epoch 22/100: Training Loss: 0.001611580514604119
Epoch 23/100: Training Loss: 0.0009532523383000853
Epoch 24/100: Training Loss: 0.002555831982071992
Epoch 25/100: Training Loss: 0.001152767497263137
Epoch 26/100: Training Loss: 0.0009446757234585513
Epoch 27/100: Training Loss: 0.0010274258578658864
Epoch 28/100: Training Loss: 0.001034072250317616
Epoch 29/100: Training Loss: 0.0013735899879674244
Epoch 30/100: Training Loss: 0.0013700460742233666
Epoch 31/100: Training Loss: 0.001549041195280233
Epoch 32/100: Training Loss: 0.0008277450777163172
Epoch 33/100: Training Loss: 0.000609227995963613
Epoch 34/100: Training Loss: 0.000651559347559692
Epoch 35/100: Training Loss: 0.0012151536288534759
Epoch 36/100: Training Loss: 0.0006882436336225764
Epoch 37/100: Training Loss: 0.0005248059892350701
Epoch 38/100: Training Loss: 0.000493952423144298
Epoch 39/100: Training Loss: 0.0008962788399617383
Epoch 40/100: Training Loss: 0.0006026735256431968
Epoch 41/100: Training Loss: 0.00024390602662305164
Epoch 42/100: Training Loss: 0.0004944516595002192
Epoch 43/100: Training Loss: 0.00045490601829662445
Epoch 44/100: Training Loss: 0.00043955113098120233
Epoch 45/100: Training Loss: 0.0004590767773852986
Epoch 46/100: Training Loss: 0.0005135384334880075
Epoch 47/100: Training Loss: 0.0005819182012491165
Epoch 48/100: Training Loss: 0.0006133770676934795
Epoch 49/100: Training Loss: 0.000631059905525985
Epoch 50/100: Training Loss: 0.000772192125107832
Epoch 51/100: Training Loss: 0.0005967917430932354
Epoch 52/100: Training Loss: 0.000736610240237728
Epoch 53/100: Training Loss: 0.0005940438550748643
Epoch 54/100: Training Loss: 0.0003252793933935226
Epoch 55/100: Training Loss: 0.0007754082133056252
Epoch 56/100: Training Loss: 0.0004961463106665641
Epoch 57/100: Training Loss: 0.00046530361198315954
Epoch 58/100: Training Loss: 0.00038061661135618856
Epoch 59/100: Training Loss: 0.00023008123704582262
Epoch 60/100: Training Loss: 0.0004710961773896673
Epoch 61/100: Training Loss: 0.0005605902261794752
Epoch 62/100: Training Loss: 0.0005955938131186613
Epoch 63/100: Training Loss: 0.0003654416294614221
Epoch 64/100: Training Loss: 0.0004671627453937652
Epoch 65/100: Training Loss: 0.0003953367756430511
Epoch 66/100: Training Loss: 0.0004200328877017756
Epoch 67/100: Training Loss: 0.0011783032470448
Epoch 68/100: Training Loss: 0.00038301361024759375
Epoch 69/100: Training Loss: 0.00032650828836070504
Epoch 70/100: Training Loss: 0.00035481050515630443
Epoch 71/100: Training Loss: 0.0003664238959740681
Epoch 72/100: Training Loss: 0.0002782957948696841
Epoch 73/100: Training Loss: 0.000331515292073511
Epoch 74/100: Training Loss: 0.0002674524951132999
Epoch 75/100: Training Loss: 0.0002798443766915874
Epoch 76/100: Training Loss: 0.00025652282556910426
Epoch 77/100: Training Loss: 0.00024031161977227327
Epoch 78/100: Training Loss: 0.0003236667460696712
Epoch 79/100: Training Loss: 0.0001503505097453002
Epoch 80/100: Training Loss: 0.00048486299955161516
Epoch 81/100: Training Loss: 0.0002272567314327143
Epoch 82/100: Training Loss: 0.00022410549175967077
Epoch 83/100: Training Loss: 0.0002351612991588131
Epoch 84/100: Training Loss: 0.0008909355872755597
Epoch 85/100: Training Loss: 0.0012114668727680375
Epoch 86/100: Training Loss: 0.0003531541983792736
Epoch 87/100: Training Loss: 0.0003252458420528728
Epoch 88/100: Training Loss: 0.00021766660035036172
Epoch 89/100: Training Loss: 0.00010932000816627673
Epoch 90/100: Training Loss: 0.0002766737038162863
Epoch 91/100: Training Loss: 0.0002651165720004185
Epoch 92/100: Training Loss: 0.00037644874707908386
Epoch 93/100: Training Loss: 0.0002181979406411481
Epoch 94/100: Training Loss: 0.001990215793536727
Epoch 95/100: Training Loss: 0.00022180110311052602
Epoch 96/100: Training Loss: 0.00021893540575246143
Epoch 97/100: Training Loss: 0.00019352992249142592
Epoch 98/100: Training Loss: 0.0002852391665148887
Epoch 99/100: Training Loss: 0.00020044603070635704
Epoch 0/100: Training Loss: 0.0034899601083717598
Epoch 1/100: Training Loss: 0.0027825654737207274
Epoch 2/100: Training Loss: 0.0019180133248007062
Epoch 3/100: Training Loss: 0.0033459927862053676
Epoch 4/100: Training Loss: 0.002747230182420339
Epoch 5/100: Training Loss: 0.0030043417254820566
Epoch 6/100: Training Loss: 0.0034076145153171967
Epoch 7/100: Training Loss: 0.0034077542507095844
Epoch 8/100: Training Loss: 0.004049734564016986
Epoch 9/100: Training Loss: 0.003096364191825816
Epoch 10/100: Training Loss: 0.003502837869505219
Epoch 11/100: Training Loss: 0.0029659196241012473
Epoch 12/100: Training Loss: 0.0033028881676149683
Epoch 13/100: Training Loss: 0.0026645163037129584
Epoch 14/100: Training Loss: 0.0024753627398156172
Epoch 15/100: Training Loss: 0.002099330851573818
Epoch 16/100: Training Loss: 0.0028582864644511644
Epoch 17/100: Training Loss: 0.0041109287975639695
Epoch 18/100: Training Loss: 0.003214573623328809
Epoch 19/100: Training Loss: 0.003106207444967813
Epoch 20/100: Training Loss: 0.0026980756923852376
Epoch 21/100: Training Loss: 0.0028392702143713337
Epoch 22/100: Training Loss: 0.0031209625550453236
Epoch 23/100: Training Loss: 0.002723768057412659
Epoch 24/100: Training Loss: 0.00299792376575091
Epoch 25/100: Training Loss: 0.0024021931041944894
Epoch 26/100: Training Loss: 0.002144317358534857
Epoch 27/100: Training Loss: 0.002216557793269884
Epoch 28/100: Training Loss: 0.0026845549116071487
Epoch 29/100: Training Loss: 0.001951099034176757
Epoch 30/100: Training Loss: 0.0021794329810616198
Epoch 31/100: Training Loss: 0.0019807089243503596
Epoch 32/100: Training Loss: 0.0019169734013791116
Epoch 33/100: Training Loss: 0.0018745542362036294
Epoch 34/100: Training Loss: 0.0016951335976455385
Epoch 35/100: Training Loss: 0.002135078835961045
Epoch 36/100: Training Loss: 0.0020486096672664416
Epoch 37/100: Training Loss: 0.002379525773572606
Epoch 38/100: Training Loss: 0.0021908981910604515
Epoch 39/100: Training Loss: 0.0028972639548068015
Epoch 40/100: Training Loss: 0.0017139329420809714
Epoch 41/100: Training Loss: 0.0015751946446121922
Epoch 42/100: Training Loss: 0.0020221181657930085
Epoch 43/100: Training Loss: 0.0017014836633442254
Epoch 44/100: Training Loss: 0.0015125815442066318
Epoch 45/100: Training Loss: 0.0016351218847249517
Epoch 46/100: Training Loss: 0.0006342299331892406
Epoch 47/100: Training Loss: 0.0013388307481412066
Epoch 48/100: Training Loss: 0.0010074988895694152
Epoch 49/100: Training Loss: 0.001975889632243984
Epoch 50/100: Training Loss: 0.002187969471445147
Epoch 51/100: Training Loss: 0.0017618191558004216
Epoch 52/100: Training Loss: 0.0017818488822078074
Epoch 53/100: Training Loss: 0.00123555376040225
Epoch 54/100: Training Loss: 0.0018801649674674533
Epoch 55/100: Training Loss: 0.001193020221413366
Epoch 56/100: Training Loss: 0.0012038461617286632
Epoch 57/100: Training Loss: 0.0010468014423420887
Epoch 58/100: Training Loss: 0.0007299806009854702
Epoch 59/100: Training Loss: 0.0006780534094532594
Epoch 60/100: Training Loss: 0.0003956147771797433
Epoch 61/100: Training Loss: 0.0009147937329399664
Epoch 62/100: Training Loss: 0.0008587306303693759
Epoch 63/100: Training Loss: 0.0012276103559708752
Epoch 64/100: Training Loss: 0.001145186803198808
Epoch 65/100: Training Loss: 0.0011336808370438632
Epoch 66/100: Training Loss: 0.0010468701258400418
Epoch 67/100: Training Loss: 0.000962698104365772
Epoch 68/100: Training Loss: 0.001252911343479788
Epoch 69/100: Training Loss: 0.0007672860034254213
Epoch 70/100: Training Loss: 0.0004954388402155693
Epoch 71/100: Training Loss: 0.0007870523937490603
Epoch 72/100: Training Loss: 0.000891030328163248
Epoch 73/100: Training Loss: 0.0002947191627609809
Epoch 74/100: Training Loss: 0.000701571734535773
Epoch 75/100: Training Loss: 0.0007362161448459752
Epoch 76/100: Training Loss: 0.0009385580258653653
Epoch 77/100: Training Loss: 0.0008319715000935738
Epoch 78/100: Training Loss: 0.0006462792489702338
Epoch 79/100: Training Loss: 0.0012522171072612536
Epoch 80/100: Training Loss: 0.0009901073594756474
Epoch 81/100: Training Loss: 0.00047739738265410167
Epoch 82/100: Training Loss: 0.0008020772049758608
Epoch 83/100: Training Loss: 0.0007961947495574193
Epoch 84/100: Training Loss: 0.0007909000511990478
Epoch 85/100: Training Loss: 0.001396180186050617
Epoch 86/100: Training Loss: 0.0008787263110773453
Epoch 87/100: Training Loss: 0.0008802493873810926
Epoch 88/100: Training Loss: 0.0009965967658339748
Epoch 89/100: Training Loss: 0.0008388108370319897
Epoch 90/100: Training Loss: 0.000587163826092979
Epoch 91/100: Training Loss: 0.0007837554871641247
Epoch 92/100: Training Loss: 0.0005714051573481781
Epoch 93/100: Training Loss: 0.0009943647495168724
Epoch 94/100: Training Loss: 0.0014592203872882768
Epoch 95/100: Training Loss: 0.0010220480478362532
Epoch 96/100: Training Loss: 0.0007907744275023606
Epoch 97/100: Training Loss: 0.0007564325798426242
Epoch 98/100: Training Loss: 0.0008770697164219736
Epoch 99/100: Training Loss: 0.000765683339131589
Epoch 0/100: Training Loss: 0.0031821546175621993
Epoch 1/100: Training Loss: 0.002917557954788208
Epoch 2/100: Training Loss: 0.0025173386201163793
Epoch 3/100: Training Loss: 0.0023845534845693224
Epoch 4/100: Training Loss: 0.003250708051075209
Epoch 5/100: Training Loss: 0.0037826035196418003
Epoch 6/100: Training Loss: 0.0036918639347253254
Epoch 7/100: Training Loss: 0.0037749713619813223
Epoch 8/100: Training Loss: 0.002562106642502033
Epoch 9/100: Training Loss: 0.002591799229186102
Epoch 10/100: Training Loss: 0.0018012563124397733
Epoch 11/100: Training Loss: 0.0013746975470852378
Epoch 12/100: Training Loss: 0.002203144182432566
Epoch 13/100: Training Loss: 0.0028349328909488703
Epoch 14/100: Training Loss: 0.0018045513440441612
Epoch 15/100: Training Loss: 0.002502646469911992
Epoch 16/100: Training Loss: 0.0033248726105847897
Epoch 17/100: Training Loss: 0.0026623358000193212
Epoch 18/100: Training Loss: 0.002044926021272773
Epoch 19/100: Training Loss: 0.001674355457950112
Epoch 20/100: Training Loss: 0.002621540565364408
Epoch 21/100: Training Loss: 0.0025610011934444604
Epoch 22/100: Training Loss: 0.0024165140082504574
Epoch 23/100: Training Loss: 0.0025724006804409404
Epoch 24/100: Training Loss: 0.0026932696238258814
Epoch 25/100: Training Loss: 0.002055569989791769
Epoch 26/100: Training Loss: 0.0021366070437905016
Epoch 27/100: Training Loss: 0.0016455894077061029
Epoch 28/100: Training Loss: 0.0025391389202597914
Epoch 29/100: Training Loss: 0.00227183774607071
Epoch 30/100: Training Loss: 0.00206173492583218
Epoch 31/100: Training Loss: 0.0022959932191482443
Epoch 32/100: Training Loss: 0.0014203662509160327
Epoch 33/100: Training Loss: 0.0017061312467057184
Epoch 34/100: Training Loss: 0.0023302287850158893
Epoch 35/100: Training Loss: 0.0023812065456087226
Epoch 36/100: Training Loss: 0.00196928239816072
Epoch 37/100: Training Loss: 0.002212956843786682
Epoch 38/100: Training Loss: 0.0018894739498365794
Epoch 39/100: Training Loss: 0.0021123497296642786
Epoch 40/100: Training Loss: 0.0022754862608499087
Epoch 41/100: Training Loss: 0.0016522322664197708
Epoch 42/100: Training Loss: 0.0018640452662840583
Epoch 43/100: Training Loss: 0.0019491792514624185
Epoch 44/100: Training Loss: 0.0020865734839281497
Epoch 45/100: Training Loss: 0.0011546237776611024
Epoch 46/100: Training Loss: 0.0014582483578991416
Epoch 47/100: Training Loss: 0.0017727804894478905
Epoch 48/100: Training Loss: 0.000951688415956813
Epoch 49/100: Training Loss: 0.0011970963304405971
Epoch 50/100: Training Loss: 0.0013917988499268792
Epoch 51/100: Training Loss: 0.001592586569438707
Epoch 52/100: Training Loss: 0.0019573928504590167
Epoch 53/100: Training Loss: 0.0015845412252754566
Epoch 54/100: Training Loss: 0.0012399072678673346
Epoch 55/100: Training Loss: 0.0012394497726137274
Epoch 56/100: Training Loss: 0.0017296786339867194
Epoch 57/100: Training Loss: 0.0019496931934988263
Epoch 58/100: Training Loss: 0.001883233225108772
Epoch 59/100: Training Loss: 0.0016361648673253344
Epoch 60/100: Training Loss: 0.0016930671717157427
Epoch 61/100: Training Loss: 0.0016171235517160782
Epoch 62/100: Training Loss: 0.001341424240971243
Epoch 63/100: Training Loss: 0.0015783580328455034
Epoch 64/100: Training Loss: 0.0012733805061176124
Epoch 65/100: Training Loss: 0.0014472201170510803
Epoch 66/100: Training Loss: 0.0011942420179480748
Epoch 67/100: Training Loss: 0.0014404187731395495
Epoch 68/100: Training Loss: 0.0011224050987635227
Epoch 69/100: Training Loss: 0.0012608539584456691
Epoch 70/100: Training Loss: 0.0010793940318341287
Epoch 71/100: Training Loss: 0.001365012088358797
Epoch 72/100: Training Loss: 0.0012377895266804475
Epoch 73/100: Training Loss: 0.0012812386482756659
Epoch 74/100: Training Loss: 0.0013165164072781999
Epoch 75/100: Training Loss: 0.0013405739866345133
Epoch 76/100: Training Loss: 0.0007376097981503468
Epoch 77/100: Training Loss: 0.0009299867003169281
Epoch 78/100: Training Loss: 0.0008979471314032346
Epoch 79/100: Training Loss: 0.0008570455162730438
Epoch 80/100: Training Loss: 0.0009145705115716189
Epoch 81/100: Training Loss: 0.000616350020004424
Epoch 82/100: Training Loss: 0.0009033389833589264
Epoch 83/100: Training Loss: 0.0004109439520251672
Epoch 84/100: Training Loss: 0.001052210366489082
Epoch 85/100: Training Loss: 0.0008126963053318049
Epoch 86/100: Training Loss: 0.001225622481857704
Epoch 87/100: Training Loss: 0.0007595128274911287
Epoch 88/100: Training Loss: 0.00056831736043589
Epoch 89/100: Training Loss: 0.0002360283736361573
Epoch 90/100: Training Loss: 0.0005656863186533087
Epoch 91/100: Training Loss: 0.0009089740893698686
Epoch 92/100: Training Loss: 0.001301452024093527
Epoch 93/100: Training Loss: 0.0010029292659254263
Epoch 94/100: Training Loss: 0.0008087290438595198
Epoch 95/100: Training Loss: 0.000738690872460801
Epoch 96/100: Training Loss: 0.000959260475556582
Epoch 97/100: Training Loss: 0.0005129502703811948
Epoch 98/100: Training Loss: 0.000599379324360399
Epoch 99/100: Training Loss: 0.0006800307738070457
Epoch 0/100: Training Loss: 0.00405892078450184
Epoch 1/100: Training Loss: 0.002395028704839037
Epoch 2/100: Training Loss: 0.002768154570598476
Epoch 3/100: Training Loss: 0.002799251813762235
Epoch 4/100: Training Loss: 0.002813779952510303
Epoch 5/100: Training Loss: 0.002572427522267727
Epoch 6/100: Training Loss: 0.002786060437461398
Epoch 7/100: Training Loss: 0.0031311435020522564
Epoch 8/100: Training Loss: 0.0034727829181595356
Epoch 9/100: Training Loss: 0.003757230888139333
Epoch 10/100: Training Loss: 0.002388566535040243
Epoch 11/100: Training Loss: 0.0026742044268854405
Epoch 12/100: Training Loss: 0.0028428243880240333
Epoch 13/100: Training Loss: 0.004421599258650218
Epoch 14/100: Training Loss: 0.0030468952971578435
Epoch 15/100: Training Loss: 0.0036912457832437478
Epoch 16/100: Training Loss: 0.0024393698237589654
Epoch 17/100: Training Loss: 0.0028930908402070303
Epoch 18/100: Training Loss: 0.0015135202186786576
Epoch 19/100: Training Loss: 0.0032004238359186034
Epoch 20/100: Training Loss: 0.003790818697569386
Epoch 21/100: Training Loss: 0.002245305784490724
Epoch 22/100: Training Loss: 0.003168936399434576
Epoch 23/100: Training Loss: 0.003210103867069775
Epoch 24/100: Training Loss: 0.002619332035645744
Epoch 25/100: Training Loss: 0.002343342004232849
Epoch 26/100: Training Loss: 0.0022642355880989934
Epoch 27/100: Training Loss: 0.0018752339659937171
Epoch 28/100: Training Loss: 0.001626308291953131
Epoch 29/100: Training Loss: 0.0014663425502398156
Epoch 30/100: Training Loss: 0.0017638474900201456
Epoch 31/100: Training Loss: 0.0026666624656576194
Epoch 32/100: Training Loss: 0.001773337654720079
Epoch 33/100: Training Loss: 0.0024155944783166546
Epoch 34/100: Training Loss: 0.0020750997871752607
Epoch 35/100: Training Loss: 0.001541012367665373
Epoch 36/100: Training Loss: 0.0017274112890887734
Epoch 37/100: Training Loss: 0.001368189686970995
Epoch 38/100: Training Loss: 0.0016171592750296687
Epoch 39/100: Training Loss: 0.002009314811782332
Epoch 40/100: Training Loss: 0.0014293237237740827
Epoch 41/100: Training Loss: 0.0022154142525022394
Epoch 42/100: Training Loss: 0.0019656080283866023
Epoch 43/100: Training Loss: 0.0021815641431619
Epoch 44/100: Training Loss: 0.0017514244610110656
Epoch 45/100: Training Loss: 0.0019173918180907799
Epoch 46/100: Training Loss: 0.0015199496256594626
Epoch 47/100: Training Loss: 0.0018373031095163712
Epoch 48/100: Training Loss: 0.001518074053012772
Epoch 49/100: Training Loss: 0.001530919446060989
Epoch 50/100: Training Loss: 0.0014395216442891303
Epoch 51/100: Training Loss: 0.0010399987168659437
Epoch 52/100: Training Loss: 0.0020546423678366554
Epoch 53/100: Training Loss: 0.0021887540027795247
Epoch 54/100: Training Loss: 0.001893377856702994
Epoch 55/100: Training Loss: 0.0016311201828205033
Epoch 56/100: Training Loss: 0.002411790636201568
Epoch 57/100: Training Loss: 0.0011252770755464667
Epoch 58/100: Training Loss: 0.001625021364515191
Epoch 59/100: Training Loss: 0.001594644311248072
Epoch 60/100: Training Loss: 0.0020413169797682605
Epoch 61/100: Training Loss: 0.001627821499938207
Epoch 62/100: Training Loss: 0.0014931954295429963
Epoch 63/100: Training Loss: 0.0010287514980265636
Epoch 64/100: Training Loss: 0.0016020310438231917
Epoch 65/100: Training Loss: 0.0011074321948929338
Epoch 66/100: Training Loss: 0.0010557282247290706
Epoch 67/100: Training Loss: 0.0009806435431865667
Epoch 68/100: Training Loss: 0.0011688227092982917
Epoch 69/100: Training Loss: 0.001156604053168897
Epoch 70/100: Training Loss: 0.001052167932718795
Epoch 71/100: Training Loss: 0.0009558428794342951
Epoch 72/100: Training Loss: 0.0008611617893572674
Epoch 73/100: Training Loss: 0.0008636513687916939
Epoch 74/100: Training Loss: 0.0008942421698412359
Epoch 75/100: Training Loss: 0.0011425343965063032
Epoch 76/100: Training Loss: 0.0008105817713484858
Epoch 77/100: Training Loss: 0.0011814985646317337
Epoch 78/100: Training Loss: 0.001048891552236696
Epoch 79/100: Training Loss: 0.0008132204117364441
Epoch 80/100: Training Loss: 0.0010555836538605342
Epoch 81/100: Training Loss: 0.0007740236966815216
Epoch 82/100: Training Loss: 0.0008063823576794555
Epoch 83/100: Training Loss: 0.0009404646837158708
Epoch 84/100: Training Loss: 0.000519256718111354
Epoch 85/100: Training Loss: 0.0009425686092566181
Epoch 86/100: Training Loss: 0.00033792246453809424
Epoch 87/100: Training Loss: 0.0003365402091417881
Epoch 88/100: Training Loss: 0.0007054312339681663
Epoch 89/100: Training Loss: 0.00039795519697745114
Epoch 90/100: Training Loss: 0.0014476512638938348
Epoch 91/100: Training Loss: 0.0012043761891245052
Epoch 92/100: Training Loss: 0.0011809241298018703
Epoch 93/100: Training Loss: 0.0011588082408273456
Epoch 94/100: Training Loss: 0.0011474979634316551
Epoch 95/100: Training Loss: 0.001077433591646864
Epoch 96/100: Training Loss: 0.0011390660772260452
Epoch 97/100: Training Loss: 0.0012909938760151136
Epoch 98/100: Training Loss: 0.0014648052635571815
Epoch 99/100: Training Loss: 0.0017305545459519948
Epoch 0/100: Training Loss: 0.0028135837877497952
Epoch 1/100: Training Loss: 0.0023111532716190114
Epoch 2/100: Training Loss: 0.0018283039331436156
Epoch 3/100: Training Loss: 0.002288037889144
Epoch 4/100: Training Loss: 0.001958204367581536
Epoch 5/100: Training Loss: 0.0009611832744934979
Epoch 6/100: Training Loss: 0.0007868288194431978
Epoch 7/100: Training Loss: 0.000894850492477417
Epoch 8/100: Training Loss: 0.0014832370421465706
Epoch 9/100: Training Loss: 0.0014770388603210448
Epoch 10/100: Training Loss: 0.0018572901978212244
Epoch 11/100: Training Loss: 0.00014408423620111802
Epoch 12/100: Training Loss: 0.0009472133482203764
Epoch 13/100: Training Loss: 0.0013067713555167703
Epoch 14/100: Training Loss: 0.0005887509268872878
Epoch 15/100: Training Loss: 0.0011392801123506883
Epoch 16/100: Training Loss: 0.0009918403099564944
Epoch 17/100: Training Loss: 0.0011630003066623912
Epoch 18/100: Training Loss: 0.0011569468414082246
Epoch 19/100: Training Loss: 0.00035113755832700167
Epoch 20/100: Training Loss: 0.0010217163492651548
Epoch 21/100: Training Loss: 0.0007326110320932725
Epoch 22/100: Training Loss: 0.0003645885516615475
Epoch 23/100: Training Loss: 0.000898766254677492
Epoch 24/100: Training Loss: 0.0004654605160741245
Epoch 25/100: Training Loss: 0.00012363819953273324
Epoch 26/100: Training Loss: 0.0007593201363787932
Epoch 27/100: Training Loss: 0.0005708342089372523
Epoch 28/100: Training Loss: 0.0005695024833959691
Epoch 29/100: Training Loss: 0.0007592476466122796
Epoch 30/100: Training Loss: 0.0006053569124025457
Epoch 31/100: Training Loss: 0.0006409031503340777
Epoch 32/100: Training Loss: 0.0005806940443375531
Epoch 33/100: Training Loss: 0.001038250677725848
Epoch 34/100: Training Loss: 0.0005930440829080693
Epoch 35/100: Training Loss: 0.0001261414960026741
Epoch 36/100: Training Loss: 0.0005379453739699196
Epoch 37/100: Training Loss: 5.24190637995215e-05
Epoch 38/100: Training Loss: 0.0007221356910817764
Epoch 39/100: Training Loss: 0.0006130254443954019
Epoch 40/100: Training Loss: 0.0004915649400037878
Epoch 41/100: Training Loss: 0.00029633691643967346
Epoch 42/100: Training Loss: 0.00033288216766189126
Epoch 43/100: Training Loss: 0.0004995646283907049
Epoch 44/100: Training Loss: 0.0004248757572735057
Epoch 45/100: Training Loss: 0.0006466551738626816
Epoch 46/100: Training Loss: 0.00046238066519007964
Epoch 47/100: Training Loss: 0.00011123523992650649
Epoch 48/100: Training Loss: 0.0005631298703305861
Epoch 49/100: Training Loss: 9.895127047510708e-05
Epoch 50/100: Training Loss: 0.0005238692550098195
Epoch 51/100: Training Loss: 0.00046566001632634334
Epoch 52/100: Training Loss: 0.0002907964017461328
Epoch 53/100: Training Loss: 5.183712414958898e-05
Epoch 54/100: Training Loss: 0.0004056306446299833
Epoch 55/100: Training Loss: 3.527540932683384e-05
Epoch 56/100: Training Loss: 7.138212814050563e-05
Epoch 57/100: Training Loss: 0.00035051387899062215
Epoch 58/100: Training Loss: 0.00024099385037141687
Epoch 59/100: Training Loss: 0.0013554837773827946
Epoch 60/100: Training Loss: 0.00012176366632475573
Epoch 61/100: Training Loss: 0.000187984700588619
Epoch 62/100: Training Loss: 1.8965484410086098e-05
Epoch 63/100: Training Loss: 5.444153485929265e-05
Epoch 64/100: Training Loss: 0.0003673997872016009
Epoch 65/100: Training Loss: 2.5754380861625953e-05
Epoch 66/100: Training Loss: 3.43316316823749e-05
Epoch 67/100: Training Loss: 0.00010899068678126616
Epoch 68/100: Training Loss: 0.00011591365670456605
Epoch 69/100: Training Loss: 6.243432598078953e-05
Epoch 70/100: Training Loss: 7.83547485137687e-05
Epoch 71/100: Training Loss: 8.034033372121698e-05
Epoch 72/100: Training Loss: 0.00018151532858610154
Epoch 73/100: Training Loss: 4.354307756704443e-05
Epoch 74/100: Training Loss: 0.00015115535215419883
Epoch 75/100: Training Loss: 3.835906219833037e-05
Epoch 76/100: Training Loss: 3.479578094009091e-05
Epoch 77/100: Training Loss: 0.00010135814766673481
Epoch 78/100: Training Loss: 4.553141291527187e-05
Epoch 79/100: Training Loss: 0.0004978986785692327
Epoch 80/100: Training Loss: 2.430447596399223e-05
Epoch 81/100: Training Loss: 8.278036599650103e-05
Epoch 82/100: Training Loss: 3.5333562204066445e-05
Epoch 83/100: Training Loss: 0.0008501957444583668
Epoch 84/100: Training Loss: 2.4252825909677675e-05
Epoch 85/100: Training Loss: 6.15004778784864e-05
Epoch 86/100: Training Loss: 4.641160588054096e-05
Epoch 87/100: Training Loss: 1.599478453178616e-05
Epoch 88/100: Training Loss: 0.0004124377580250011
Epoch 89/100: Training Loss: 3.799695850295179e-05
Epoch 90/100: Training Loss: 2.940647742327522e-05
Epoch 91/100: Training Loss: 2.7061976930674385e-05
Epoch 92/100: Training Loss: 2.1189458065611473e-05
Epoch 93/100: Training Loss: 4.091408541973899e-05
Epoch 94/100: Training Loss: 1.5325797721743584e-05
Epoch 95/100: Training Loss: 2.8477362630998386e-05
Epoch 96/100: Training Loss: 4.114834865664734e-05
Epoch 97/100: Training Loss: 1.8281171865323012e-05
Epoch 98/100: Training Loss: 3.3819927450488596e-05
Epoch 99/100: Training Loss: 1.1345164110774502e-05
Epoch 0/100: Training Loss: 0.0032392095117007983
Epoch 1/100: Training Loss: 0.0022230304339352774
Epoch 2/100: Training Loss: 0.001587617923231686
Epoch 3/100: Training Loss: 0.002282363176345825
Epoch 4/100: Training Loss: 0.0006176536574083216
Epoch 5/100: Training Loss: 0.0018769560491337495
Epoch 6/100: Training Loss: 0.0010727076846010544
Epoch 7/100: Training Loss: 0.0017174058100756476
Epoch 8/100: Training Loss: 0.0007248062859563267
Epoch 9/100: Training Loss: 0.0006957927609191222
Epoch 10/100: Training Loss: 0.00034053351949242983
Epoch 11/100: Training Loss: 0.0004641258103006026
Epoch 12/100: Training Loss: 0.0004218925886294421
Epoch 13/100: Training Loss: 0.0009895483360571019
Epoch 14/100: Training Loss: 0.0012048472376430736
Epoch 15/100: Training Loss: 3.5384113845579765e-05
Epoch 16/100: Training Loss: 0.0006358125630547019
Epoch 17/100: Training Loss: 0.0008847489076502183
Epoch 18/100: Training Loss: 0.0005325247259700999
Epoch 19/100: Training Loss: 7.254303816486808e-05
Epoch 20/100: Training Loss: 0.0007152330349473392
Epoch 21/100: Training Loss: 0.00011275347103090847
Epoch 22/100: Training Loss: 0.000484809630057391
Epoch 23/100: Training Loss: 0.000308757384910303
Epoch 24/100: Training Loss: 7.563684245242792e-05
Epoch 25/100: Training Loss: 0.0008044686387566959
Epoch 26/100: Training Loss: 0.00020275924573926366
Epoch 27/100: Training Loss: 0.00019329790683353647
Epoch 28/100: Training Loss: 7.536750944221721e-05
Epoch 29/100: Training Loss: 0.00015389127108980628
Epoch 30/100: Training Loss: 0.00015180708073517856
Epoch 31/100: Training Loss: 0.00018046496107297785
Epoch 32/100: Training Loss: 9.179725585614934e-05
Epoch 33/100: Training Loss: 0.0006127735271173365
Epoch 34/100: Training Loss: 0.0006654404542025397
Epoch 35/100: Training Loss: 4.091061213437249e-05
Epoch 36/100: Training Loss: 7.188575999701724e-05
Epoch 37/100: Training Loss: 2.439076847889844e-05
Epoch 38/100: Training Loss: 0.0001900401623810039
Epoch 39/100: Training Loss: 7.751887006794705e-05
Epoch 40/100: Training Loss: 0.00012244464939131458
Epoch 41/100: Training Loss: 0.00022484020275228165
Epoch 42/100: Training Loss: 3.0066689257236088e-05
Epoch 43/100: Training Loss: 5.645565688610077e-05
Epoch 44/100: Training Loss: 0.0005664470441201154
Epoch 45/100: Training Loss: 5.724814117831342e-05
Epoch 46/100: Training Loss: 1.2442262788467548e-05
Epoch 47/100: Training Loss: 2.7463185217450648e-05
Epoch 48/100: Training Loss: 1.9750934477676364e-05
Epoch 49/100: Training Loss: 6.313846610924777e-05
Epoch 50/100: Training Loss: 0.00019164455725866205
Epoch 51/100: Training Loss: 0.00040086016935460706
Epoch 52/100: Training Loss: 5.4493108216454e-05
Epoch 53/100: Training Loss: 7.85176532671732e-05
Epoch 54/100: Training Loss: 6.266422021914931e-05
Epoch 55/100: Training Loss: 0.0001794798847507028
Epoch 56/100: Training Loss: 4.844287519945818e-05
Epoch 57/100: Training Loss: 7.176495787194546e-06
Epoch 58/100: Training Loss: 0.0002916495808783699
Epoch 59/100: Training Loss: 0.00012335975599639555
Epoch 60/100: Training Loss: 2.6735531933167402e-05
Epoch 61/100: Training Loss: 2.5433080051751698e-05
Epoch 62/100: Training Loss: 0.00012554057380732367
Epoch 63/100: Training Loss: 0.0002111994825741824
Epoch 64/100: Training Loss: 1.743898233946632e-05
Epoch 65/100: Training Loss: 3.568667589741595e-05
Epoch 66/100: Training Loss: 2.3437381776816706e-05
Epoch 67/100: Training Loss: 3.035710028865758e-05
Epoch 68/100: Training Loss: 8.636633043780046e-05
Epoch 69/100: Training Loss: 3.3189148149069615e-05
Epoch 70/100: Training Loss: 0.00014045956161092308
Epoch 71/100: Training Loss: 2.689597416011726e-05
Epoch 72/100: Training Loss: 1.5673970611875547e-06
Epoch 73/100: Training Loss: 9.568069842370118e-06
Epoch 74/100: Training Loss: 6.174656617290834e-05
Epoch 75/100: Training Loss: 0.0008410670301493477
Epoch 76/100: Training Loss: 1.8153280731947983e-05
Epoch 77/100: Training Loss: 2.7683204697335466e-05
Epoch 78/100: Training Loss: 1.9884013625628808e-05
Epoch 79/100: Training Loss: 6.8673679112073256e-06
Epoch 80/100: Training Loss: 4.242722506580108e-06
Epoch 81/100: Training Loss: 1.3706879690289497e-05
Epoch 82/100: Training Loss: 2.0866519223679514e-05
Epoch 83/100: Training Loss: 1.2403699185918358e-05
Epoch 84/100: Training Loss: 1.6412353964851182e-05
Epoch 85/100: Training Loss: 9.55416916814797e-06
Epoch 86/100: Training Loss: 1.966355724588913e-05
Epoch 87/100: Training Loss: 2.4674723938326624e-06
Epoch 88/100: Training Loss: 1.104990129961687e-05
Epoch 89/100: Training Loss: 7.752092856475536e-06
Epoch 90/100: Training Loss: 1.3556288938750239e-05
Epoch 91/100: Training Loss: 1.3061511494657572e-05
Epoch 92/100: Training Loss: 2.3131017737528856e-05
Epoch 93/100: Training Loss: 1.1191261000931263e-05
Epoch 94/100: Training Loss: 1.445532201186699e-05
Epoch 95/100: Training Loss: 4.4864240814657775e-05
Epoch 96/100: Training Loss: 1.0353304199216996e-05
Epoch 97/100: Training Loss: 1.6017000683966805e-05
Epoch 98/100: Training Loss: 2.0474899450645726e-05
Epoch 99/100: Training Loss: 1.1284928768873215e-05
Epoch 0/100: Training Loss: 0.0023320420699961047
Epoch 1/100: Training Loss: 0.003061602746739107
Epoch 2/100: Training Loss: 0.0030719914857079
Epoch 3/100: Training Loss: 0.00190581433913287
Epoch 4/100: Training Loss: 0.0009342167307348812
Epoch 5/100: Training Loss: 0.00232580304145813
Epoch 6/100: Training Loss: 0.0004868861068697537
Epoch 7/100: Training Loss: 0.0006219275295734405
Epoch 8/100: Training Loss: 0.0011556583292344038
Epoch 9/100: Training Loss: 0.00010556185289340861
Epoch 10/100: Training Loss: 0.000791285844410167
Epoch 11/100: Training Loss: 0.0008242387981975779
Epoch 12/100: Training Loss: 0.0012076069326961742
Epoch 13/100: Training Loss: 0.0011193403426338646
Epoch 14/100: Training Loss: 0.0009657931678435382
Epoch 15/100: Training Loss: 0.0003966374432339388
Epoch 16/100: Training Loss: 0.0002197596080162946
Epoch 17/100: Training Loss: 0.0002482337329317542
Epoch 18/100: Training Loss: 0.000203065004418878
Epoch 19/100: Training Loss: 0.0003926352104719947
Epoch 20/100: Training Loss: 0.0004257161827648387
Epoch 21/100: Training Loss: 0.00019545430207953734
Epoch 22/100: Training Loss: 9.245680754675585e-05
Epoch 23/100: Training Loss: 0.0004604741492692162
Epoch 24/100: Training Loss: 0.0003648216671803418
Epoch 25/100: Training Loss: 0.00044295358307221356
Epoch 26/100: Training Loss: 0.000644728190758649
Epoch 27/100: Training Loss: 0.0006829625543426065
Epoch 28/100: Training Loss: 0.0006280265310231377
Epoch 29/100: Training Loss: 0.0006024228299365324
Epoch 30/100: Training Loss: 2.147739698343417e-05
Epoch 31/100: Training Loss: 1.8037696752478094e-05
Epoch 32/100: Training Loss: 0.0002900505767149084
Epoch 33/100: Training Loss: 0.0006195832701290355
Epoch 34/100: Training Loss: 8.724419509663302e-05
Epoch 35/100: Training Loss: 0.0006207827259512509
Epoch 36/100: Training Loss: 4.0680052274290253e-05
Epoch 37/100: Training Loss: 1.578790491775555e-05
Epoch 38/100: Training Loss: 0.0005499746869592106
Epoch 39/100: Training Loss: 0.0005947112598839928
Epoch 40/100: Training Loss: 5.6655978893532475e-05
Epoch 41/100: Training Loss: 0.000305032598621705
Epoch 42/100: Training Loss: 0.0005590143449166242
Epoch 43/100: Training Loss: 7.509831339120865e-05
Epoch 44/100: Training Loss: 0.00016768632785362357
Epoch 45/100: Training Loss: 1.6909989747492707e-05
Epoch 46/100: Training Loss: 6.325840950012207e-05
Epoch 47/100: Training Loss: 0.0006509190096574671
Epoch 48/100: Training Loss: 0.0005228579044342041
Epoch 49/100: Training Loss: 2.123797175419681e-05
Epoch 50/100: Training Loss: 0.0004410415011293748
Epoch 51/100: Training Loss: 5.474059897310594e-05
Epoch 52/100: Training Loss: 0.0005115573897081263
Epoch 53/100: Training Loss: 0.0006215822609031902
Epoch 54/100: Training Loss: 0.000161881503813407
Epoch 55/100: Training Loss: 8.133931962006232e-05
Epoch 56/100: Training Loss: 6.800680421292782e-06
Epoch 57/100: Training Loss: 0.00011955414186505711
Epoch 58/100: Training Loss: 0.0001346974359715686
Epoch 59/100: Training Loss: 0.0005550110164810629
Epoch 60/100: Training Loss: 3.641214605201693e-05
Epoch 61/100: Training Loss: 0.00019505681360469144
Epoch 62/100: Training Loss: 0.001435294046121485
Epoch 63/100: Training Loss: 3.290917888721999e-05
Epoch 64/100: Training Loss: 9.422640398363857e-06
Epoch 65/100: Training Loss: 0.00017633977181771222
Epoch 66/100: Training Loss: 3.4855511587332275e-05
Epoch 67/100: Training Loss: 4.380017738131916e-05
Epoch 68/100: Training Loss: 4.1412414215943394e-05
Epoch 69/100: Training Loss: 4.2956316953196246e-05
Epoch 70/100: Training Loss: 0.00017886477358200972
Epoch 71/100: Training Loss: 0.000439897267257466
Epoch 72/100: Training Loss: 3.614883650751675e-05
Epoch 73/100: Training Loss: 5.446648334755617e-05
Epoch 74/100: Training Loss: 4.599455963162815e-05
Epoch 75/100: Training Loss: 0.00011443746659685584
Epoch 76/100: Training Loss: 8.49000342628535e-05
Epoch 77/100: Training Loss: 2.422380425474223e-05
Epoch 78/100: Training Loss: 6.378927432438906e-05
Epoch 79/100: Training Loss: 0.00011481678880312863
Epoch 80/100: Training Loss: 5.1068898071261013e-05
Epoch 81/100: Training Loss: 1.3980817268876469e-05
Epoch 82/100: Training Loss: 1.836749998962178e-05
Epoch 83/100: Training Loss: 1.5333134626202724e-05
Epoch 84/100: Training Loss: 2.5664116529857412e-05
Epoch 85/100: Training Loss: 4.132726394078311e-05
Epoch 86/100: Training Loss: 2.5685334249454387e-05
Epoch 87/100: Training Loss: 0.0009843674652716694
Epoch 88/100: Training Loss: 2.0773704711566953e-05
Epoch 89/100: Training Loss: 3.960929427515058e-05
Epoch 90/100: Training Loss: 1.8558526576003607e-05
Epoch 91/100: Training Loss: 0.0001514064476770513
Epoch 92/100: Training Loss: 4.1837688973721336e-05
Epoch 93/100: Training Loss: 5.413546939106549e-05
Epoch 94/100: Training Loss: 6.38068954953376e-06
Epoch 95/100: Training Loss: 2.450451857465155e-05
Epoch 96/100: Training Loss: 1.4490445199258188e-05
Epoch 97/100: Training Loss: 2.451596562476719e-05
Epoch 98/100: Training Loss: 0.002021244343589334
Epoch 99/100: Training Loss: 3.2920582110390946e-05
dataset: capitals layer_num_from_end:-1 Avg_acc:tensor(512.4493) Avg_AUC:0.8772742411669405 Avg_threshold:0.7701331476370493
dataset: inventions layer_num_from_end:-1 Avg_acc:tensor(475.7381) Avg_AUC:0.678541038946546 Avg_threshold:0.16826699177424112
dataset: elements layer_num_from_end:-1 Avg_acc:tensor(498.0444) Avg_AUC:0.6585470381932401 Avg_threshold:0.5977060993512472
dataset: animals layer_num_from_end:-1 Avg_acc:tensor(502.5417) Avg_AUC:0.641894605064248 Avg_threshold:0.6613416572411855
dataset: companies layer_num_from_end:-1 Avg_acc:tensor(504.2456) Avg_AUC:0.8355925925925926 Avg_threshold:0.5061971147855123
dataset: facts layer_num_from_end:-1 Avg_acc:tensor(489.3333) Avg_AUC:0.7675573497372805 Avg_threshold:0.7278543909390768
Epoch 0/100: Training Loss: 0.0035251316490706864
Epoch 1/100: Training Loss: 0.002629163173528818
Epoch 2/100: Training Loss: 0.004485143231345223
Epoch 3/100: Training Loss: 0.004541973967652221
Epoch 4/100: Training Loss: 0.003188695315714483
Epoch 5/100: Training Loss: 0.003460661931471391
Epoch 6/100: Training Loss: 0.003946318076207087
Epoch 7/100: Training Loss: 0.003794850169361888
Epoch 8/100: Training Loss: 0.003405732083153891
Epoch 9/100: Training Loss: 0.002407699614971668
Epoch 10/100: Training Loss: 0.002621286815696663
Epoch 11/100: Training Loss: 0.003037363290786743
Epoch 12/100: Training Loss: 0.003140393045398739
Epoch 13/100: Training Loss: 0.0019404006587875472
Epoch 14/100: Training Loss: 0.0032489620722257174
Epoch 15/100: Training Loss: 0.003348203388961045
Epoch 16/100: Training Loss: 0.0031664632000289595
Epoch 17/100: Training Loss: 0.002902424418842876
Epoch 18/100: Training Loss: 0.002218392345455143
Epoch 19/100: Training Loss: 0.0025729361947599825
Epoch 20/100: Training Loss: 0.0021741421489448816
Epoch 21/100: Training Loss: 0.0037570549891545223
Epoch 22/100: Training Loss: 0.0021046408406504382
Epoch 23/100: Training Loss: 0.0018861035366992017
Epoch 24/100: Training Loss: 0.0016178872201826188
Epoch 25/100: Training Loss: 0.0011881834768748783
Epoch 26/100: Training Loss: 0.0019399861355761548
Epoch 27/100: Training Loss: 0.0016939933900232916
Epoch 28/100: Training Loss: 0.0013754563731747074
Epoch 29/100: Training Loss: 0.0014323592185974121
Epoch 30/100: Training Loss: 0.0017669824036684904
Epoch 31/100: Training Loss: 0.0006018434162740107
Epoch 32/100: Training Loss: 0.0007298305839091748
Epoch 33/100: Training Loss: 0.0013481825590133667
Epoch 34/100: Training Loss: 0.0014875731268129148
Epoch 35/100: Training Loss: 0.0011915689581757658
Epoch 36/100: Training Loss: 0.002724212783199924
Epoch 37/100: Training Loss: 0.0014765983694916838
Epoch 38/100: Training Loss: 0.0016966770578931262
Epoch 39/100: Training Loss: 0.0009003260752537867
Epoch 40/100: Training Loss: 0.0015154422579945384
Epoch 41/100: Training Loss: 0.0016018405452474847
Epoch 42/100: Training Loss: 0.0014064122330058706
Epoch 43/100: Training Loss: 0.0014015179830831248
Epoch 44/100: Training Loss: 0.0016010151459620548
Epoch 45/100: Training Loss: 0.0008585958943500386
Epoch 46/100: Training Loss: 0.0007552291963483904
Epoch 47/100: Training Loss: 0.0008415435473402063
Epoch 48/100: Training Loss: 0.0006930511835571769
Epoch 49/100: Training Loss: 0.0001302124424414201
Epoch 50/100: Training Loss: 0.0009748020789006373
Epoch 51/100: Training Loss: 0.0014371107091436853
Epoch 52/100: Training Loss: 0.0006856718263426027
Epoch 53/100: Training Loss: 0.0013358003192848258
Epoch 54/100: Training Loss: 0.000784171002728122
Epoch 55/100: Training Loss: 0.0007106452346681715
Epoch 56/100: Training Loss: 0.001259637983528884
Epoch 57/100: Training Loss: 0.001473234041587456
Epoch 58/100: Training Loss: 0.0007004082932338848
Epoch 59/100: Training Loss: 0.0010897762083507084
Epoch 60/100: Training Loss: 0.0012764743158033677
Epoch 61/100: Training Loss: 0.0012287474595583403
Epoch 62/100: Training Loss: 0.0006934302774342624
Epoch 63/100: Training Loss: 0.0006806131426271026
Epoch 64/100: Training Loss: 0.00042629762962981537
Epoch 65/100: Training Loss: 0.0004310405233523229
Epoch 66/100: Training Loss: 0.000591519725072634
Epoch 67/100: Training Loss: 0.0005893989147006215
Epoch 68/100: Training Loss: 0.00037507957720256353
Epoch 69/100: Training Loss: 0.0003533704610137673
Epoch 70/100: Training Loss: 0.0002671285317494319
Epoch 71/100: Training Loss: 0.0011844067098377468
Epoch 72/100: Training Loss: 0.00023019923405213788
Epoch 73/100: Training Loss: 0.0004226317616192611
Epoch 74/100: Training Loss: 0.0003566508526568646
Epoch 75/100: Training Loss: 0.00032511389964110367
Epoch 76/100: Training Loss: 0.00029965662039243255
Epoch 77/100: Training Loss: 0.0003156422839298115
Epoch 78/100: Training Loss: 0.0006879480568679063
Epoch 79/100: Training Loss: 8.608243213250086e-05
Epoch 80/100: Training Loss: 0.00037720538936294876
Epoch 81/100: Training Loss: 0.00029771225331546544
Epoch 82/100: Training Loss: 0.00017730112780224192
Epoch 83/100: Training Loss: 0.0002448149806969649
Epoch 84/100: Training Loss: 0.0005852911438975301
Epoch 85/100: Training Loss: 0.0003832685989099783
Epoch 86/100: Training Loss: 0.00029772965537084564
Epoch 87/100: Training Loss: 0.0002648390412747443
Epoch 88/100: Training Loss: 0.0002839645774631234
Epoch 89/100: Training Loss: 0.0004299527906871342
Epoch 90/100: Training Loss: 0.00023621552578219167
Epoch 91/100: Training Loss: 0.0003955229283212782
Epoch 92/100: Training Loss: 0.00023171016490542806
Epoch 93/100: Training Loss: 0.0004350137668889719
Epoch 94/100: Training Loss: 0.0008409989255291599
Epoch 95/100: Training Loss: 0.0002542359011990207
Epoch 96/100: Training Loss: 0.0005594563859326022
Epoch 97/100: Training Loss: 0.0002490572006135554
Epoch 98/100: Training Loss: 0.0005278052775176255
Epoch 99/100: Training Loss: 0.000406172621500242
Epoch 0/100: Training Loss: 0.0046692177965924455
Epoch 1/100: Training Loss: 0.004396615745304348
Epoch 2/100: Training Loss: 0.004577372040781942
Epoch 3/100: Training Loss: 0.0030337797178255094
Epoch 4/100: Training Loss: 0.00369354686536989
Epoch 5/100: Training Loss: 0.0028256159145515283
Epoch 6/100: Training Loss: 0.00402570187628686
Epoch 7/100: Training Loss: 0.0027007448089706315
Epoch 8/100: Training Loss: 0.005971338365461443
Epoch 9/100: Training Loss: 0.005073601549321955
Epoch 10/100: Training Loss: 0.004539081266709975
Epoch 11/100: Training Loss: 0.003586859136194616
Epoch 12/100: Training Loss: 0.0015668107287867086
Epoch 13/100: Training Loss: 0.002817292313475709
