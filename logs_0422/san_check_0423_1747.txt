2024-04-23 21:47:19.773176: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-04-23 21:47:20.840506: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
================layer -5================
Epoch 0/100: Training Loss: 0.0020253316505805595
Epoch 1/100: Training Loss: 0.002865568115994647
Epoch 2/100: Training Loss: 0.0025433612453353988
Epoch 3/100: Training Loss: 0.00286536366789491
Epoch 4/100: Training Loss: 0.003342969851060347
Epoch 5/100: Training Loss: 0.0032306373952985645
Epoch 6/100: Training Loss: 0.0024469678218548116
Epoch 7/100: Training Loss: 0.0027330749101572103
Epoch 8/100: Training Loss: 0.0022654387500736263
Epoch 9/100: Training Loss: 0.0024893588119453483
Epoch 10/100: Training Loss: 0.0032039480609493656
Epoch 11/100: Training Loss: 0.002973478782427061
Epoch 12/100: Training Loss: 0.0022869512334570186
Epoch 13/100: Training Loss: 0.0026301458165362166
Epoch 14/100: Training Loss: 0.0018856998506959501
Epoch 15/100: Training Loss: 0.0017024500595106112
Epoch 16/100: Training Loss: 0.0017092138320416003
Epoch 17/100: Training Loss: 0.00174431096423756
Epoch 18/100: Training Loss: 0.0016122526013767802
Epoch 19/100: Training Loss: 0.0013319017378600328
Epoch 20/100: Training Loss: 0.001967093327662328
Epoch 21/100: Training Loss: 0.0013644016914434367
Epoch 22/100: Training Loss: 0.0011621060279699473
Epoch 23/100: Training Loss: 0.0012292428450150924
Epoch 24/100: Training Loss: 0.0015602743292188312
Epoch 25/100: Training Loss: 0.0012917285198931927
Epoch 26/100: Training Loss: 0.0011621593803792566
Epoch 27/100: Training Loss: 0.0012985755513598035
Epoch 28/100: Training Loss: 0.0007908581332726912
Epoch 29/100: Training Loss: 0.0009590101825607407
Epoch 30/100: Training Loss: 0.001776410566343294
Epoch 31/100: Training Loss: 0.0010414895686236296
Epoch 32/100: Training Loss: 0.0016068494194871061
Epoch 33/100: Training Loss: 0.0010412937694496208
Epoch 34/100: Training Loss: 0.0011363561962034318
Epoch 35/100: Training Loss: 0.0012349734356353332
Epoch 36/100: Training Loss: 0.0010349920371195652
Epoch 37/100: Training Loss: 0.000964152750435409
Epoch 38/100: Training Loss: 0.0005942888401605032
Epoch 39/100: Training Loss: 0.0009597634727304632
Epoch 40/100: Training Loss: 0.0009646135401892496
Epoch 41/100: Training Loss: 0.0008523538708686829
Epoch 42/100: Training Loss: 0.0009479365357152232
Epoch 43/100: Training Loss: 0.000970748546240213
Epoch 44/100: Training Loss: 0.0011397673116697298
Epoch 45/100: Training Loss: 0.0012236234399822209
Epoch 46/100: Training Loss: 0.001376535404812206
Epoch 47/100: Training Loss: 0.0009098721967710482
Epoch 48/100: Training Loss: 0.0007830789977020317
Epoch 49/100: Training Loss: 0.0011373301902851025
Epoch 50/100: Training Loss: 0.0008752303315209342
Epoch 51/100: Training Loss: 0.0007831014536477469
Epoch 52/100: Training Loss: 0.0007674550780883202
Epoch 53/100: Training Loss: 0.0013610435115707503
Epoch 54/100: Training Loss: 0.0009452624546064364
Epoch 55/100: Training Loss: 0.0008468310628737603
Epoch 56/100: Training Loss: 0.0008508701111886885
Epoch 57/100: Training Loss: 0.0007676033081708255
Epoch 58/100: Training Loss: 0.0010427537706348446
Epoch 59/100: Training Loss: 0.0008608669667810827
Epoch 60/100: Training Loss: 0.0007782234074352504
Epoch 61/100: Training Loss: 0.0006688896280068618
Epoch 62/100: Training Loss: 0.001084564970089839
Epoch 63/100: Training Loss: 0.0005038698444833289
Epoch 64/100: Training Loss: 0.0006148648428750204
Epoch 65/100: Training Loss: 0.0011558792182615586
Epoch 66/100: Training Loss: 0.0008269924071285274
Epoch 67/100: Training Loss: 0.0008348306784263024
Epoch 68/100: Training Loss: 0.0008434230214232331
Epoch 69/100: Training Loss: 0.0007500935059327346
Epoch 70/100: Training Loss: 0.0006147378182911373
Epoch 71/100: Training Loss: 0.0008636785449681583
Epoch 72/100: Training Loss: 0.0006841112162683393
Epoch 73/100: Training Loss: 0.0008886465867916188
Epoch 74/100: Training Loss: 0.0005202977494759994
Epoch 75/100: Training Loss: 0.0022603658946244035
Epoch 76/100: Training Loss: 0.0008378307094107141
Epoch 77/100: Training Loss: 0.00041649560932512884
Epoch 78/100: Training Loss: 0.0010280926952828894
Epoch 79/100: Training Loss: 0.0007290417602012207
Epoch 80/100: Training Loss: 0.0008701989492336354
Epoch 81/100: Training Loss: 0.000781592997637662
Epoch 82/100: Training Loss: 0.0008303312050712692
Epoch 83/100: Training Loss: 0.0007552338334230277
Epoch 84/100: Training Loss: 0.0009027471492340515
Epoch 85/100: Training Loss: 0.00080883195558628
Epoch 86/100: Training Loss: 0.0005143045649661885
Epoch 87/100: Training Loss: 0.00036787345901235835
Epoch 88/100: Training Loss: 0.00037768881741937225
Epoch 89/100: Training Loss: 0.0006184428409262971
Epoch 90/100: Training Loss: 0.0004943164807933194
Epoch 91/100: Training Loss: 0.0006957553274981625
Epoch 92/100: Training Loss: 0.0004058660275035805
Epoch 93/100: Training Loss: 0.0007344332712513583
Epoch 94/100: Training Loss: 0.00039420860422241104
Epoch 95/100: Training Loss: 0.0006005217457984711
Epoch 96/100: Training Loss: 0.00038638747447020523
Epoch 97/100: Training Loss: 0.0007151802415614362
Epoch 98/100: Training Loss: 0.0004099404530508535
Epoch 99/100: Training Loss: 0.0006787931689849267
Epoch 0/100: Training Loss: 0.002585416907197112
Epoch 1/100: Training Loss: 0.0027553653800404154
Epoch 2/100: Training Loss: 0.001993789539470539
Epoch 3/100: Training Loss: 0.0026226520955145776
Epoch 4/100: Training Loss: 0.0022397514406617704
Epoch 5/100: Training Loss: 0.0035535309698198226
Epoch 6/100: Training Loss: 0.0029896007967995597
Epoch 7/100: Training Loss: 0.002669349625394061
Epoch 8/100: Training Loss: 0.002545287975898156
Epoch 9/100: Training Loss: 0.0025767677730613656
Epoch 10/100: Training Loss: 0.002272238264550696
Epoch 11/100: Training Loss: 0.0019039003165451796
Epoch 12/100: Training Loss: 0.0015327022834257646
Epoch 13/100: Training Loss: 0.002550976051317228
Epoch 14/100: Training Loss: 0.002244192195105386
Epoch 15/100: Training Loss: 0.0015773963886541087
Epoch 16/100: Training Loss: 0.0012989104627729295
Epoch 17/100: Training Loss: 0.0017082440061169071
Epoch 18/100: Training Loss: 0.0011567674525134214
Epoch 19/100: Training Loss: 0.0015161475101550977
Epoch 20/100: Training Loss: 0.0015154324628256417
Epoch 21/100: Training Loss: 0.0015698734905336287
Epoch 22/100: Training Loss: 0.002131514199130185
Epoch 23/100: Training Loss: 0.0011514780613092275
Epoch 24/100: Training Loss: 0.0011231189632749225
Epoch 25/100: Training Loss: 0.0013193687150528381
Epoch 26/100: Training Loss: 0.0009216084763720319
Epoch 27/100: Training Loss: 0.001173672455174106
Epoch 28/100: Training Loss: 0.0012803993650249668
Epoch 29/100: Training Loss: 0.0011443645804078428
Epoch 30/100: Training Loss: 0.0010476889726998922
Epoch 31/100: Training Loss: 0.0013004876933731398
Epoch 32/100: Training Loss: 0.0008643654051360551
Epoch 33/100: Training Loss: 0.0009318585996027593
Epoch 34/100: Training Loss: 0.001057631277537846
Epoch 35/100: Training Loss: 0.0009941975553552588
Epoch 36/100: Training Loss: 0.001025370263553166
Epoch 37/100: Training Loss: 0.000941229866934823
Epoch 38/100: Training Loss: 0.0003306653897662263
Epoch 39/100: Training Loss: 0.0008590890394224154
Epoch 40/100: Training Loss: 0.0006491737765865726
Epoch 41/100: Training Loss: 0.0009584742617773843
Epoch 42/100: Training Loss: 0.0007052457520178148
Epoch 43/100: Training Loss: 0.0005525812715083569
Epoch 44/100: Training Loss: 0.0004062244890036283
Epoch 45/100: Training Loss: 0.0005119774308237996
Epoch 46/100: Training Loss: 0.0004195426623304407
Epoch 47/100: Training Loss: 0.0010851976963189931
Epoch 48/100: Training Loss: 0.00046369407352034027
Epoch 49/100: Training Loss: 0.0008895264847295268
Epoch 50/100: Training Loss: 0.0010578492721477588
Epoch 51/100: Training Loss: 0.0006085681644353
Epoch 52/100: Training Loss: 0.0007068216279670076
Epoch 53/100: Training Loss: 0.0005826468130091687
Epoch 54/100: Training Loss: 0.0013539336778067208
Epoch 55/100: Training Loss: 0.0011852561802297205
Epoch 56/100: Training Loss: 0.000939801126926929
Epoch 57/100: Training Loss: 0.0007440139884715314
Epoch 58/100: Training Loss: 0.0009699735816542085
Epoch 59/100: Training Loss: 0.0007624223932519659
Epoch 60/100: Training Loss: 0.0006982034944987797
Epoch 61/100: Training Loss: 0.0006390516157750483
Epoch 62/100: Training Loss: 0.0006133246046679837
Epoch 63/100: Training Loss: 0.0004619064030947385
Epoch 64/100: Training Loss: 0.0008065497854372838
Epoch 65/100: Training Loss: 0.0003270465177255911
Epoch 66/100: Training Loss: 0.0005107724166416622
Epoch 67/100: Training Loss: 0.0008969813376873523
Epoch 68/100: Training Loss: 0.0005829045614162525
Epoch 69/100: Training Loss: 0.0016215337323142098
Epoch 70/100: Training Loss: 0.0002333441345424919
Epoch 71/100: Training Loss: 0.0007467315226168065
Epoch 72/100: Training Loss: 0.0006656244501367316
Epoch 73/100: Training Loss: 0.0005884570154276761
Epoch 74/100: Training Loss: 0.0008361988238521389
Epoch 75/100: Training Loss: 0.0006495205672470839
Epoch 76/100: Training Loss: 0.0003246055928977219
Epoch 77/100: Training Loss: 0.0005773284010120205
Epoch 78/100: Training Loss: 0.0001267091455784711
Epoch 79/100: Training Loss: 0.0003952613243689904
Epoch 80/100: Training Loss: 0.0005309449417607768
Epoch 81/100: Training Loss: 0.0008098660232303859
Epoch 82/100: Training Loss: 0.000640841422381101
Epoch 83/100: Training Loss: 0.0003913370566768246
Epoch 84/100: Training Loss: 0.00017664405693124224
Epoch 85/100: Training Loss: 0.0007277854776882625
Epoch 86/100: Training Loss: 0.0003320883464563143
Epoch 87/100: Training Loss: 0.00040862224735580124
Epoch 88/100: Training Loss: 0.00020165849555205633
Epoch 89/100: Training Loss: 0.0006461859895632818
Epoch 90/100: Training Loss: 0.0011542225842709308
Epoch 91/100: Training Loss: 0.0009253240757055215
Epoch 92/100: Training Loss: 0.00045310622537052714
Epoch 93/100: Training Loss: 0.0004086758602749218
Epoch 94/100: Training Loss: 0.00023656666695654808
Epoch 95/100: Training Loss: 0.00033206857376165324
Epoch 96/100: Training Loss: 0.0006040578538721257
Epoch 97/100: Training Loss: 0.0004413500428199768
Epoch 98/100: Training Loss: 0.0007676474906347849
Epoch 99/100: Training Loss: 0.0003817639202921541
Epoch 0/100: Training Loss: 0.0018596951361302728
Epoch 1/100: Training Loss: 0.002729253335432573
Epoch 2/100: Training Loss: 0.002912657869445694
Epoch 3/100: Training Loss: 0.0023537044758563273
Epoch 4/100: Training Loss: 0.0025860294178649264
Epoch 5/100: Training Loss: 0.0032556588416332967
Epoch 6/100: Training Loss: 0.0030411988288372546
Epoch 7/100: Training Loss: 0.0033944747247895996
Epoch 8/100: Training Loss: 0.0026833879780936076
Epoch 9/100: Training Loss: 0.0025512701147919767
Epoch 10/100: Training Loss: 0.002614364340588763
Epoch 11/100: Training Loss: 0.0025041140876449907
Epoch 12/100: Training Loss: 0.0024653170909081307
Epoch 13/100: Training Loss: 0.0019998329502719267
Epoch 14/100: Training Loss: 0.001672165689768491
Epoch 15/100: Training Loss: 0.0014319632436845687
Epoch 16/100: Training Loss: 0.001586852373776736
Epoch 17/100: Training Loss: 0.0008605481548742814
Epoch 18/100: Training Loss: 0.001798239829656961
Epoch 19/100: Training Loss: 0.0012094354504471891
Epoch 20/100: Training Loss: 0.001431846952104902
Epoch 21/100: Training Loss: 0.0013856975348679336
Epoch 22/100: Training Loss: 0.0009485234122176271
Epoch 23/100: Training Loss: 0.0012157520839384385
Epoch 24/100: Training Loss: 0.0011907655459183913
Epoch 25/100: Training Loss: 0.00119297842045764
Epoch 26/100: Training Loss: 0.0008865349984669185
Epoch 27/100: Training Loss: 0.0010319654341344232
Epoch 28/100: Training Loss: 0.0005710031706970055
Epoch 29/100: Training Loss: 0.0012831057493503278
Epoch 30/100: Training Loss: 0.0004084238691346629
Epoch 31/100: Training Loss: 0.0012532665387733834
Epoch 32/100: Training Loss: 0.0010989553236461186
Epoch 33/100: Training Loss: 0.0011683573256005774
Epoch 34/100: Training Loss: 0.0010724289642347323
Epoch 35/100: Training Loss: 0.0003973870323254512
Epoch 36/100: Training Loss: 0.0009584300793134249
Epoch 37/100: Training Loss: 0.000981901596476148
Epoch 38/100: Training Loss: 0.0014611406551374422
Epoch 39/100: Training Loss: 0.0009584117394227248
Epoch 40/100: Training Loss: 0.0007615975149861582
Epoch 41/100: Training Loss: 0.0009997279702366648
Epoch 42/100: Training Loss: 0.0005667127304143838
Epoch 43/100: Training Loss: 0.000993223040253966
Epoch 44/100: Training Loss: 0.0004047228583505937
Epoch 45/100: Training Loss: 0.0010549364598480971
Epoch 46/100: Training Loss: 0.0010285694282371682
Epoch 47/100: Training Loss: 0.0010261638806416439
Epoch 48/100: Training Loss: 0.0009733917829873679
Epoch 49/100: Training Loss: 0.0010341064079658134
Epoch 50/100: Training Loss: 0.001075204435761992
Epoch 51/100: Training Loss: 0.0006897271304697423
Epoch 52/100: Training Loss: 0.0010018316390631083
Epoch 53/100: Training Loss: 0.0008999454183178348
Epoch 54/100: Training Loss: 0.0010014428542210506
Epoch 55/100: Training Loss: 0.0010910527064250065
Epoch 56/100: Training Loss: 0.0008599301213984723
Epoch 57/100: Training Loss: 0.0012964446853090832
Epoch 58/100: Training Loss: 0.001096760476385797
Epoch 59/100: Training Loss: 0.0009722705487604741
Epoch 60/100: Training Loss: 0.0009419783637240216
Epoch 61/100: Training Loss: 0.0008597902797318838
Epoch 62/100: Training Loss: 0.0009767575697465377
Epoch 63/100: Training Loss: 0.0009161199514682477
Epoch 64/100: Training Loss: 0.0003275391938803079
Epoch 65/100: Training Loss: 0.0005413240694499516
Epoch 66/100: Training Loss: 0.0003895672311732819
Epoch 67/100: Training Loss: 0.0005184745976141283
Epoch 68/100: Training Loss: 0.0005115566032749789
Epoch 69/100: Training Loss: 0.000979274198725507
Epoch 70/100: Training Loss: 0.000703879221752807
Epoch 71/100: Training Loss: 0.0007044868348361729
Epoch 72/100: Training Loss: 0.00027542905165598943
Epoch 73/100: Training Loss: 0.0007076423380758379
Epoch 74/100: Training Loss: 0.0007404381266007057
Epoch 75/100: Training Loss: 0.0004388599858417378
Epoch 76/100: Training Loss: 0.0005619846336491458
Epoch 77/100: Training Loss: 0.0011231477235580658
Epoch 78/100: Training Loss: 0.0005754146959398176
Epoch 79/100: Training Loss: 0.0004005397741611187
Epoch 80/100: Training Loss: 0.0004288241579816058
Epoch 81/100: Training Loss: 0.0005130207726171801
Epoch 82/100: Training Loss: 0.0008280111567957418
Epoch 83/100: Training Loss: 0.0002872886044995768
Epoch 84/100: Training Loss: 0.0002446869661757996
Epoch 85/100: Training Loss: 0.000509398800509793
Epoch 86/100: Training Loss: 0.00013563580081596242
Epoch 87/100: Training Loss: 0.0004979723712781092
Epoch 88/100: Training Loss: 0.0005562178842671268
Epoch 89/100: Training Loss: 0.00037621315959450246
Epoch 90/100: Training Loss: 0.0005164756537317396
Epoch 91/100: Training Loss: 0.000424724489033639
Epoch 92/100: Training Loss: 0.00039863896432456436
Epoch 93/100: Training Loss: 0.0005012075384180029
Epoch 94/100: Training Loss: 0.0005198264872277533
Epoch 95/100: Training Loss: 0.00016678404714260902
Epoch 96/100: Training Loss: 0.00047807145368802795
Epoch 97/100: Training Loss: 0.0005980868126962568
Epoch 98/100: Training Loss: 0.00029765824963162826
Epoch 99/100: Training Loss: 0.0006612515845498838
Epoch 0/100: Training Loss: 0.0015424162697938323
Epoch 1/100: Training Loss: 0.0009664639922007461
Epoch 2/100: Training Loss: 0.0012006941565706685
Epoch 3/100: Training Loss: 0.0013645039014289716
Epoch 4/100: Training Loss: 0.0012364332661306931
Epoch 5/100: Training Loss: 0.000895587304618461
Epoch 6/100: Training Loss: 0.0006359056826749463
Epoch 7/100: Training Loss: 0.0004706965542278407
Epoch 8/100: Training Loss: 0.0012818795223177577
Epoch 9/100: Training Loss: 0.0008933347609876855
Epoch 10/100: Training Loss: 0.0009715297470794865
Epoch 11/100: Training Loss: 0.0007428504937996894
Epoch 12/100: Training Loss: 0.00074039120798462
Epoch 13/100: Training Loss: 0.00021907970583512007
Epoch 14/100: Training Loss: 0.00034474284370984037
Epoch 15/100: Training Loss: 0.0003203527510531841
Epoch 16/100: Training Loss: 0.00030975436871768505
Epoch 17/100: Training Loss: 0.00026013807285051407
Epoch 18/100: Training Loss: 6.285165236588637e-05
Epoch 19/100: Training Loss: 0.00011429002085712059
Epoch 20/100: Training Loss: 2.8637977762639158e-05
Epoch 21/100: Training Loss: 8.083969651738559e-05
Epoch 22/100: Training Loss: 3.134866353650035e-05
Epoch 23/100: Training Loss: 0.00015471419719464939
Epoch 24/100: Training Loss: 0.00011524456364611175
Epoch 25/100: Training Loss: 1.4731804754562173e-05
Epoch 26/100: Training Loss: 0.00019877499963608257
Epoch 27/100: Training Loss: 0.00011852211061796528
Epoch 28/100: Training Loss: 0.00044157630278288953
Epoch 29/100: Training Loss: 0.0005546756591533591
Epoch 30/100: Training Loss: 4.738977213213049e-05
Epoch 31/100: Training Loss: 0.00022033375739319923
Epoch 32/100: Training Loss: 0.00012134184683758789
Epoch 33/100: Training Loss: 3.735718825843436e-05
Epoch 34/100: Training Loss: 0.0001761854449115648
Epoch 35/100: Training Loss: 9.231172941990791e-06
Epoch 36/100: Training Loss: 2.486478434475653e-05
Epoch 37/100: Training Loss: 5.045804649340046e-06
Epoch 38/100: Training Loss: 6.396172477498056e-07
Epoch 39/100: Training Loss: 4.288541335909644e-05
Epoch 40/100: Training Loss: 2.3985775816111476e-05
Epoch 41/100: Training Loss: 5.5899803578122265e-05
Epoch 42/100: Training Loss: 8.792864970085811e-06
Epoch 43/100: Training Loss: 0.00046832192163525916
Epoch 44/100: Training Loss: 6.623965533781636e-05
Epoch 45/100: Training Loss: 3.722414340069689e-05
Epoch 46/100: Training Loss: 5.391519516706467e-05
Epoch 47/100: Training Loss: 0.00011549411238702528
Epoch 48/100: Training Loss: 0.0004087719449236349
Epoch 49/100: Training Loss: 1.450745028906439e-05
Epoch 50/100: Training Loss: 1.80275653677484e-05
Epoch 51/100: Training Loss: 1.974978078545237e-05
Epoch 52/100: Training Loss: 6.402096499694636e-05
Epoch 53/100: Training Loss: 0.00036888338762558313
Epoch 54/100: Training Loss: 3.4550239154897586e-05
Epoch 55/100: Training Loss: 1.128485920536189e-05
Epoch 56/100: Training Loss: 4.052967004523687e-05
Epoch 57/100: Training Loss: 5.329455491126818e-07
Epoch 58/100: Training Loss: 1.9138515980462858e-07
Epoch 59/100: Training Loss: 0.00013176507196543407
Epoch 60/100: Training Loss: 3.265892941093335e-06
Epoch 61/100: Training Loss: 1.0715503291301201e-05
Epoch 62/100: Training Loss: 5.3616758418823675e-06
Epoch 63/100: Training Loss: 2.9375913775771674e-06
Epoch 64/100: Training Loss: 1.630070904966878e-05
Epoch 65/100: Training Loss: 6.567220420696618e-06
Epoch 66/100: Training Loss: 3.576150486828542e-06
Epoch 67/100: Training Loss: 8.982140570878983e-05
Epoch 68/100: Training Loss: 2.867232063968032e-06
Epoch 69/100: Training Loss: 9.165407569816141e-06
Epoch 70/100: Training Loss: 8.312655624253618e-05
Epoch 71/100: Training Loss: 4.231996914269002e-06
Epoch 72/100: Training Loss: 2.0824799862650275e-05
Epoch 73/100: Training Loss: 2.67747312532795e-06
Epoch 74/100: Training Loss: 3.2606806771749734e-06
Epoch 75/100: Training Loss: 3.2551538062059075e-06
Epoch 76/100: Training Loss: 1.1621723008667765e-05
Epoch 77/100: Training Loss: 9.591269101147637e-06
Epoch 78/100: Training Loss: 1.6927066333363392e-05
Epoch 79/100: Training Loss: 2.6229514734693832e-05
Epoch 80/100: Training Loss: 0.00012077559128495082
Epoch 81/100: Training Loss: 1.3281708366117595e-05
Epoch 82/100: Training Loss: 4.961405612208361e-05
Epoch 83/100: Training Loss: 7.77226612771764e-06
Epoch 84/100: Training Loss: 2.4312835735228896e-05
Epoch 85/100: Training Loss: 1.4089233601934339e-05
Epoch 86/100: Training Loss: 1.259729197603062e-05
Epoch 87/100: Training Loss: 3.945278689448088e-05
Epoch 88/100: Training Loss: 7.604474688885859e-07
Epoch 89/100: Training Loss: 1.1429667902790422e-06
Epoch 90/100: Training Loss: 3.984921463290972e-06
Epoch 91/100: Training Loss: 4.9244938132587386e-05
Epoch 92/100: Training Loss: 1.1084176331438536e-05
Epoch 93/100: Training Loss: 6.274559443667988e-06
Epoch 94/100: Training Loss: 0.0003139805391522273
Epoch 95/100: Training Loss: 5.866954548775784e-05
Epoch 96/100: Training Loss: 2.296787166705161e-06
Epoch 97/100: Training Loss: 5.27496499643644e-06
Epoch 98/100: Training Loss: 6.494271151858605e-05
Epoch 99/100: Training Loss: 7.212478410384041e-06
Epoch 0/100: Training Loss: 0.0017018020153045654
Epoch 1/100: Training Loss: 0.0012265835985815598
Epoch 2/100: Training Loss: 0.0009475537787185857
Epoch 3/100: Training Loss: 0.0013954126578898517
Epoch 4/100: Training Loss: 0.000855734651805433
Epoch 5/100: Training Loss: 0.0008011499614071992
Epoch 6/100: Training Loss: 0.0005514780802229431
Epoch 7/100: Training Loss: 0.0004715294194367766
Epoch 8/100: Training Loss: 0.001504201373439625
Epoch 9/100: Training Loss: 0.0013362988372521898
Epoch 10/100: Training Loss: 0.0012403448666531616
Epoch 11/100: Training Loss: 0.0009498405127437568
Epoch 12/100: Training Loss: 0.000664524276929399
Epoch 13/100: Training Loss: 0.0006357014544902404
Epoch 14/100: Training Loss: 0.00041930423192451335
Epoch 15/100: Training Loss: 0.0001983289475455606
Epoch 16/100: Training Loss: 0.00019554983527382457
Epoch 17/100: Training Loss: 0.0003106719557127338
Epoch 18/100: Training Loss: 0.00028293294135046883
Epoch 19/100: Training Loss: 0.000177768464742994
Epoch 20/100: Training Loss: 0.0001734966766432019
Epoch 21/100: Training Loss: 0.00016710874081754976
Epoch 22/100: Training Loss: 0.00018060061097876427
Epoch 23/100: Training Loss: 2.2698886566823978e-05
Epoch 24/100: Training Loss: 0.00016072566158201065
Epoch 25/100: Training Loss: 0.00025750532106387833
Epoch 26/100: Training Loss: 0.0005016768911133515
Epoch 27/100: Training Loss: 6.147666742876629e-06
Epoch 28/100: Training Loss: 6.089949740405463e-06
Epoch 29/100: Training Loss: 1.0001892366795086e-05
Epoch 30/100: Training Loss: 4.122820768528189e-06
Epoch 31/100: Training Loss: 6.385272557338689e-06
Epoch 32/100: Training Loss: 0.00015032145143286583
Epoch 33/100: Training Loss: 8.083447996824065e-05
Epoch 34/100: Training Loss: 0.0001819537941115034
Epoch 35/100: Training Loss: 0.0009918486230943832
Epoch 36/100: Training Loss: 2.4155793608697647e-05
Epoch 37/100: Training Loss: 3.580882733950586e-05
Epoch 38/100: Training Loss: 0.00010693820616218941
Epoch 39/100: Training Loss: 1.729968106774099e-05
Epoch 40/100: Training Loss: 1.5822297427803278e-06
Epoch 41/100: Training Loss: 0.00016261476086326903
Epoch 42/100: Training Loss: 8.503221363926227e-05
Epoch 43/100: Training Loss: 7.349220704447272e-05
Epoch 44/100: Training Loss: 3.983905150389379e-05
Epoch 45/100: Training Loss: 1.4801068300027058e-05
Epoch 46/100: Training Loss: 1.5330546609828808e-05
Epoch 47/100: Training Loss: 7.104908510402668e-06
Epoch 48/100: Training Loss: 2.238182265288625e-05
Epoch 49/100: Training Loss: 1.5825770246836305e-05
Epoch 50/100: Training Loss: 7.707199961678382e-05
Epoch 51/100: Training Loss: 1.0311740673392829e-05
Epoch 52/100: Training Loss: 3.0327639742497285e-05
Epoch 53/100: Training Loss: 9.562535228584807e-06
Epoch 54/100: Training Loss: 8.955105790651284e-06
Epoch 55/100: Training Loss: 0.00033037769008267874
Epoch 56/100: Training Loss: 2.7088894068829122e-05
Epoch 57/100: Training Loss: 8.484834313758312e-06
Epoch 58/100: Training Loss: 2.6984637720293247e-06
Epoch 59/100: Training Loss: 6.132678359466965e-07
Epoch 60/100: Training Loss: 1.9146992247894498e-05
Epoch 61/100: Training Loss: 0.00020135982544875584
Epoch 62/100: Training Loss: 3.094597793886998e-05
Epoch 63/100: Training Loss: 2.4592061098550726e-05
Epoch 64/100: Training Loss: 0.00010089828787040126
Epoch 65/100: Training Loss: 2.419125243015816e-05
Epoch 66/100: Training Loss: 0.00013346127891467394
Epoch 67/100: Training Loss: 5.1276684035918465e-05
Epoch 68/100: Training Loss: 1.7673768848752135e-06
Epoch 69/100: Training Loss: 0.00018878558799167351
Epoch 70/100: Training Loss: 1.607738732377444e-05
Epoch 71/100: Training Loss: 2.207892462668335e-06
Epoch 72/100: Training Loss: 5.488864580409293e-07
Epoch 73/100: Training Loss: 6.095225925620166e-07
Epoch 74/100: Training Loss: 8.644676121648835e-06
Epoch 75/100: Training Loss: 1.1088293809243736e-07
Epoch 76/100: Training Loss: 2.440200308680626e-06
Epoch 77/100: Training Loss: 0.002222024772796163
Epoch 78/100: Training Loss: 1.3816863172716157e-05
Epoch 79/100: Training Loss: 1.5373590282318782e-05
Epoch 80/100: Training Loss: 1.2982684229372035e-07
Epoch 81/100: Training Loss: 1.385666864232783e-05
Epoch 82/100: Training Loss: 1.198051927112717e-07
Epoch 83/100: Training Loss: 1.9979238598693186e-06
Epoch 84/100: Training Loss: 4.532821605016301e-06
Epoch 85/100: Training Loss: 0.0015593538620720612
Epoch 86/100: Training Loss: 5.654301037269136e-05
Epoch 87/100: Training Loss: 3.091682982773868e-05
Epoch 88/100: Training Loss: 1.4637079642674218e-06
Epoch 89/100: Training Loss: 9.219289115955768e-07
Epoch 90/100: Training Loss: 1.435532903104472e-05
Epoch 91/100: Training Loss: 2.6720010695487625e-06
Epoch 92/100: Training Loss: 5.688995374309505e-06
Epoch 93/100: Training Loss: 2.949569478596738e-06
Epoch 94/100: Training Loss: 5.208134674038624e-05
Epoch 95/100: Training Loss: 2.869459846131275e-06
Epoch 96/100: Training Loss: 6.887237569945722e-07
Epoch 97/100: Training Loss: 8.680892356905652e-07
Epoch 98/100: Training Loss: 1.1893726336809755e-05
Epoch 99/100: Training Loss: 7.0471186590798065e-06
Epoch 0/100: Training Loss: 0.001651402814256633
Epoch 1/100: Training Loss: 0.0012765279028313292
Epoch 2/100: Training Loss: 0.0005816908701797204
Epoch 3/100: Training Loss: 0.0010689551844918655
Epoch 4/100: Training Loss: 0.001216297866376631
Epoch 5/100: Training Loss: 0.0010842572143472777
Epoch 6/100: Training Loss: 0.0004490738266084823
Epoch 7/100: Training Loss: 0.0005667230194331678
Epoch 8/100: Training Loss: 0.0009231357121028783
Epoch 9/100: Training Loss: 0.0020181940742796914
Epoch 10/100: Training Loss: 0.001375842953752155
Epoch 11/100: Training Loss: 0.0010841531804734212
Epoch 12/100: Training Loss: 0.00047447148459089315
Epoch 13/100: Training Loss: 0.000460779054398917
Epoch 14/100: Training Loss: 0.0004190884850507865
Epoch 15/100: Training Loss: 0.00041082515124163014
Epoch 16/100: Training Loss: 0.00021222716003107877
Epoch 17/100: Training Loss: 8.831726261443156e-05
Epoch 18/100: Training Loss: 9.440064453091358e-05
Epoch 19/100: Training Loss: 0.0003018178777094999
Epoch 20/100: Training Loss: 6.036292990109672e-05
Epoch 21/100: Training Loss: 0.0001313179457114518
Epoch 22/100: Training Loss: 2.8281101237045476e-05
Epoch 23/100: Training Loss: 2.2103740016649838e-05
Epoch 24/100: Training Loss: 1.6088397499242444e-05
Epoch 25/100: Training Loss: 3.738641064682621e-05
Epoch 26/100: Training Loss: 6.319942421342698e-05
Epoch 27/100: Training Loss: 0.0001635879301037525
Epoch 28/100: Training Loss: 0.0008887819534430474
Epoch 29/100: Training Loss: 0.0006219411920184738
Epoch 30/100: Training Loss: 0.00032657848819633205
Epoch 31/100: Training Loss: 0.0002664346590729579
Epoch 32/100: Training Loss: 0.00022188356957552623
Epoch 33/100: Training Loss: 0.0002877267957465049
Epoch 34/100: Training Loss: 0.00017664695809955247
Epoch 35/100: Training Loss: 5.1092967088191425e-06
Epoch 36/100: Training Loss: 0.0007175624461993118
Epoch 37/100: Training Loss: 0.00010302218328590042
Epoch 38/100: Training Loss: 0.00013111987682573636
Epoch 39/100: Training Loss: 0.0002587046588491077
Epoch 40/100: Training Loss: 3.7746158274222004e-05
Epoch 41/100: Training Loss: 8.184323944379947e-05
Epoch 42/100: Training Loss: 1.7304160915018041e-06
Epoch 43/100: Training Loss: 7.527908403140727e-07
Epoch 44/100: Training Loss: 7.613462773065991e-06
Epoch 45/100: Training Loss: 1.6230942704286312e-05
Epoch 46/100: Training Loss: 7.328063826665191e-06
Epoch 47/100: Training Loss: 2.9595508782958693e-05
Epoch 48/100: Training Loss: 2.974625448324929e-05
Epoch 49/100: Training Loss: 8.77956819771989e-05
Epoch 50/100: Training Loss: 0.0005931546625915481
Epoch 51/100: Training Loss: 0.0002590162805253011
Epoch 52/100: Training Loss: 0.00018567449064708195
Epoch 53/100: Training Loss: 2.410765444471335e-06
Epoch 54/100: Training Loss: 1.1860389416705977e-05
Epoch 55/100: Training Loss: 0.0002586062471925115
Epoch 56/100: Training Loss: 1.030987731403917e-05
Epoch 57/100: Training Loss: 2.038980972741736e-06
Epoch 58/100: Training Loss: 7.736347358605247e-06
Epoch 59/100: Training Loss: 2.694967277119496e-05
Epoch 60/100: Training Loss: 9.918028563389589e-08
Epoch 61/100: Training Loss: 9.120890493590408e-05
Epoch 62/100: Training Loss: 0.00012833324540977828
Epoch 63/100: Training Loss: 2.7384113683222444e-06
Epoch 64/100: Training Loss: 6.685157712160444e-06
Epoch 65/100: Training Loss: 2.3193247444881984e-05
Epoch 66/100: Training Loss: 1.6206719746893169e-06
Epoch 67/100: Training Loss: 6.99700007752224e-06
Epoch 68/100: Training Loss: 6.827342958172406e-06
Epoch 69/100: Training Loss: 2.7415985055839175e-06
Epoch 70/100: Training Loss: 2.143132654618632e-05
Epoch 71/100: Training Loss: 4.197378354318486e-06
Epoch 72/100: Training Loss: 2.631349267689454e-06
Epoch 73/100: Training Loss: 7.615689233951042e-05
Epoch 74/100: Training Loss: 8.69067517206347e-07
Epoch 75/100: Training Loss: 5.9555611070359776e-06
Epoch 76/100: Training Loss: 1.835105605023496e-06
Epoch 77/100: Training Loss: 1.8452710552038225e-06
Epoch 78/100: Training Loss: 5.067211855774277e-06
Epoch 79/100: Training Loss: 2.4240963915191546e-05
Epoch 80/100: Training Loss: 3.5532552354677687e-06
Epoch 81/100: Training Loss: 6.623335462634915e-06
Epoch 82/100: Training Loss: 0.00014592579171701444
Epoch 83/100: Training Loss: 0.00012965646623833778
Epoch 84/100: Training Loss: 2.648932802165213e-06
Epoch 85/100: Training Loss: 5.709955935149334e-07
Epoch 86/100: Training Loss: 1.708192299434378e-05
Epoch 87/100: Training Loss: 6.90610327992154e-06
Epoch 88/100: Training Loss: 4.860747139900923e-06
Epoch 89/100: Training Loss: 6.632562269645234e-05
Epoch 90/100: Training Loss: 6.265277642916682e-06
Epoch 91/100: Training Loss: 1.215406242140963e-05
Epoch 92/100: Training Loss: 3.558628909586946e-06
Epoch 93/100: Training Loss: 1.3632697526056981e-06
Epoch 94/100: Training Loss: 3.0111692891530464e-05
Epoch 95/100: Training Loss: 1.6002207308824807e-05
Epoch 96/100: Training Loss: 3.252291274856936e-05
Epoch 97/100: Training Loss: 4.5245631699353184e-07
Epoch 98/100: Training Loss: 7.308177518561208e-06
Epoch 99/100: Training Loss: 4.082425723160886e-06
Epoch 0/100: Training Loss: 0.0014533279463648796
Epoch 1/100: Training Loss: 0.0011612153612077236
Epoch 2/100: Training Loss: 0.0007306856103241443
Epoch 3/100: Training Loss: 0.0006588336545974016
Epoch 4/100: Training Loss: 0.0008754764683544635
Epoch 5/100: Training Loss: 0.0008301341906189919
Epoch 6/100: Training Loss: 0.0007834752090275287
Epoch 7/100: Training Loss: 0.0009579883888363838
Epoch 8/100: Training Loss: 0.0011455695144832133
Epoch 9/100: Training Loss: 0.0009498512372374535
Epoch 10/100: Training Loss: 0.00013690333580598235
Epoch 11/100: Training Loss: 0.00041512553580105307
Epoch 12/100: Training Loss: 0.0007068253587931394
Epoch 13/100: Training Loss: 0.00065181078389287
Epoch 14/100: Training Loss: 0.0004683727864176035
Epoch 15/100: Training Loss: 0.0006347521208226681
Epoch 16/100: Training Loss: 0.0007916073314845562
Epoch 17/100: Training Loss: 0.0007403436116874218
Epoch 18/100: Training Loss: 0.0008060666732490063
Epoch 19/100: Training Loss: 0.0007802306208759546
Epoch 20/100: Training Loss: 0.0003848909866064787
Epoch 21/100: Training Loss: 0.0006656557321548462
Epoch 22/100: Training Loss: 0.0006351478863507509
Epoch 23/100: Training Loss: 0.0006602576933801175
Epoch 24/100: Training Loss: 0.0002270472701638937
Epoch 25/100: Training Loss: 0.00034447875805199146
Epoch 26/100: Training Loss: 0.000607806071639061
Epoch 27/100: Training Loss: 0.0006704259198158979
Epoch 28/100: Training Loss: 0.0006792247761040926
Epoch 29/100: Training Loss: 0.0008656521327793598
Epoch 30/100: Training Loss: 0.00021507563069462777
Epoch 31/100: Training Loss: 0.00047425893135368826
Epoch 32/100: Training Loss: 0.0001666859374381602
Epoch 33/100: Training Loss: 0.0001337727764621377
Epoch 34/100: Training Loss: 0.0004763448145240545
Epoch 35/100: Training Loss: 0.0003450159216299653
Epoch 36/100: Training Loss: 0.0005374227650463582
Epoch 37/100: Training Loss: 0.0003770842682570219
Epoch 38/100: Training Loss: 0.0004657469689846039
Epoch 39/100: Training Loss: 0.0005455639213323594
Epoch 40/100: Training Loss: 0.0006301738787442446
Epoch 41/100: Training Loss: 0.0005020235199481249
Epoch 42/100: Training Loss: 0.00017416973132640124
Epoch 43/100: Training Loss: 0.0004222051706165075
Epoch 44/100: Training Loss: 0.00012625494273379445
Epoch 45/100: Training Loss: 0.0005252427887171507
Epoch 46/100: Training Loss: 0.0004849760327488184
Epoch 47/100: Training Loss: 0.00032962362747639417
Epoch 48/100: Training Loss: 0.00033578176517039536
Epoch 49/100: Training Loss: 0.00031476926524192094
Epoch 50/100: Training Loss: 0.0005286224186420441
Epoch 51/100: Training Loss: 0.0001939431647770107
Epoch 52/100: Training Loss: 3.448766947258264e-05
Epoch 53/100: Training Loss: 0.00026578374672681093
Epoch 54/100: Training Loss: 2.143014135072008e-05
Epoch 55/100: Training Loss: 0.000250695482827723
Epoch 56/100: Training Loss: 0.001655430719256401
Epoch 57/100: Training Loss: 9.868983179330825e-05
Epoch 58/100: Training Loss: 0.000267759314738214
Epoch 59/100: Training Loss: 0.00010792886605486274
Epoch 60/100: Training Loss: 0.0003265558974817395
Epoch 61/100: Training Loss: 0.0002678560558706522
Epoch 62/100: Training Loss: 0.00014368966221809388
Epoch 63/100: Training Loss: 6.3095554651226845e-06
Epoch 64/100: Training Loss: 1.663961484155152e-06
Epoch 65/100: Training Loss: 0.00023511839099228383
Epoch 66/100: Training Loss: 2.8753449441865087e-05
Epoch 67/100: Training Loss: 5.669647362083197e-06
Epoch 68/100: Training Loss: 5.85469497309532e-06
Epoch 69/100: Training Loss: 7.998021901585161e-05
Epoch 70/100: Training Loss: 0.0018678173422813416
Epoch 71/100: Training Loss: 3.2886615372262895e-05
Epoch 72/100: Training Loss: 0.00017043041298165918
Epoch 73/100: Training Loss: 0.0001699772896245122
Epoch 74/100: Training Loss: 7.950491271913052e-05
Epoch 75/100: Training Loss: 9.331422043032945e-05
Epoch 76/100: Training Loss: 0.00011860954109579324
Epoch 77/100: Training Loss: 1.6386681818403303e-05
Epoch 78/100: Training Loss: 2.8160098008811473e-05
Epoch 79/100: Training Loss: 9.727763426781165e-07
Epoch 80/100: Training Loss: 3.277587893535383e-06
Epoch 81/100: Training Loss: 1.504940155427903e-05
Epoch 82/100: Training Loss: 3.7841484299860895e-05
Epoch 83/100: Training Loss: 2.0756454614456742e-05
Epoch 84/100: Training Loss: 2.310879935976118e-05
Epoch 85/100: Training Loss: 1.8751222523860633e-05
Epoch 86/100: Training Loss: 3.012048255186528e-05
Epoch 87/100: Training Loss: 7.924424426164479e-06
Epoch 88/100: Training Loss: 0.00014025664422661065
Epoch 89/100: Training Loss: 1.3341901649255306e-05
Epoch 90/100: Training Loss: 1.4344718692882452e-06
Epoch 91/100: Training Loss: 0.0005385075230151415
Epoch 92/100: Training Loss: 0.000488451449200511
Epoch 93/100: Training Loss: 1.621273986529559e-05
Epoch 94/100: Training Loss: 4.391387847135775e-06
Epoch 95/100: Training Loss: 1.6429137031082063e-05
Epoch 96/100: Training Loss: 6.8228553573135285e-06
Epoch 97/100: Training Loss: 5.345236422726884e-06
Epoch 98/100: Training Loss: 4.327264832681976e-06
Epoch 99/100: Training Loss: 7.386935758404433e-05
Epoch 0/100: Training Loss: 0.0018620720133185388
Epoch 1/100: Training Loss: 0.001245585735887289
Epoch 2/100: Training Loss: 0.0004185158293694258
Epoch 3/100: Training Loss: 0.0007064984180033207
Epoch 4/100: Training Loss: 0.0008948137983679771
Epoch 5/100: Training Loss: 0.001196217816323042
Epoch 6/100: Training Loss: 0.0010685795918107034
Epoch 7/100: Training Loss: 0.0008872824721038342
Epoch 8/100: Training Loss: 0.0006181014236062765
Epoch 9/100: Training Loss: 0.0007312729954719544
Epoch 10/100: Training Loss: 0.0007030743174254895
Epoch 11/100: Training Loss: 0.000753121729940176
Epoch 12/100: Training Loss: 0.0008849399164319038
Epoch 13/100: Training Loss: 0.0006983240600675344
Epoch 14/100: Training Loss: 0.00030999991577118635
Epoch 15/100: Training Loss: 0.0005857572425156832
Epoch 16/100: Training Loss: 0.0006243095267564058
Epoch 17/100: Training Loss: 0.0005442106164991855
Epoch 18/100: Training Loss: 0.0005350079387426377
Epoch 19/100: Training Loss: 0.0007953547872602939
Epoch 20/100: Training Loss: 0.0005913495551794767
Epoch 21/100: Training Loss: 0.0006700470112264156
Epoch 22/100: Training Loss: 0.0006324304733425379
Epoch 23/100: Training Loss: 0.000346425105817616
Epoch 24/100: Training Loss: 0.0007343064993619919
Epoch 25/100: Training Loss: 0.000670651625841856
Epoch 26/100: Training Loss: 0.0005307124461978674
Epoch 27/100: Training Loss: 0.000739580811932683
Epoch 28/100: Training Loss: 0.0004115148913115263
Epoch 29/100: Training Loss: 0.0009478600695729256
Epoch 30/100: Training Loss: 0.0007083216216415167
Epoch 31/100: Training Loss: 0.0006614969111979008
Epoch 32/100: Training Loss: 0.0007185113616287708
Epoch 33/100: Training Loss: 0.0007647121790796518
Epoch 34/100: Training Loss: 0.0005965727847069502
Epoch 35/100: Training Loss: 0.0006460644770413637
Epoch 36/100: Training Loss: 0.0005281859543174505
Epoch 37/100: Training Loss: 0.0006373699754476548
Epoch 38/100: Training Loss: 0.0008082780055701733
Epoch 39/100: Training Loss: 0.0007850641384720802
Epoch 40/100: Training Loss: 0.0004026294220238924
Epoch 41/100: Training Loss: 0.0004026952665299177
Epoch 42/100: Training Loss: 0.00039016345981508495
Epoch 43/100: Training Loss: 0.00028271707706153395
Epoch 44/100: Training Loss: 0.00012238805647939443
Epoch 45/100: Training Loss: 0.0003603795310482383
Epoch 46/100: Training Loss: 0.0001929346239194274
Epoch 47/100: Training Loss: 0.00043805204331874846
Epoch 48/100: Training Loss: 0.0003129785414785147
Epoch 49/100: Training Loss: 0.0001613420550711453
Epoch 50/100: Training Loss: 0.00018308194121345877
Epoch 51/100: Training Loss: 0.0014909417368471622
Epoch 52/100: Training Loss: 0.00034447251819074153
Epoch 53/100: Training Loss: 0.0007502235472202301
Epoch 54/100: Training Loss: 0.00017662303289398552
Epoch 55/100: Training Loss: 0.00035781424958258867
Epoch 56/100: Training Loss: 0.0002092176815494895
Epoch 57/100: Training Loss: 0.00011654923437163234
Epoch 58/100: Training Loss: 0.00021427280735224484
Epoch 59/100: Training Loss: 0.00017085878644138575
Epoch 60/100: Training Loss: 0.00033645720686763524
Epoch 61/100: Training Loss: 7.96604377683252e-05
Epoch 62/100: Training Loss: 6.260942609515041e-06
Epoch 63/100: Training Loss: 9.047691710293294e-05
Epoch 64/100: Training Loss: 5.774383316747844e-05
Epoch 65/100: Training Loss: 7.779303705319762e-05
Epoch 66/100: Training Loss: 9.510567761026323e-05
Epoch 67/100: Training Loss: 8.327718824148178e-05
Epoch 68/100: Training Loss: 0.00010142303071916103
Epoch 69/100: Training Loss: 0.0002073777373880148
Epoch 70/100: Training Loss: 8.871849277056754e-05
Epoch 71/100: Training Loss: 3.2084755366668106e-05
Epoch 72/100: Training Loss: 0.0002795901382341981
Epoch 73/100: Training Loss: 2.3158465046435596e-05
Epoch 74/100: Training Loss: 8.015428466023878e-06
Epoch 75/100: Training Loss: 3.528757079038769e-05
Epoch 76/100: Training Loss: 0.0003635365515947342
Epoch 77/100: Training Loss: 2.651094982866198e-05
Epoch 78/100: Training Loss: 0.0018232667818665505
Epoch 79/100: Training Loss: 3.151048440486193e-05
Epoch 80/100: Training Loss: 4.5225376379676166e-05
Epoch 81/100: Training Loss: 3.5393625148572026e-05
Epoch 82/100: Training Loss: 2.3605520254932344e-05
Epoch 83/100: Training Loss: 8.669833769090474e-05
Epoch 84/100: Training Loss: 0.00011651847744360566
Epoch 85/100: Training Loss: 0.00027525534387677907
Epoch 86/100: Training Loss: 0.00027543429750949145
Epoch 87/100: Training Loss: 1.9303835870232433e-05
Epoch 88/100: Training Loss: 3.0229899493861013e-06
Epoch 89/100: Training Loss: 0.0017718130722641945
Epoch 90/100: Training Loss: 7.535691111115739e-06
Epoch 91/100: Training Loss: 3.3675352460704745e-05
Epoch 92/100: Training Loss: 1.91097948118113e-05
Epoch 93/100: Training Loss: 6.783691060263664e-06
Epoch 94/100: Training Loss: 9.79616233962588e-06
Epoch 95/100: Training Loss: 8.572546648792923e-05
Epoch 96/100: Training Loss: 6.367188761942088e-05
Epoch 97/100: Training Loss: 7.662720599910245e-06
Epoch 98/100: Training Loss: 2.1774627384729683e-05
Epoch 99/100: Training Loss: 2.3479628725908697e-05
Epoch 0/100: Training Loss: 0.0007726286072283983
Epoch 1/100: Training Loss: 0.0006067027803510428
Epoch 2/100: Training Loss: 0.0016409561038017273
Epoch 3/100: Training Loss: 0.0009871812537312508
Epoch 4/100: Training Loss: 0.0005255773197859525
Epoch 5/100: Training Loss: 0.0011225697584450246
Epoch 6/100: Training Loss: 0.0007256746757775545
Epoch 7/100: Training Loss: 0.0006897367537021637
Epoch 8/100: Training Loss: 0.00084812231361866
Epoch 9/100: Training Loss: 0.000587664870545268
Epoch 10/100: Training Loss: 0.0009132429957389831
Epoch 11/100: Training Loss: 0.0005980947520583868
Epoch 12/100: Training Loss: 0.0005172557663172484
Epoch 13/100: Training Loss: 0.0009763918817043305
Epoch 14/100: Training Loss: 0.0005914955399930477
Epoch 15/100: Training Loss: 0.0008466800674796105
Epoch 16/100: Training Loss: 0.0007724716328084469
Epoch 17/100: Training Loss: 0.0008683043532073497
Epoch 18/100: Training Loss: 0.0006898996420204639
Epoch 19/100: Training Loss: 0.0008669259026646614
Epoch 20/100: Training Loss: 0.000780985178425908
Epoch 21/100: Training Loss: 0.0006782259326428175
Epoch 22/100: Training Loss: 0.0007601398974657059
Epoch 23/100: Training Loss: 0.0005865870509296656
Epoch 24/100: Training Loss: 0.000517522031441331
Epoch 25/100: Training Loss: 0.00028920588083565236
Epoch 26/100: Training Loss: 0.00033968903589993716
Epoch 27/100: Training Loss: 0.000359801365993917
Epoch 28/100: Training Loss: 0.0005554244387894868
Epoch 29/100: Training Loss: 0.0005694699008017779
Epoch 30/100: Training Loss: 0.0005524926818907261
Epoch 31/100: Training Loss: 0.0005833137314766645
Epoch 32/100: Training Loss: 0.0004263958893716335
Epoch 33/100: Training Loss: 0.00017736259615048767
Epoch 34/100: Training Loss: 0.00019664517603814601
Epoch 35/100: Training Loss: 0.00039330474101006985
Epoch 36/100: Training Loss: 0.0005121015477925539
Epoch 37/100: Training Loss: 0.0005063869524747133
Epoch 38/100: Training Loss: 0.0002534408587962389
Epoch 39/100: Training Loss: 0.00014500602846965194
Epoch 40/100: Training Loss: 0.00027534926775842907
Epoch 41/100: Training Loss: 0.00038256566040217876
Epoch 42/100: Training Loss: 0.00036879542749375106
Epoch 43/100: Training Loss: 0.00010846296790987254
Epoch 44/100: Training Loss: 0.0005350928287953139
Epoch 45/100: Training Loss: 0.0004333813209086657
Epoch 46/100: Training Loss: 0.0010791453532874584
Epoch 47/100: Training Loss: 0.00025455181021243333
Epoch 48/100: Training Loss: 9.900899603962899e-05
Epoch 49/100: Training Loss: 7.019828772172331e-05
Epoch 50/100: Training Loss: 0.0001969805685803294
Epoch 51/100: Training Loss: 0.00034769531339406967
Epoch 52/100: Training Loss: 0.00016321670264005662
Epoch 53/100: Training Loss: 0.0003299884730949998
Epoch 54/100: Training Loss: 0.0005898300092667341
Epoch 55/100: Training Loss: 0.0003179983701556921
Epoch 56/100: Training Loss: 0.00011909662280231714
Epoch 57/100: Training Loss: 0.0003887132275849581
Epoch 58/100: Training Loss: 0.0003468270879238844
Epoch 59/100: Training Loss: 0.0006717357318848372
Epoch 60/100: Training Loss: 0.00032527861185371876
Epoch 61/100: Training Loss: 0.00016260752454400063
Epoch 62/100: Training Loss: 0.0002553898608312011
Epoch 63/100: Training Loss: 0.00010173565242439508
Epoch 64/100: Training Loss: 0.0003247977001592517
Epoch 65/100: Training Loss: 0.00028338641859591005
Epoch 66/100: Training Loss: 0.00039257216267287733
Epoch 67/100: Training Loss: 2.552868099883199e-05
Epoch 68/100: Training Loss: 6.0271576512604955e-05
Epoch 69/100: Training Loss: 9.091448155231773e-05
Epoch 70/100: Training Loss: 5.972810904495418e-05
Epoch 71/100: Training Loss: 2.2243018611334266e-05
Epoch 72/100: Training Loss: 4.4904655078426e-05
Epoch 73/100: Training Loss: 0.00011004166444763541
Epoch 74/100: Training Loss: 0.00010268513578921556
Epoch 75/100: Training Loss: 9.390804916620255e-05
Epoch 76/100: Training Loss: 9.133278217632323e-06
Epoch 77/100: Training Loss: 6.239289650693536e-05
Epoch 78/100: Training Loss: 3.467079295660369e-06
Epoch 79/100: Training Loss: 9.255502664018423e-06
Epoch 80/100: Training Loss: 8.039274835027754e-05
Epoch 81/100: Training Loss: 0.0003497093915939331
Epoch 82/100: Training Loss: 3.3296859328402205e-06
Epoch 83/100: Training Loss: 6.165954255266115e-06
Epoch 84/100: Training Loss: 4.2205115278193264e-07
Epoch 85/100: Training Loss: 4.9455851694801826e-06
Epoch 86/100: Training Loss: 3.7342397263273597e-06
Epoch 87/100: Training Loss: 7.312856268981705e-07
Epoch 88/100: Training Loss: 2.54381557169836e-06
Epoch 89/100: Training Loss: 8.537876419723034e-06
Epoch 90/100: Training Loss: 1.365963998978259e-06
Epoch 91/100: Training Loss: 3.0804301331954775e-07
Epoch 92/100: Training Loss: 1.2995859833608846e-06
Epoch 93/100: Training Loss: 7.033935253275558e-06
Epoch 94/100: Training Loss: 9.31689896788157e-08
Epoch 95/100: Training Loss: 1.213381692650728e-05
Epoch 96/100: Training Loss: 4.996654752176255e-07
Epoch 97/100: Training Loss: 1.2954963494848926e-06
Epoch 98/100: Training Loss: 5.331281045073411e-07
Epoch 99/100: Training Loss: 0.00030500220600515604
Epoch 0/100: Training Loss: 0.001722089044607369
Epoch 1/100: Training Loss: 0.0016720414541329547
Epoch 2/100: Training Loss: 0.0014437093476580966
Epoch 3/100: Training Loss: 0.0013441091320317263
Epoch 4/100: Training Loss: 0.0016727065964109579
Epoch 5/100: Training Loss: 0.0011416503768058338
Epoch 6/100: Training Loss: 0.001645391344264814
Epoch 7/100: Training Loss: 0.0018910228446790368
Epoch 8/100: Training Loss: 0.0010595986038256602
Epoch 9/100: Training Loss: 0.0017464460840650425
Epoch 10/100: Training Loss: 0.0014485006879089745
Epoch 11/100: Training Loss: 0.0013459276431685041
Epoch 12/100: Training Loss: 0.0014025119079905711
Epoch 13/100: Training Loss: 0.0013999377086663701
Epoch 14/100: Training Loss: 0.0008953974884786423
Epoch 15/100: Training Loss: 0.0008447631529182385
Epoch 16/100: Training Loss: 0.0007720092299637521
Epoch 17/100: Training Loss: 0.0007926349522201879
Epoch 18/100: Training Loss: 0.0008841873544036962
Epoch 19/100: Training Loss: 0.0008021235276179709
Epoch 20/100: Training Loss: 0.0008328378580178424
Epoch 21/100: Training Loss: 0.0009427248102844141
Epoch 22/100: Training Loss: 0.0008849031796121293
Epoch 23/100: Training Loss: 0.0006954627242057946
Epoch 24/100: Training Loss: 0.0006242785484168181
Epoch 25/100: Training Loss: 0.0006663019585002
Epoch 26/100: Training Loss: 0.00048748816654181024
Epoch 27/100: Training Loss: 0.0008087923192674187
Epoch 28/100: Training Loss: 0.00044951953326061275
Epoch 29/100: Training Loss: 0.0007160767248481702
Epoch 30/100: Training Loss: 0.0006228986724167113
Epoch 31/100: Training Loss: 0.0005658153610624326
Epoch 32/100: Training Loss: 0.0003837443603451844
Epoch 33/100: Training Loss: 0.0005916192842896577
Epoch 34/100: Training Loss: 0.000718701844382438
Epoch 35/100: Training Loss: 0.0007896090673792894
Epoch 36/100: Training Loss: 0.0006790362820503818
Epoch 37/100: Training Loss: 0.0006337173900027184
Epoch 38/100: Training Loss: 0.0005985094578402816
Epoch 39/100: Training Loss: 0.0005843832519403688
Epoch 40/100: Training Loss: 0.0006766871661896918
Epoch 41/100: Training Loss: 0.0007018600679506922
Epoch 42/100: Training Loss: 0.00047045756297506346
Epoch 43/100: Training Loss: 0.0003244453554700135
Epoch 44/100: Training Loss: 0.0006457173330768659
Epoch 45/100: Training Loss: 0.0007313038133511877
Epoch 46/100: Training Loss: 0.0007510280153553957
Epoch 47/100: Training Loss: 0.0006442700222039678
Epoch 48/100: Training Loss: 0.0006466122569551893
Epoch 49/100: Training Loss: 0.0006430005761468487
Epoch 50/100: Training Loss: 0.0006240630035947083
Epoch 51/100: Training Loss: 0.0003325147139039009
Epoch 52/100: Training Loss: 0.00021364492405751709
Epoch 53/100: Training Loss: 0.0005110330452584917
Epoch 54/100: Training Loss: 0.0005999959198532591
Epoch 55/100: Training Loss: 0.0005785719414425504
Epoch 56/100: Training Loss: 0.0004537174845956693
Epoch 57/100: Training Loss: 0.000581953888106498
Epoch 58/100: Training Loss: 0.0005806119768482865
Epoch 59/100: Training Loss: 0.0005261651269949166
