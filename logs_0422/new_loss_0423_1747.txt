2024-04-23 21:47:50.716370: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-04-23 21:47:52.773109: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
================layer -5================
Epoch 0/100: Training Loss: 0.00410933511240499
Epoch 1/100: Training Loss: 0.0043553955905087345
Epoch 2/100: Training Loss: 0.0022955011654567053
Epoch 3/100: Training Loss: 0.00446519901702454
Epoch 4/100: Training Loss: 0.005272706071813623
Epoch 5/100: Training Loss: 0.004308194964082091
Epoch 6/100: Training Loss: 0.0037547879285745687
Epoch 7/100: Training Loss: 0.003690925511446866
Epoch 8/100: Training Loss: 0.003265655540919804
Epoch 9/100: Training Loss: 0.003530004641392848
Epoch 10/100: Training Loss: 0.003367511542526992
Epoch 11/100: Training Loss: 0.0017670530539292556
Epoch 12/100: Training Loss: 0.003414789696673413
Epoch 13/100: Training Loss: 0.0026389239551304106
Epoch 14/100: Training Loss: 0.0022534836422313342
Epoch 15/100: Training Loss: 0.0035660125158883476
Epoch 16/100: Training Loss: 0.003072413531216708
Epoch 17/100: Training Loss: 0.0019709057324416154
Epoch 18/100: Training Loss: 0.003626069405695775
Epoch 19/100: Training Loss: 0.002401499273060085
Epoch 20/100: Training Loss: 0.001889107319024893
Epoch 21/100: Training Loss: 0.0012994464877602104
Epoch 22/100: Training Loss: 0.0037064585652384725
Epoch 23/100: Training Loss: 0.003523337257491959
Epoch 24/100: Training Loss: 0.0017824573116702633
Epoch 25/100: Training Loss: 0.002059122482379833
Epoch 26/100: Training Loss: 0.003487808095825302
Epoch 27/100: Training Loss: 0.0039094586472411255
Epoch 28/100: Training Loss: 0.0022028174016859147
Epoch 29/100: Training Loss: 0.0019224169371011373
Epoch 30/100: Training Loss: 0.0011975642267640653
Epoch 31/100: Training Loss: 0.0018080164502550671
Epoch 32/100: Training Loss: 0.0023123638613240702
Epoch 33/100: Training Loss: 0.0013315867293964732
Epoch 34/100: Training Loss: 0.0015038326903656647
Epoch 35/100: Training Loss: 0.0023456511797604863
Epoch 36/100: Training Loss: 0.0018789826573191823
Epoch 37/100: Training Loss: 0.0016176267103715377
Epoch 38/100: Training Loss: 0.001803873093811782
Epoch 39/100: Training Loss: 0.0020808744263815715
Epoch 40/100: Training Loss: 0.0019615509293296122
Epoch 41/100: Training Loss: 0.0016934941490213355
Epoch 42/100: Training Loss: 0.00171134115515889
Epoch 43/100: Training Loss: 0.0020057488154698083
Epoch 44/100: Training Loss: 0.0015933660568890871
Epoch 45/100: Training Loss: 0.0010919966897764406
Epoch 46/100: Training Loss: 0.0018370409945507983
Epoch 47/100: Training Loss: 0.0015550684053581077
Epoch 48/100: Training Loss: 0.0015129023915404207
Epoch 49/100: Training Loss: 0.0016839295834094495
Epoch 50/100: Training Loss: 0.0017008868964401992
Epoch 51/100: Training Loss: 0.0016025779964206936
Epoch 52/100: Training Loss: 0.0015867950616182981
Epoch 53/100: Training Loss: 0.0016300443794343856
Epoch 54/100: Training Loss: 0.0015609781225244482
Epoch 55/100: Training Loss: 0.0016918581474077452
Epoch 56/100: Training Loss: 0.001062190407639617
Epoch 57/100: Training Loss: 0.0016141871472338696
Epoch 58/100: Training Loss: 0.0015513717711388647
Epoch 59/100: Training Loss: 0.0010922982559337482
Epoch 60/100: Training Loss: 0.0013914895015996653
Epoch 61/100: Training Loss: 0.001217336296201586
Epoch 62/100: Training Loss: 0.0013239617114300494
Epoch 63/100: Training Loss: 0.0007377745179863243
Epoch 64/100: Training Loss: 0.00111467844539589
Epoch 65/100: Training Loss: 0.001236242639434921
Epoch 66/100: Training Loss: 0.0012895123108283622
Epoch 67/100: Training Loss: 0.001097027967859815
Epoch 68/100: Training Loss: 0.0009618743316276924
Epoch 69/100: Training Loss: 0.00103947884969778
Epoch 70/100: Training Loss: 0.0011937823537346366
Epoch 71/100: Training Loss: 0.0012235890526871582
Epoch 72/100: Training Loss: 0.0006630066391471383
Epoch 73/100: Training Loss: 0.0013874608736771804
Epoch 74/100: Training Loss: 0.0010641182844455426
Epoch 75/100: Training Loss: 0.0004949671821994382
Epoch 76/100: Training Loss: 0.0008404701948165894
Epoch 77/100: Training Loss: 0.0006524635465828689
Epoch 78/100: Training Loss: 0.0010010329159823332
Epoch 79/100: Training Loss: 0.0007889938729626315
Epoch 80/100: Training Loss: 0.0005298655975115049
Epoch 81/100: Training Loss: 0.0007392167524024324
Epoch 82/100: Training Loss: 0.0008158144000526908
Epoch 83/100: Training Loss: 0.0007509476133993456
Epoch 84/100: Training Loss: 0.0009149956953275454
Epoch 85/100: Training Loss: 0.0006400266518959632
Epoch 86/100: Training Loss: 0.00045243588152465286
Epoch 87/100: Training Loss: 0.0007664255433149271
Epoch 88/100: Training Loss: 0.0008862442695177519
Epoch 89/100: Training Loss: 0.001554003753862181
Epoch 90/100: Training Loss: 0.000789946401035869
Epoch 91/100: Training Loss: 0.0006490559219480394
Epoch 92/100: Training Loss: 0.0008187625374827352
Epoch 93/100: Training Loss: 0.0004766301050052776
Epoch 94/100: Training Loss: 0.0006293441865827654
Epoch 95/100: Training Loss: 0.0005836524329819046
Epoch 96/100: Training Loss: 0.0004260434241561623
Epoch 97/100: Training Loss: 0.000763188344198507
Epoch 98/100: Training Loss: 0.0006301778179782254
Epoch 99/100: Training Loss: 0.0006210633091159633
Epoch 0/100: Training Loss: 0.003714072954404604
Epoch 1/100: Training Loss: 0.00421772761778398
Epoch 2/100: Training Loss: 0.004030995852463729
Epoch 3/100: Training Loss: 0.004383184693076394
Epoch 4/100: Training Loss: 0.0024852736012918966
Epoch 5/100: Training Loss: 0.0037829700883451876
Epoch 6/100: Training Loss: 0.004271593544033024
Epoch 7/100: Training Loss: 0.004064679145812988
Epoch 8/100: Training Loss: 0.0028321938914852543
Epoch 9/100: Training Loss: 0.0031831906392024113
Epoch 10/100: Training Loss: 0.002894355283750521
Epoch 11/100: Training Loss: 0.003104895144909412
Epoch 12/100: Training Loss: 0.0029068095283908443
Epoch 13/100: Training Loss: 0.0029640839650080754
Epoch 14/100: Training Loss: 0.003107893300223184
Epoch 15/100: Training Loss: 0.0023965010276207556
Epoch 16/100: Training Loss: 0.0025907283896332853
Epoch 17/100: Training Loss: 0.0022415377043343925
Epoch 18/100: Training Loss: 0.0027446640538168953
Epoch 19/100: Training Loss: 0.0025990126016256694
Epoch 20/100: Training Loss: 0.002563371108128474
Epoch 21/100: Training Loss: 0.0019101654732977594
Epoch 22/100: Training Loss: 0.0018897212885476493
Epoch 23/100: Training Loss: 0.0017534884122701792
Epoch 24/100: Training Loss: 0.002082613381472501
Epoch 25/100: Training Loss: 0.003768847955690397
Epoch 26/100: Training Loss: 0.0019408683260004004
Epoch 27/100: Training Loss: 0.002295957161830022
Epoch 28/100: Training Loss: 0.0013700703849325647
Epoch 29/100: Training Loss: 0.002343153620099688
Epoch 30/100: Training Loss: 0.002015561907441466
Epoch 31/100: Training Loss: 0.0019633503226967127
Epoch 32/100: Training Loss: 0.001406979519170481
Epoch 33/100: Training Loss: 0.0022142742063615706
Epoch 34/100: Training Loss: 0.0016349395671924512
Epoch 35/100: Training Loss: 0.0017643958538562268
Epoch 36/100: Training Loss: 0.0021654557098041882
Epoch 37/100: Training Loss: 0.0013479231954454543
Epoch 38/100: Training Loss: 0.0022357866897449626
Epoch 39/100: Training Loss: 0.0018372202253008221
Epoch 40/100: Training Loss: 0.001668875450854535
Epoch 41/100: Training Loss: 0.0017757849259810014
Epoch 42/100: Training Loss: 0.0016844762371970223
Epoch 43/100: Training Loss: 0.0015901147902428686
Epoch 44/100: Training Loss: 0.001218733254012528
Epoch 45/100: Training Loss: 0.002986941721055891
Epoch 46/100: Training Loss: 0.002772892688537811
Epoch 47/100: Training Loss: 0.0019919209546976156
Epoch 48/100: Training Loss: 0.0017555226812829504
Epoch 49/100: Training Loss: 0.0017587246594729124
Epoch 50/100: Training Loss: 0.0009210980855501615
Epoch 51/100: Training Loss: 0.0010027079940675857
Epoch 52/100: Training Loss: 0.001172778072890702
Epoch 53/100: Training Loss: 0.001248559126487145
Epoch 54/100: Training Loss: 0.0008871446956287731
Epoch 55/100: Training Loss: 0.0010189536984983858
Epoch 56/100: Training Loss: 0.0019055156857817324
Epoch 57/100: Training Loss: 0.001366827037784603
Epoch 58/100: Training Loss: 0.0012405103111600543
Epoch 59/100: Training Loss: 0.0008954601687984866
Epoch 60/100: Training Loss: 0.0016109422370270416
Epoch 61/100: Training Loss: 0.0012225490975213218
Epoch 62/100: Training Loss: 0.0012486212320261068
Epoch 63/100: Training Loss: 0.0006137965963437007
Epoch 64/100: Training Loss: 0.0012470102393543803
Epoch 65/100: Training Loss: 0.0006713388891486855
Epoch 66/100: Training Loss: 0.001035295270539664
Epoch 67/100: Training Loss: 0.0010368044559772199
Epoch 68/100: Training Loss: 0.0005286149941124283
Epoch 69/100: Training Loss: 0.0006793972912368241
Epoch 70/100: Training Loss: 0.0014480100228236271
Epoch 71/100: Training Loss: 0.0013716347984500698
Epoch 72/100: Training Loss: 0.0004639845419596959
Epoch 73/100: Training Loss: 0.0005012810021847278
Epoch 74/100: Training Loss: 0.0008234862055811849
Epoch 75/100: Training Loss: 0.0006667868449137761
Epoch 76/100: Training Loss: 0.0008919393265997614
Epoch 77/100: Training Loss: 0.0001812935057845149
Epoch 78/100: Training Loss: 0.0010855202074651117
Epoch 79/100: Training Loss: 0.000421211670537095
Epoch 80/100: Training Loss: 0.0008664025918587104
Epoch 81/100: Training Loss: 0.00042712000700143667
Epoch 82/100: Training Loss: 0.00041616963339852285
Epoch 83/100: Training Loss: 0.0006079915520194527
Epoch 84/100: Training Loss: 0.0006316603272111266
Epoch 85/100: Training Loss: 0.00026874885067239507
Epoch 86/100: Training Loss: 0.0008501148932463639
Epoch 87/100: Training Loss: 0.0010661178535514779
Epoch 88/100: Training Loss: 0.0009755706870472515
Epoch 89/100: Training Loss: 0.0008713736921757251
Epoch 90/100: Training Loss: 0.0004883700317436165
Epoch 91/100: Training Loss: 0.000949259925555516
Epoch 92/100: Training Loss: 8.136685286368524e-05
Epoch 93/100: Training Loss: 0.0006734415159358845
Epoch 94/100: Training Loss: 0.0009376730743821685
Epoch 95/100: Training Loss: 0.0003485629608581116
Epoch 96/100: Training Loss: 0.0011149133210415607
Epoch 97/100: Training Loss: 0.0006784051656723022
Epoch 98/100: Training Loss: 0.0014817135317342265
Epoch 99/100: Training Loss: 0.0011120950217013593
Epoch 0/100: Training Loss: 0.00363339494158338
Epoch 1/100: Training Loss: 0.004668879759061587
Epoch 2/100: Training Loss: 0.003494861659470138
Epoch 3/100: Training Loss: 0.00447550186744103
Epoch 4/100: Training Loss: 0.004476342167887655
Epoch 5/100: Training Loss: 0.004185279349347095
Epoch 6/100: Training Loss: 0.004388828377623658
Epoch 7/100: Training Loss: 0.004294889373379154
Epoch 8/100: Training Loss: 0.003567001202723363
Epoch 9/100: Training Loss: 0.003953153020018464
Epoch 10/100: Training Loss: 0.002631668444280024
Epoch 11/100: Training Loss: 0.0027648396008498185
Epoch 12/100: Training Loss: 0.0028961134123635457
Epoch 13/100: Training Loss: 0.0036860512686776115
Epoch 14/100: Training Loss: 0.004127242765226564
Epoch 15/100: Training Loss: 0.003536856257832134
Epoch 16/100: Training Loss: 0.003552579379581905
Epoch 17/100: Training Loss: 0.0026579163708053267
Epoch 18/100: Training Loss: 0.0034451555538844397
Epoch 19/100: Training Loss: 0.0033007405854605294
Epoch 20/100: Training Loss: 0.0016522878533476717
Epoch 21/100: Training Loss: 0.0024276873865327637
Epoch 22/100: Training Loss: 0.002300219102339311
Epoch 23/100: Training Loss: 0.0024975821688458634
Epoch 24/100: Training Loss: 0.00238887267512875
Epoch 25/100: Training Loss: 0.0013873041509748338
Epoch 26/100: Training Loss: 0.0017190472437785221
Epoch 27/100: Training Loss: 0.003066685649898502
Epoch 28/100: Training Loss: 0.0025131210580572384
Epoch 29/100: Training Loss: 0.0018734573484300735
Epoch 30/100: Training Loss: 0.002382922005820108
Epoch 31/100: Training Loss: 0.0020734034218154586
Epoch 32/100: Training Loss: 0.00262638280441711
Epoch 33/100: Training Loss: 0.00141954296952361
Epoch 34/100: Training Loss: 0.002212562761106691
Epoch 35/100: Training Loss: 0.0022064101445924984
Epoch 36/100: Training Loss: 0.00309435387591382
Epoch 37/100: Training Loss: 0.0022540221681128015
Epoch 38/100: Training Loss: 0.002212016732542665
Epoch 39/100: Training Loss: 0.0028030372166133427
Epoch 40/100: Training Loss: 0.001746893554300695
Epoch 41/100: Training Loss: 0.0019141000050764817
Epoch 42/100: Training Loss: 0.001718342721045434
Epoch 43/100: Training Loss: 0.0012619474551060818
Epoch 44/100: Training Loss: 0.0015348831673602124
Epoch 45/100: Training Loss: 0.0010555090604128538
Epoch 46/100: Training Loss: 0.0016015021951048525
Epoch 47/100: Training Loss: 0.0015320744547810588
Epoch 48/100: Training Loss: 0.0010599291824794316
Epoch 49/100: Training Loss: 0.001346200496166736
Epoch 50/100: Training Loss: 0.0011111371792279757
Epoch 51/100: Training Loss: 0.0007491053922193034
Epoch 52/100: Training Loss: 0.0013381724174206073
Epoch 53/100: Training Loss: 0.001157044426544563
Epoch 54/100: Training Loss: 0.0014183728636561574
Epoch 55/100: Training Loss: 0.0008880226136921169
Epoch 56/100: Training Loss: 0.0011598142710599032
Epoch 57/100: Training Loss: 0.0006841606610304826
Epoch 58/100: Training Loss: 0.0014103088345560994
Epoch 59/100: Training Loss: 0.0005714731824981583
Epoch 60/100: Training Loss: 0.0012050494030639008
Epoch 61/100: Training Loss: 0.0012439558139214148
Epoch 62/100: Training Loss: 0.0009906236733589972
Epoch 63/100: Training Loss: 0.0007853244359676654
Epoch 64/100: Training Loss: 0.0006953498700281957
Epoch 65/100: Training Loss: 0.0017088428660706207
Epoch 66/100: Training Loss: 0.0010515343059193003
Epoch 67/100: Training Loss: 0.0008347313720863183
Epoch 68/100: Training Loss: 0.0008592420628854444
Epoch 69/100: Training Loss: 0.0007977727409842965
Epoch 70/100: Training Loss: 0.001062761445145507
Epoch 71/100: Training Loss: 0.0006146115752366873
Epoch 72/100: Training Loss: 0.0006019900311956873
Epoch 73/100: Training Loss: 0.0008760305134566514
Epoch 74/100: Training Loss: 0.000602616400985451
Epoch 75/100: Training Loss: 0.0007256288211662452
Epoch 76/100: Training Loss: 0.0006458883831551025
Epoch 77/100: Training Loss: 0.0007742079092072441
Epoch 78/100: Training Loss: 0.0005947740136326609
Epoch 79/100: Training Loss: 0.0025175322185863147
Epoch 80/100: Training Loss: 0.001200032713529947
Epoch 81/100: Training Loss: 0.0010729695741946881
Epoch 82/100: Training Loss: 0.000496840404046999
Epoch 83/100: Training Loss: 0.0003399199226519445
Epoch 84/100: Training Loss: 0.0008104080920452838
Epoch 85/100: Training Loss: 0.0010015149633367578
Epoch 86/100: Training Loss: 0.00030060112476348877
Epoch 87/100: Training Loss: 0.001145208840603595
Epoch 88/100: Training Loss: 0.0008810980545057284
Epoch 89/100: Training Loss: 0.0007569700271099597
Epoch 90/100: Training Loss: 0.0009999189760301497
Epoch 91/100: Training Loss: 0.0008605444556349641
Epoch 92/100: Training Loss: 0.00260422175580805
Epoch 93/100: Training Loss: 0.002954507207537031
Epoch 94/100: Training Loss: 0.0010754548377924032
Epoch 95/100: Training Loss: 0.0009998902157470062
Epoch 96/100: Training Loss: 0.0009253371011960756
Epoch 97/100: Training Loss: 0.0009183440800313349
Epoch 98/100: Training Loss: 0.000740054395648983
Epoch 99/100: Training Loss: 0.0008447641259306794
Epoch 0/100: Training Loss: 0.0031104000068149684
Epoch 1/100: Training Loss: 0.00332936735972305
Epoch 2/100: Training Loss: 0.004302587977216288
Epoch 3/100: Training Loss: 0.0034999138007134746
Epoch 4/100: Training Loss: 0.002195457922169036
Epoch 5/100: Training Loss: 0.0012108228323649776
Epoch 6/100: Training Loss: 0.0006443597604892005
Epoch 7/100: Training Loss: 0.000993830386114998
Epoch 8/100: Training Loss: 0.0006508162599399778
Epoch 9/100: Training Loss: 0.0010557039383730274
Epoch 10/100: Training Loss: 0.0024037397712286263
Epoch 11/100: Training Loss: 0.0017408351956700987
Epoch 12/100: Training Loss: 0.00039331742956594457
Epoch 13/100: Training Loss: 0.0018132848242309195
Epoch 14/100: Training Loss: 0.00039032307322039924
Epoch 15/100: Training Loss: 0.0004248600795956477
Epoch 16/100: Training Loss: 0.0010033056048527817
Epoch 17/100: Training Loss: 0.00034809798185079377
Epoch 18/100: Training Loss: 0.0003040358881277541
Epoch 19/100: Training Loss: 0.0002303342649176077
Epoch 20/100: Training Loss: 0.0005171477246138216
Epoch 21/100: Training Loss: 0.0003750639184852319
Epoch 22/100: Training Loss: 0.0003138106384891674
Epoch 23/100: Training Loss: 0.00042897910793866117
Epoch 24/100: Training Loss: 0.00019927071464573677
Epoch 25/100: Training Loss: 0.0021246766751529247
Epoch 26/100: Training Loss: 6.975931761089278e-05
Epoch 27/100: Training Loss: 0.0007815350966950867
Epoch 28/100: Training Loss: 0.0001443289334605808
Epoch 29/100: Training Loss: 0.0002806976300441414
Epoch 30/100: Training Loss: 6.288118043377355e-05
Epoch 31/100: Training Loss: 1.2712183185607377e-05
Epoch 32/100: Training Loss: 1.1121582788375258e-05
Epoch 33/100: Training Loss: 2.6750998240124228e-05
Epoch 34/100: Training Loss: 8.730936589782223e-05
Epoch 35/100: Training Loss: 3.9080903505437945e-05
Epoch 36/100: Training Loss: 4.5789437128539466e-05
Epoch 37/100: Training Loss: 5.1965531350279146e-05
Epoch 38/100: Training Loss: 1.5754429794893674e-05
Epoch 39/100: Training Loss: 0.00017246357136708828
Epoch 40/100: Training Loss: 3.585498494330359e-05
Epoch 41/100: Training Loss: 0.00016919746309335977
Epoch 42/100: Training Loss: 0.00012940246086179112
Epoch 43/100: Training Loss: 7.946698989239207e-05
Epoch 44/100: Training Loss: 1.356384339828067e-05
Epoch 45/100: Training Loss: 2.5994478627765106e-05
Epoch 46/100: Training Loss: 0.0008029038189378984
Epoch 47/100: Training Loss: 5.448384473294568e-05
Epoch 48/100: Training Loss: 7.82896824569607e-06
Epoch 49/100: Training Loss: 6.413910583667229e-05
Epoch 50/100: Training Loss: 2.135557802451169e-05
Epoch 51/100: Training Loss: 1.5120137699999692e-05
Epoch 52/100: Training Loss: 5.815888360782635e-05
Epoch 53/100: Training Loss: 1.5972456409155958e-05
Epoch 54/100: Training Loss: 0.00016414768353927356
Epoch 55/100: Training Loss: 3.4219016852371534e-05
Epoch 56/100: Training Loss: 9.09351360944151e-06
Epoch 57/100: Training Loss: 4.90086650235887e-06
Epoch 58/100: Training Loss: 1.6874516957033814e-06
Epoch 59/100: Training Loss: 1.7452974335227641e-06
Epoch 60/100: Training Loss: 3.821304013689786e-06
Epoch 61/100: Training Loss: 8.62514905837233e-06
Epoch 62/100: Training Loss: 0.0003233025410423981
Epoch 63/100: Training Loss: 3.3091930843935425e-05
Epoch 64/100: Training Loss: 3.3787797359418286e-05
Epoch 65/100: Training Loss: 3.0403171146025686e-05
Epoch 66/100: Training Loss: 5.006828817447499e-07
Epoch 67/100: Training Loss: 1.274537226539448e-05
Epoch 68/100: Training Loss: 5.222421048060517e-05
Epoch 69/100: Training Loss: 1.5103669574289966e-05
Epoch 70/100: Training Loss: 2.5913159311549064e-05
Epoch 71/100: Training Loss: 8.817560730810546e-06
Epoch 72/100: Training Loss: 8.420924623239717e-07
Epoch 73/100: Training Loss: 2.4274525809690264e-05
Epoch 74/100: Training Loss: 1.2567371452810025e-06
Epoch 75/100: Training Loss: 4.591803820632352e-06
Epoch 76/100: Training Loss: 1.3619477066434233e-05
Epoch 77/100: Training Loss: 2.9671637261389224e-06
Epoch 78/100: Training Loss: 4.7640389322868883e-05
Epoch 79/100: Training Loss: 2.170772365997174e-05
Epoch 80/100: Training Loss: 1.4084644302828638e-06
Epoch 81/100: Training Loss: 6.458586802494718e-07
Epoch 82/100: Training Loss: 5.099280348813241e-06
Epoch 83/100: Training Loss: 4.3205361230516e-06
Epoch 84/100: Training Loss: 3.680246139368031e-06
Epoch 85/100: Training Loss: 8.475190768464029e-07
Epoch 86/100: Training Loss: 1.579294969653425e-05
Epoch 87/100: Training Loss: 1.7524224083207868e-05
Epoch 88/100: Training Loss: 3.338031802166459e-05
Epoch 89/100: Training Loss: 3.5572471382793473e-05
Epoch 90/100: Training Loss: 4.322595174418636e-06
Epoch 91/100: Training Loss: 5.011408206966757e-06
Epoch 92/100: Training Loss: 4.021748968262934e-07
Epoch 93/100: Training Loss: 3.6804425009480603e-07
Epoch 94/100: Training Loss: 4.106298363281905e-06
Epoch 95/100: Training Loss: 1.7061193925014303e-05
Epoch 96/100: Training Loss: 6.205697138250971e-05
Epoch 97/100: Training Loss: 1.4151126557698278e-05
Epoch 98/100: Training Loss: 4.185790030365707e-06
Epoch 99/100: Training Loss: 2.4103904871464322e-06
Epoch 0/100: Training Loss: 0.003468326995709191
Epoch 1/100: Training Loss: 0.0030748153025387257
Epoch 2/100: Training Loss: 0.002569579090808798
Epoch 3/100: Training Loss: 0.0032849882277974324
Epoch 4/100: Training Loss: 0.002293523103912915
Epoch 5/100: Training Loss: 0.0017743827375166256
Epoch 6/100: Training Loss: 0.0011304602301193893
Epoch 7/100: Training Loss: 0.0010997245282483247
Epoch 8/100: Training Loss: 0.0008723378364293853
Epoch 9/100: Training Loss: 0.0012107624963748674
Epoch 10/100: Training Loss: 0.0015621518064861649
Epoch 11/100: Training Loss: 0.0009706236102098336
Epoch 12/100: Training Loss: 0.0006136727845010582
Epoch 13/100: Training Loss: 0.0005677232713055757
Epoch 14/100: Training Loss: 0.0009544359211541392
Epoch 15/100: Training Loss: 0.0005325289226017115
Epoch 16/100: Training Loss: 0.0004022281868326152
Epoch 17/100: Training Loss: 0.00040827760118648316
Epoch 18/100: Training Loss: 0.0003418042524460635
Epoch 19/100: Training Loss: 0.00012343482005815564
Epoch 20/100: Training Loss: 0.00010846607798447638
Epoch 21/100: Training Loss: 0.00032906083424398503
Epoch 22/100: Training Loss: 0.00018694194639387306
Epoch 23/100: Training Loss: 0.0001335110675337856
Epoch 24/100: Training Loss: 0.00036185646166830707
Epoch 25/100: Training Loss: 0.0003975450626911561
Epoch 26/100: Training Loss: 0.00040674735249185856
Epoch 27/100: Training Loss: 0.0004624684621219986
Epoch 28/100: Training Loss: 0.0007634215468277961
Epoch 29/100: Training Loss: 0.00036085694114123385
Epoch 30/100: Training Loss: 0.0001846153769025042
Epoch 31/100: Training Loss: 0.00022812891591545995
Epoch 32/100: Training Loss: 0.0002915376534491229
Epoch 33/100: Training Loss: 1.6319226942087976e-05
Epoch 34/100: Training Loss: 5.828085258328842e-05
Epoch 35/100: Training Loss: 0.00022454646650267525
Epoch 36/100: Training Loss: 0.00014604374400677126
Epoch 37/100: Training Loss: 1.2242403795565565e-05
Epoch 38/100: Training Loss: 0.00026944019128939857
Epoch 39/100: Training Loss: 0.00032022439994695
Epoch 40/100: Training Loss: 0.00028955968428243155
Epoch 41/100: Training Loss: 5.7957072290906145e-05
Epoch 42/100: Training Loss: 3.991660839118109e-05
Epoch 43/100: Training Loss: 5.941620907900524e-06
Epoch 44/100: Training Loss: 1.1468034786132216e-05
Epoch 45/100: Training Loss: 0.00014472236296881927
Epoch 46/100: Training Loss: 0.0004421222063661353
Epoch 47/100: Training Loss: 0.0012198456225951024
Epoch 48/100: Training Loss: 0.0006458015163983304
Epoch 49/100: Training Loss: 0.00039220020814907333
Epoch 50/100: Training Loss: 8.919796237916303e-06
Epoch 51/100: Training Loss: 5.214410102669081e-06
Epoch 52/100: Training Loss: 5.2906281423349324e-05
Epoch 53/100: Training Loss: 9.650350948281448e-06
Epoch 54/100: Training Loss: 0.00010821473516203874
Epoch 55/100: Training Loss: 5.542446910603646e-05
Epoch 56/100: Training Loss: 4.771433233574856e-05
Epoch 57/100: Training Loss: 1.0073127116482317e-05
Epoch 58/100: Training Loss: 0.0006468127841598417
Epoch 59/100: Training Loss: 0.0004898840847190904
Epoch 60/100: Training Loss: 7.401075963348014e-07
Epoch 61/100: Training Loss: 3.3008197516751435e-05
Epoch 62/100: Training Loss: 2.2421633836133347e-05
Epoch 63/100: Training Loss: 8.492745556211544e-06
Epoch 64/100: Training Loss: 2.9805187775861998e-05
Epoch 65/100: Training Loss: 4.786258117735751e-06
Epoch 66/100: Training Loss: 8.023866777953926e-06
Epoch 67/100: Training Loss: 1.2624800273794338e-05
Epoch 68/100: Training Loss: 6.117318054101218e-06
Epoch 69/100: Training Loss: 0.00012930048389668845
Epoch 70/100: Training Loss: 5.263188408196338e-05
Epoch 71/100: Training Loss: 9.205876821023556e-05
Epoch 72/100: Training Loss: 2.855240464484765e-05
Epoch 73/100: Training Loss: 0.0003345727326314142
Epoch 74/100: Training Loss: 1.2253102577902789e-05
Epoch 75/100: Training Loss: 5.550616266537298e-05
Epoch 76/100: Training Loss: 5.863608358187909e-06
Epoch 77/100: Training Loss: 6.743119686750547e-06
Epoch 78/100: Training Loss: 1.4840158136801485e-05
Epoch 79/100: Training Loss: 5.0074747260774204e-06
Epoch 80/100: Training Loss: 4.605465480040736e-06
Epoch 81/100: Training Loss: 9.694986582442295e-06
Epoch 82/100: Training Loss: 3.017575533252118e-07
Epoch 83/100: Training Loss: 2.6598420962577036e-06
Epoch 84/100: Training Loss: 6.802488072701027e-05
Epoch 85/100: Training Loss: 5.075924007073502e-05
Epoch 86/100: Training Loss: 6.5343591607421454e-06
Epoch 87/100: Training Loss: 2.9327119329224333e-05
Epoch 88/100: Training Loss: 1.1402335834969406e-05
Epoch 89/100: Training Loss: 1.4760317223752203e-05
Epoch 90/100: Training Loss: 4.187962640236857e-06
Epoch 91/100: Training Loss: 3.497637351232072e-05
Epoch 92/100: Training Loss: 5.958995448305426e-07
Epoch 93/100: Training Loss: 1.967248065465707e-06
Epoch 94/100: Training Loss: 7.494941262379746e-06
Epoch 95/100: Training Loss: 1.8411815206468836e-06
Epoch 96/100: Training Loss: 1.1635177548791916e-06
Epoch 97/100: Training Loss: 3.645370465823295e-06
Epoch 98/100: Training Loss: 2.5151935591722375e-06
Epoch 99/100: Training Loss: 4.0949549221725094e-07
Epoch 0/100: Training Loss: 0.003471477265738271
Epoch 1/100: Training Loss: 0.0020908082555408125
Epoch 2/100: Training Loss: 0.0027982866837203135
Epoch 3/100: Training Loss: 0.003313181590448859
Epoch 4/100: Training Loss: 0.0022165336491871464
Epoch 5/100: Training Loss: 0.001320339602195412
Epoch 6/100: Training Loss: 0.0013276945593898282
Epoch 7/100: Training Loss: 0.0024292406860304755
Epoch 8/100: Training Loss: 0.0011406187996542527
Epoch 9/100: Training Loss: 0.0009612937463573151
Epoch 10/100: Training Loss: 0.001079799116023479
Epoch 11/100: Training Loss: 0.0024519950334279815
Epoch 12/100: Training Loss: 0.0020081510573077056
Epoch 13/100: Training Loss: 0.001964971331730942
Epoch 14/100: Training Loss: 0.0008882854613789751
Epoch 15/100: Training Loss: 0.000647592718250181
Epoch 16/100: Training Loss: 0.0005853109381681571
Epoch 17/100: Training Loss: 0.0006086153029664162
Epoch 18/100: Training Loss: 0.0004917170189641005
Epoch 19/100: Training Loss: 0.0003677643835544586
Epoch 20/100: Training Loss: 0.0002940715136337865
Epoch 21/100: Training Loss: 0.00018824640364369
Epoch 22/100: Training Loss: 0.00013353059673967536
Epoch 23/100: Training Loss: 0.0002287716084828406
Epoch 24/100: Training Loss: 8.009406138051507e-05
Epoch 25/100: Training Loss: 0.00016273288456208866
Epoch 26/100: Training Loss: 6.375440104973097e-05
Epoch 27/100: Training Loss: 5.7917745338261495e-05
Epoch 28/100: Training Loss: 0.0002687182163168316
Epoch 29/100: Training Loss: 0.00013564227640263141
Epoch 30/100: Training Loss: 0.00021190459102940707
Epoch 31/100: Training Loss: 0.00014113425705696177
Epoch 32/100: Training Loss: 1.4213900841726848e-05
Epoch 33/100: Training Loss: 0.0009004069617920858
Epoch 34/100: Training Loss: 4.8700297262770996e-05
Epoch 35/100: Training Loss: 6.608451299506463e-05
Epoch 36/100: Training Loss: 4.079309149296737e-05
Epoch 37/100: Training Loss: 8.888896326163064e-05
Epoch 38/100: Training Loss: 1.62052184891847e-05
Epoch 39/100: Training Loss: 1.4176003726347824e-05
Epoch 40/100: Training Loss: 0.00011158503729141563
Epoch 41/100: Training Loss: 7.333648191090742e-06
Epoch 42/100: Training Loss: 5.561447033852887e-06
Epoch 43/100: Training Loss: 5.006999280189444e-05
Epoch 44/100: Training Loss: 0.0006808154970590323
Epoch 45/100: Training Loss: 0.00039524831837671665
Epoch 46/100: Training Loss: 2.0644657508087304e-05
Epoch 47/100: Training Loss: 3.7997935332952104e-05
Epoch 48/100: Training Loss: 1.7332171655597132e-05
Epoch 49/100: Training Loss: 0.001696609463428427
Epoch 50/100: Training Loss: 0.0002747085288258418
Epoch 51/100: Training Loss: 5.103065489442802e-05
Epoch 52/100: Training Loss: 1.3075373278347993e-05
Epoch 53/100: Training Loss: 2.4684890533334638e-05
Epoch 54/100: Training Loss: 2.933206340937984e-06
Epoch 55/100: Training Loss: 1.978940306162816e-06
Epoch 56/100: Training Loss: 1.4872207346320883e-06
Epoch 57/100: Training Loss: 3.795755492877558e-06
Epoch 58/100: Training Loss: 4.838175637407537e-05
Epoch 59/100: Training Loss: 3.3930315424693874e-05
Epoch 60/100: Training Loss: 1.692487729679039e-06
Epoch 61/100: Training Loss: 3.1257138953207096e-06
Epoch 62/100: Training Loss: 1.2049606274184914e-06
Epoch 63/100: Training Loss: 0.00010716144932559663
Epoch 64/100: Training Loss: 2.1909464984857963e-05
Epoch 65/100: Training Loss: 7.178344723819955e-05
Epoch 66/100: Training Loss: 6.221588329623448e-06
Epoch 67/100: Training Loss: 6.202738332145053e-06
Epoch 68/100: Training Loss: 1.3338550468163988e-05
Epoch 69/100: Training Loss: 1.677350809603381e-05
Epoch 70/100: Training Loss: 9.41449879137285e-05
Epoch 71/100: Training Loss: 5.164808962219881e-06
Epoch 72/100: Training Loss: 0.0008018592749636597
Epoch 73/100: Training Loss: 9.785750252337543e-05
Epoch 74/100: Training Loss: 6.352888385942377e-05
Epoch 75/100: Training Loss: 1.7878093808158043e-06
Epoch 76/100: Training Loss: 2.5615697095250608e-06
Epoch 77/100: Training Loss: 5.758532460070461e-06
Epoch 78/100: Training Loss: 1.4323584072611814e-05
Epoch 79/100: Training Loss: 1.7556587542675756e-05
Epoch 80/100: Training Loss: 8.536768581523942e-07
Epoch 81/100: Training Loss: 9.625865163658659e-06
Epoch 82/100: Training Loss: 3.7076976578796932e-06
Epoch 83/100: Training Loss: 7.53406823778445e-06
Epoch 84/100: Training Loss: 1.8289132740782814e-06
Epoch 85/100: Training Loss: 6.785073483282803e-05
Epoch 86/100: Training Loss: 0.001700560922271635
Epoch 87/100: Training Loss: 0.0003854819327775686
Epoch 88/100: Training Loss: 0.00017968442953803057
Epoch 89/100: Training Loss: 6.131435875878012e-05
Epoch 90/100: Training Loss: 4.672120750933154e-06
Epoch 91/100: Training Loss: 1.2315100164615486e-06
Epoch 92/100: Training Loss: 9.907777364040079e-06
Epoch 93/100: Training Loss: 7.254423493622271e-05
Epoch 94/100: Training Loss: 2.550720265763669e-05
Epoch 95/100: Training Loss: 2.0330533259576816e-05
Epoch 96/100: Training Loss: 4.759700426066214e-06
Epoch 97/100: Training Loss: 1.3966643268619578e-05
Epoch 98/100: Training Loss: 8.101092274386458e-06
Epoch 99/100: Training Loss: 0.0005694494291317244
Epoch 0/100: Training Loss: 0.003250826895236969
Epoch 1/100: Training Loss: 0.0026800546795129774
Epoch 2/100: Training Loss: 0.0027373362332582473
Epoch 3/100: Training Loss: 0.0018867958337068559
Epoch 4/100: Training Loss: 0.002667779102921486
Epoch 5/100: Training Loss: 0.0024525105953216554
Epoch 6/100: Training Loss: 0.0022322086617350577
Epoch 7/100: Training Loss: 0.00229057390242815
Epoch 8/100: Training Loss: 0.0020957255735993387
Epoch 9/100: Training Loss: 0.002262001298367977
Epoch 10/100: Training Loss: 0.001482030935585499
Epoch 11/100: Training Loss: 0.0011669933795928956
Epoch 12/100: Training Loss: 0.0014307443052530289
Epoch 13/100: Training Loss: 0.0017745332792401315
Epoch 14/100: Training Loss: 0.0019159374758601188
Epoch 15/100: Training Loss: 0.0020262885838747023
Epoch 16/100: Training Loss: 0.0012075510807335378
Epoch 17/100: Training Loss: 0.0014233147725462914
Epoch 18/100: Training Loss: 0.0012189297005534172
Epoch 19/100: Training Loss: 0.0007779581472277641
Epoch 20/100: Training Loss: 0.0008200209587812424
Epoch 21/100: Training Loss: 0.0004186832346022129
Epoch 22/100: Training Loss: 0.00032364523503929376
Epoch 23/100: Training Loss: 0.0007697306573390961
Epoch 24/100: Training Loss: 0.00034365963656455276
Epoch 25/100: Training Loss: 0.00017533162608742713
Epoch 26/100: Training Loss: 0.00011172585655003787
Epoch 27/100: Training Loss: 0.0003907217178493738
Epoch 28/100: Training Loss: 0.0006841549184173346
Epoch 29/100: Training Loss: 0.0004208511672914028
Epoch 30/100: Training Loss: 0.00024486954789608716
Epoch 31/100: Training Loss: 0.0001970570534467697
Epoch 32/100: Training Loss: 0.00029644998721778393
Epoch 33/100: Training Loss: 0.0002757604233920574
Epoch 34/100: Training Loss: 0.00017740242183208465
Epoch 35/100: Training Loss: 0.00011322405189275741
Epoch 36/100: Training Loss: 0.0006527075543999672
Epoch 37/100: Training Loss: 0.0009069126099348068
Epoch 38/100: Training Loss: 0.000299826730042696
Epoch 39/100: Training Loss: 0.00021751837339252235
Epoch 40/100: Training Loss: 0.00010236988309770822
Epoch 41/100: Training Loss: 0.00013138309586793184
Epoch 42/100: Training Loss: 0.00015306356362998486
Epoch 43/100: Training Loss: 0.00011893236078321933
Epoch 44/100: Training Loss: 8.786626276560127e-05
Epoch 45/100: Training Loss: 0.000205970648676157
Epoch 46/100: Training Loss: 0.00025115746539086106
Epoch 47/100: Training Loss: 8.52596596814692e-05
Epoch 48/100: Training Loss: 0.0006935346871614456
Epoch 49/100: Training Loss: 7.922392687760293e-05
Epoch 50/100: Training Loss: 8.322669891640544e-05
Epoch 51/100: Training Loss: 0.00017284236382693053
Epoch 52/100: Training Loss: 6.772424094378949e-05
Epoch 53/100: Training Loss: 7.943096570670605e-05
Epoch 54/100: Training Loss: 7.619659299962223e-05
Epoch 55/100: Training Loss: 6.030489457771182e-05
Epoch 56/100: Training Loss: 9.041958255693317e-05
Epoch 57/100: Training Loss: 7.160590030252934e-05
Epoch 58/100: Training Loss: 2.6671538944356143e-05
Epoch 59/100: Training Loss: 4.19034855440259e-05
Epoch 60/100: Training Loss: 1.4416399062611163e-05
Epoch 61/100: Training Loss: 3.805865999311209e-05
Epoch 62/100: Training Loss: 3.809493209701031e-05
Epoch 63/100: Training Loss: 4.806258075404912e-05
Epoch 64/100: Training Loss: 5.0952989840880034e-05
Epoch 65/100: Training Loss: 4.549319564830512e-05
Epoch 66/100: Training Loss: 0.00013543613022193313
Epoch 67/100: Training Loss: 3.903010510839522e-05
Epoch 68/100: Training Loss: 4.8475177027285096e-05
Epoch 69/100: Training Loss: 3.820195270236582e-05
Epoch 70/100: Training Loss: 2.8802620363421738e-05
Epoch 71/100: Training Loss: 1.906787947518751e-05
Epoch 72/100: Training Loss: 4.4178086682222784e-05
Epoch 73/100: Training Loss: 2.3027708812151106e-05
Epoch 74/100: Training Loss: 4.151751636527479e-05
Epoch 75/100: Training Loss: 5.0286197802051905e-05
Epoch 76/100: Training Loss: 4.359884769655764e-05
Epoch 77/100: Training Loss: 3.740875399671495e-05
Epoch 78/100: Training Loss: 6.303753471001983e-05
Epoch 79/100: Training Loss: 2.8766045579686762e-05
Epoch 80/100: Training Loss: 2.965447783935815e-05
Epoch 81/100: Training Loss: 1.094177205231972e-05
Epoch 82/100: Training Loss: 2.829160075634718e-05
Epoch 83/100: Training Loss: 2.8263707645237445e-05
Epoch 84/100: Training Loss: 3.5024856333620844e-05
Epoch 85/100: Training Loss: 2.7095223776996137e-05
Epoch 86/100: Training Loss: 3.4148941631428896e-05
Epoch 87/100: Training Loss: 9.026981424540282e-05
Epoch 88/100: Training Loss: 1.6917340690270067e-05
Epoch 89/100: Training Loss: 1.99032889213413e-05
Epoch 90/100: Training Loss: 2.175153204007074e-05
Epoch 91/100: Training Loss: 3.1757866963744164e-05
Epoch 92/100: Training Loss: 4.986452986486256e-05
Epoch 93/100: Training Loss: 8.483529381919652e-06
Epoch 94/100: Training Loss: 5.763320368714631e-05
Epoch 95/100: Training Loss: 6.730265449732542e-05
Epoch 96/100: Training Loss: 1.8009330960921944e-05
Epoch 97/100: Training Loss: 2.2378222638508306e-06
Epoch 98/100: Training Loss: 1.8646505850483663e-06
Epoch 99/100: Training Loss: 5.210483504924923e-06
Epoch 0/100: Training Loss: 0.003022164851427078
Epoch 1/100: Training Loss: 0.0030178574845194815
Epoch 2/100: Training Loss: 0.002504209615290165
Epoch 3/100: Training Loss: 0.002338551729917526
Epoch 4/100: Training Loss: 0.002344425581395626
Epoch 5/100: Training Loss: 0.002236679755151272
Epoch 6/100: Training Loss: 0.0018606333062052727
Epoch 7/100: Training Loss: 0.0012334898114204408
Epoch 8/100: Training Loss: 0.0004037095699459314
Epoch 9/100: Training Loss: 0.0009667037054896354
Epoch 10/100: Training Loss: 0.001319041009992361
Epoch 11/100: Training Loss: 0.0019293149933218956
Epoch 12/100: Training Loss: 0.001401088386774063
Epoch 13/100: Training Loss: 0.0014145674183964729
Epoch 14/100: Training Loss: 0.001808169111609459
Epoch 15/100: Training Loss: 0.0011063210666179657
Epoch 16/100: Training Loss: 0.0012321101501584053
Epoch 17/100: Training Loss: 0.0012658740393817424
Epoch 18/100: Training Loss: 0.00145507100969553
Epoch 19/100: Training Loss: 0.0011894725263118743
Epoch 20/100: Training Loss: 0.00013625497231259943
Epoch 21/100: Training Loss: 0.0020091725513339044
Epoch 22/100: Training Loss: 0.0010595043189823628
Epoch 23/100: Training Loss: 0.0010559190064668655
Epoch 24/100: Training Loss: 0.00022050803527235986
Epoch 25/100: Training Loss: 0.0011429470963776112
Epoch 26/100: Training Loss: 0.00020825304090976715
Epoch 27/100: Training Loss: 0.000732242688536644
Epoch 28/100: Training Loss: 0.0008495159447193146
Epoch 29/100: Training Loss: 0.0013716531917452811
Epoch 30/100: Training Loss: 0.0011834551580250264
Epoch 31/100: Training Loss: 0.0007323065772652626
Epoch 32/100: Training Loss: 0.00042707519605755806
Epoch 33/100: Training Loss: 0.0007348149083554745
Epoch 34/100: Training Loss: 0.0010705769062042236
Epoch 35/100: Training Loss: 8.806319674476981e-05
Epoch 36/100: Training Loss: 0.0003105450654402375
Epoch 37/100: Training Loss: 0.0003174487501382828
Epoch 38/100: Training Loss: 0.00016853713896125555
Epoch 39/100: Training Loss: 8.448399603366852e-05
Epoch 40/100: Training Loss: 0.00010311298537999392
Epoch 41/100: Training Loss: 5.793966120108962e-05
Epoch 42/100: Training Loss: 8.085576700977981e-05
Epoch 43/100: Training Loss: 1.924934331327677e-05
Epoch 44/100: Training Loss: 0.00010791425593197346
Epoch 45/100: Training Loss: 5.9886492090299724e-05
Epoch 46/100: Training Loss: 3.197211190126836e-05
Epoch 47/100: Training Loss: 3.7544494261965156e-05
Epoch 48/100: Training Loss: 3.698119253385812e-05
Epoch 49/100: Training Loss: 2.1759211085736753e-05
Epoch 50/100: Training Loss: 4.068113339599222e-05
Epoch 51/100: Training Loss: 1.7742035561241208e-05
Epoch 52/100: Training Loss: 4.316188569646329e-05
Epoch 53/100: Training Loss: 2.200411690864712e-05
Epoch 54/100: Training Loss: 0.0001522056059911847
Epoch 55/100: Training Loss: 1.5379507385659962e-05
Epoch 56/100: Training Loss: 1.0678506077965721e-05
Epoch 57/100: Training Loss: 5.069411054137163e-06
Epoch 58/100: Training Loss: 1.0745760664576665e-05
Epoch 59/100: Training Loss: 1.2174980656709523e-05
Epoch 60/100: Training Loss: 1.1283631465630606e-05
Epoch 61/100: Training Loss: 7.603367703268305e-06
Epoch 62/100: Training Loss: 7.634551730006933e-05
Epoch 63/100: Training Loss: 1.1481942237878683e-06
Epoch 64/100: Training Loss: 1.5858745609875768e-05
Epoch 65/100: Training Loss: 5.1230526878498495e-06
Epoch 66/100: Training Loss: 2.739477204158902e-05
Epoch 67/100: Training Loss: 0.0014022346585988998
Epoch 68/100: Training Loss: 1.2037327542202547e-05
Epoch 69/100: Training Loss: 4.402772901812568e-06
Epoch 70/100: Training Loss: 1.0907094838330522e-05
Epoch 71/100: Training Loss: 1.0975678742397577e-05
Epoch 72/100: Training Loss: 8.928853640099988e-06
Epoch 73/100: Training Loss: 0.00012191908899694681
Epoch 74/100: Training Loss: 1.704011629044544e-06
Epoch 75/100: Training Loss: 3.3988333598244934e-06
Epoch 76/100: Training Loss: 1.1455925414338707e-05
Epoch 77/100: Training Loss: 3.6047367757419124e-06
Epoch 78/100: Training Loss: 2.414787195448298e-06
Epoch 79/100: Training Loss: 1.609133032616228e-05
Epoch 80/100: Training Loss: 1.1339005141053349e-05
Epoch 81/100: Training Loss: 1.5839646221138536e-05
Epoch 82/100: Training Loss: 8.682344923727214e-06
Epoch 83/100: Training Loss: 1.2524139310698957e-05
Epoch 84/100: Training Loss: 6.659352948190644e-06
Epoch 85/100: Training Loss: 8.789886487647891e-06
Epoch 86/100: Training Loss: 7.069391722325236e-06
Epoch 87/100: Training Loss: 2.0780862541869283e-05
Epoch 88/100: Training Loss: 4.7848516260273755e-06
Epoch 89/100: Training Loss: 4.882011126028374e-06
Epoch 90/100: Training Loss: 3.7483499909285455e-06
Epoch 91/100: Training Loss: 3.539436511346139e-06
Epoch 92/100: Training Loss: 5.066198718850501e-06
Epoch 93/100: Training Loss: 5.04451418237295e-06
Epoch 94/100: Training Loss: 2.096097705361899e-06
Epoch 95/100: Training Loss: 6.666032277280465e-06
Epoch 96/100: Training Loss: 1.5137228183448315e-05
Epoch 97/100: Training Loss: 5.332755972631276e-06
Epoch 98/100: Training Loss: 2.901735024352092e-06
Epoch 99/100: Training Loss: 7.420613110298291e-06
Epoch 0/100: Training Loss: 0.0024414587765932082
Epoch 1/100: Training Loss: 0.002603520825505257
Epoch 2/100: Training Loss: 0.0019655700773000715
Epoch 3/100: Training Loss: 0.0021530933678150176
Epoch 4/100: Training Loss: 0.0019189245998859406
Epoch 5/100: Training Loss: 0.0015318435616791248
Epoch 6/100: Training Loss: 0.001975586451590061
Epoch 7/100: Training Loss: 0.0016170158982276917
Epoch 8/100: Training Loss: 0.0015033941715955734
Epoch 9/100: Training Loss: 0.002108095586299896
Epoch 10/100: Training Loss: 0.0014579864218831062
Epoch 11/100: Training Loss: 0.0016738595440983772
Epoch 12/100: Training Loss: 0.001844707690179348
Epoch 13/100: Training Loss: 0.0014797853305935859
Epoch 14/100: Training Loss: 0.0013539902865886688
Epoch 15/100: Training Loss: 0.0015963166952133179
Epoch 16/100: Training Loss: 0.0012160428799688817
Epoch 17/100: Training Loss: 0.001437940262258053
Epoch 18/100: Training Loss: 0.0011374386958777905
Epoch 19/100: Training Loss: 0.0016264759004116058
Epoch 20/100: Training Loss: 0.0013475574553012848
Epoch 21/100: Training Loss: 0.0013974392786622047
Epoch 22/100: Training Loss: 0.0012845483608543873
Epoch 23/100: Training Loss: 0.0014315011911094188
Epoch 24/100: Training Loss: 0.0013740655034780503
Epoch 25/100: Training Loss: 0.0011449677869677544
Epoch 26/100: Training Loss: 0.0009721936658024788
Epoch 27/100: Training Loss: 0.0004374947398900986
Epoch 28/100: Training Loss: 0.000282746204175055
Epoch 29/100: Training Loss: 0.00021468184422701597
Epoch 30/100: Training Loss: 0.00010002793278545141
Epoch 31/100: Training Loss: 0.0001622499548830092
Epoch 32/100: Training Loss: 0.00011789163108915091
Epoch 33/100: Training Loss: 0.0002698316937312484
Epoch 34/100: Training Loss: 0.00014974726364016532
Epoch 35/100: Training Loss: 0.0001078378176316619
Epoch 36/100: Training Loss: 7.872733986005186e-05
Epoch 37/100: Training Loss: 9.011321235448122e-05
Epoch 38/100: Training Loss: 0.0003364220960065722
Epoch 39/100: Training Loss: 0.00014255372807383537
Epoch 40/100: Training Loss: 0.0001191470422782004
Epoch 41/100: Training Loss: 0.0002907823072746396
Epoch 42/100: Training Loss: 0.0017934218049049377
Epoch 43/100: Training Loss: 0.0003517227713018656
Epoch 44/100: Training Loss: 5.814086180180311e-05
Epoch 45/100: Training Loss: 7.6551444362849e-05
Epoch 46/100: Training Loss: 0.0027257926762104035
Epoch 47/100: Training Loss: 0.0006470165215432643
Epoch 48/100: Training Loss: 0.0005269323475658894
Epoch 49/100: Training Loss: 0.00034307027235627177
Epoch 50/100: Training Loss: 0.0006703198421746492
Epoch 51/100: Training Loss: 0.0006203850731253624
Epoch 52/100: Training Loss: 0.0005188810639083386
Epoch 53/100: Training Loss: 0.0007855170406401157
Epoch 54/100: Training Loss: 0.0006165183149278164
Epoch 55/100: Training Loss: 0.0004480228293687105
Epoch 56/100: Training Loss: 6.338234525173902e-05
Epoch 57/100: Training Loss: 0.00010304006282240153
Epoch 58/100: Training Loss: 5.7445152197033164e-05
Epoch 59/100: Training Loss: 0.00021493558306246997
Epoch 60/100: Training Loss: 2.5690611801110207e-05
Epoch 61/100: Training Loss: 0.00038396238815039394
Epoch 62/100: Training Loss: 2.855782222468406e-05
Epoch 63/100: Training Loss: 8.527302416041493e-05
Epoch 64/100: Training Loss: 2.5754477246664464e-05
Epoch 65/100: Training Loss: 7.323125610128045e-05
Epoch 66/100: Training Loss: 5.353243905119598e-05
Epoch 67/100: Training Loss: 2.381892700213939e-05
Epoch 68/100: Training Loss: 2.0123555441386997e-05
Epoch 69/100: Training Loss: 1.9031815463677047e-05
Epoch 70/100: Training Loss: 2.5308047770522536e-05
Epoch 71/100: Training Loss: 2.1380148245953025e-05
Epoch 72/100: Training Loss: 7.922672375570983e-06
Epoch 73/100: Training Loss: 1.1596379772527144e-05
Epoch 74/100: Training Loss: 1.688749762251973e-05
Epoch 75/100: Training Loss: 4.0336832171306014e-05
Epoch 76/100: Training Loss: 2.6362214703112842e-05
Epoch 77/100: Training Loss: 2.8406860656104983e-05
Epoch 78/100: Training Loss: 3.5031521110795436e-05
Epoch 79/100: Training Loss: 2.5609711883589625e-05
Epoch 80/100: Training Loss: 4.3652256135828796e-05
Epoch 81/100: Training Loss: 1.523947503301315e-06
Epoch 82/100: Training Loss: 1.0225971345789731e-05
Epoch 83/100: Training Loss: 6.034737816662528e-06
Epoch 84/100: Training Loss: 4.4678946142084894e-05
Epoch 85/100: Training Loss: 8.556372631574049e-06
Epoch 86/100: Training Loss: 6.810444756411016e-06
Epoch 87/100: Training Loss: 5.6500888604205105e-06
Epoch 88/100: Training Loss: 9.37826553126797e-06
Epoch 89/100: Training Loss: 7.018750766292214e-05
Epoch 90/100: Training Loss: 3.4706387668848035e-05
Epoch 91/100: Training Loss: 7.17324874131009e-06
Epoch 92/100: Training Loss: 2.4269185087177902e-05
Epoch 93/100: Training Loss: 5.517734098248184e-05
Epoch 94/100: Training Loss: 5.405013871495612e-06
Epoch 95/100: Training Loss: 6.49260327918455e-06
Epoch 96/100: Training Loss: 5.660661190631799e-06
Epoch 97/100: Training Loss: 7.900506170699373e-06
Epoch 98/100: Training Loss: 4.416567389853299e-06
Epoch 99/100: Training Loss: 1.3464235235005616e-05
Epoch 0/100: Training Loss: 0.003835421458930726
Epoch 1/100: Training Loss: 0.0023178706882865567
Epoch 2/100: Training Loss: 0.0033683241552608027
Epoch 3/100: Training Loss: 0.0030192151950423126
Epoch 4/100: Training Loss: 0.0034064555623728758
Epoch 5/100: Training Loss: 0.002684137244133433
Epoch 6/100: Training Loss: 0.0021434156758010766
Epoch 7/100: Training Loss: 0.0024905462933194106
Epoch 8/100: Training Loss: 0.00268173046932099
Epoch 9/100: Training Loss: 0.002091300335659343
Epoch 10/100: Training Loss: 0.0025154194634431485
Epoch 11/100: Training Loss: 0.0021068237389728524
Epoch 12/100: Training Loss: 0.0020538801979866758
Epoch 13/100: Training Loss: 0.0017508898571038701
Epoch 14/100: Training Loss: 0.0016820121342968788
Epoch 15/100: Training Loss: 0.0018362123875101661
Epoch 16/100: Training Loss: 0.0015048705468511884
Epoch 17/100: Training Loss: 0.0014144247694379965
Epoch 18/100: Training Loss: 0.001409327338455589
Epoch 19/100: Training Loss: 0.0010078334884279093
Epoch 20/100: Training Loss: 0.0012349601194357417
Epoch 21/100: Training Loss: 0.0014723926592784323
Epoch 22/100: Training Loss: 0.0016574388856341125
Epoch 23/100: Training Loss: 0.0012955282144485765
Epoch 24/100: Training Loss: 0.0010724409370665338
Epoch 25/100: Training Loss: 0.0013354765191958967
Epoch 26/100: Training Loss: 0.0015582622616154374
Epoch 27/100: Training Loss: 0.0014886508701713222
Epoch 28/100: Training Loss: 0.0015473859325336042
Epoch 29/100: Training Loss: 0.0013958161613743776
Epoch 30/100: Training Loss: 0.0010157384120734634
Epoch 31/100: Training Loss: 0.001423812311166411
Epoch 32/100: Training Loss: 0.0014661261040693635
Epoch 33/100: Training Loss: 0.001207536572863342
Epoch 34/100: Training Loss: 0.0012795312009799252
Epoch 35/100: Training Loss: 0.0013699392983867863
Epoch 36/100: Training Loss: 0.0012216011809695298
Epoch 37/100: Training Loss: 0.0011481665501928632
Epoch 38/100: Training Loss: 0.001625150821770832
Epoch 39/100: Training Loss: 0.001715015453897464
Epoch 40/100: Training Loss: 0.0012813595829495959
Epoch 41/100: Training Loss: 0.0010085975288585494
Epoch 42/100: Training Loss: 0.0010576754998249613
Epoch 43/100: Training Loss: 0.002014043604492382
Epoch 44/100: Training Loss: 0.0014416396997536823
Epoch 45/100: Training Loss: 0.0013970020849993275
Epoch 46/100: Training Loss: 0.0017001940186615963
Epoch 47/100: Training Loss: 0.0014935460439912832
Epoch 48/100: Training Loss: 0.001621892307973971
Epoch 49/100: Training Loss: 0.0008390761294941993
Epoch 50/100: Training Loss: 0.001260034123044105
Epoch 51/100: Training Loss: 0.0014555757022967004
Epoch 52/100: Training Loss: 0.0014004196710647291
Epoch 53/100: Training Loss: 0.0008025957140952918
Epoch 54/100: Training Loss: 0.0012857407141642966
Epoch 55/100: Training Loss: 0.0011335937840164087
Epoch 56/100: Training Loss: 0.00154598797582517
Epoch 57/100: Training Loss: 0.001576785828657211
Epoch 58/100: Training Loss: 0.0016802101378228255
Epoch 59/100: Training Loss: 0.0018117883402830476
Epoch 60/100: Training Loss: 0.0013676340792589128
