2024-04-24 02:18:18.607236: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-04-24 02:18:19.694744: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
================layer -1================
Epoch 0/10: Training Loss: 0.004961075899484274
Epoch 1/10: Training Loss: 0.0038583799675628023
Epoch 2/10: Training Loss: 0.0045267576937908895
Epoch 3/10: Training Loss: 0.004900423796860488
Epoch 4/10: Training Loss: 0.00518526147295545
Epoch 5/10: Training Loss: 0.004117872748341593
Epoch 6/10: Training Loss: 0.004292438913892199
Epoch 7/10: Training Loss: 0.002225256049549663
Epoch 8/10: Training Loss: 0.0036609431246777515
Epoch 9/10: Training Loss: 0.0033998439361999084
Epoch 0/10: Training Loss: 0.004574483924812371
Epoch 1/10: Training Loss: 0.005022740864253544
Epoch 2/10: Training Loss: 0.0034160614013671875
Epoch 3/10: Training Loss: 0.004683085254855923
Epoch 4/10: Training Loss: 0.005495881284033502
Epoch 5/10: Training Loss: 0.004225012305733207
Epoch 6/10: Training Loss: 0.0038542822524384184
Epoch 7/10: Training Loss: 0.004770991685507181
Epoch 8/10: Training Loss: 0.003216470246548419
Epoch 9/10: Training Loss: 0.004140099862238744
Epoch 0/10: Training Loss: 0.005253615079226194
Epoch 1/10: Training Loss: 0.005166932419463471
Epoch 2/10: Training Loss: 0.004901984771648487
Epoch 3/10: Training Loss: 0.004606427012623607
Epoch 4/10: Training Loss: 0.005332715861447208
Epoch 5/10: Training Loss: 0.004074908636666678
Epoch 6/10: Training Loss: 0.0038284225897355514
Epoch 7/10: Training Loss: 0.00418418770903474
Epoch 8/10: Training Loss: 0.004011771062037328
Epoch 9/10: Training Loss: 0.003352409059351141
Epoch 0/10: Training Loss: 0.003986877897765739
Epoch 1/10: Training Loss: 0.003365533117867686
Epoch 2/10: Training Loss: 0.003364263868039371
Epoch 3/10: Training Loss: 0.002908777057027524
Epoch 4/10: Training Loss: 0.0024771469136688606
Epoch 5/10: Training Loss: 0.0025104787086416606
Epoch 6/10: Training Loss: 0.0020837619260776264
Epoch 7/10: Training Loss: 0.0019425391419533571
Epoch 8/10: Training Loss: 0.0019308063515856221
Epoch 9/10: Training Loss: 0.0018116451479905954
Epoch 0/10: Training Loss: 0.004143679434536425
Epoch 1/10: Training Loss: 0.004184628191169786
Epoch 2/10: Training Loss: 0.0032798721015087668
Epoch 3/10: Training Loss: 0.003008267082319669
Epoch 4/10: Training Loss: 0.0027961255582563717
Epoch 5/10: Training Loss: 0.0023120568573840555
Epoch 6/10: Training Loss: 0.001822399032627878
Epoch 7/10: Training Loss: 0.0017151057354511659
Epoch 8/10: Training Loss: 0.0017425745177122713
Epoch 9/10: Training Loss: 0.00269074948287449
Epoch 0/10: Training Loss: 0.004029150389454847
Epoch 1/10: Training Loss: 0.0033440553337518424
Epoch 2/10: Training Loss: 0.00415655111242657
Epoch 3/10: Training Loss: 0.0031778757557547168
Epoch 4/10: Training Loss: 0.0018017611986288996
Epoch 5/10: Training Loss: 0.0019150050871211328
Epoch 6/10: Training Loss: 0.0022535633090083585
Epoch 7/10: Training Loss: 0.002630142894990605
Epoch 8/10: Training Loss: 0.002100884000216525
Epoch 9/10: Training Loss: 0.002657871487681851
Epoch 0/10: Training Loss: 0.0034636877477169036
Epoch 1/10: Training Loss: 0.004033973813056946
Epoch 2/10: Training Loss: 0.004293119907379151
Epoch 3/10: Training Loss: 0.0036350924521684645
Epoch 4/10: Training Loss: 0.003262181952595711
Epoch 5/10: Training Loss: 0.00356704518198967
Epoch 6/10: Training Loss: 0.0025965478271245956
Epoch 7/10: Training Loss: 0.0033461399376392364
Epoch 8/10: Training Loss: 0.0036238402128219603
Epoch 9/10: Training Loss: 0.0032636165618896485
Epoch 0/10: Training Loss: 0.003370494768023491
Epoch 1/10: Training Loss: 0.00323953777551651
Epoch 2/10: Training Loss: 0.003661441057920456
Epoch 3/10: Training Loss: 0.0038340091705322266
Epoch 4/10: Training Loss: 0.0034018054604530333
Epoch 5/10: Training Loss: 0.003738216683268547
Epoch 6/10: Training Loss: 0.0023972101509571074
Epoch 7/10: Training Loss: 0.003649573028087616
Epoch 8/10: Training Loss: 0.003995419293642044
Epoch 9/10: Training Loss: 0.002646390348672867
Epoch 0/10: Training Loss: 0.0030814755707979204
Epoch 1/10: Training Loss: 0.003721604123711586
Epoch 2/10: Training Loss: 0.0029598582535982133
Epoch 3/10: Training Loss: 0.004091755300760269
Epoch 4/10: Training Loss: 0.004011236131191254
Epoch 5/10: Training Loss: 0.003196512907743454
Epoch 6/10: Training Loss: 0.0035316582769155504
Epoch 7/10: Training Loss: 0.002654707245528698
Epoch 8/10: Training Loss: 0.002814839780330658
Epoch 9/10: Training Loss: 0.0017090387642383575
Epoch 0/10: Training Loss: 0.004583945699558137
Epoch 1/10: Training Loss: 0.0038115097458954828
Epoch 2/10: Training Loss: 0.003971365606708891
Epoch 3/10: Training Loss: 0.004174944321820691
Epoch 4/10: Training Loss: 0.003534525063387148
Epoch 5/10: Training Loss: 0.0030208989313453625
Epoch 6/10: Training Loss: 0.0037230913806113467
Epoch 7/10: Training Loss: 0.003337846819762212
Epoch 8/10: Training Loss: 0.0033451455413915547
Epoch 9/10: Training Loss: 0.002984168423209221
Epoch 0/10: Training Loss: 0.0038781234413195567
Epoch 1/10: Training Loss: 0.0035042888039995912
Epoch 2/10: Training Loss: 0.0024682215064953847
Epoch 3/10: Training Loss: 0.002770081447188262
Epoch 4/10: Training Loss: 0.0038289477111427645
Epoch 5/10: Training Loss: 0.003999711221950069
Epoch 6/10: Training Loss: 0.004149772938649366
Epoch 7/10: Training Loss: 0.004452336745656979
Epoch 8/10: Training Loss: 0.0036642065473422884
Epoch 9/10: Training Loss: 0.0023080173191750886
Epoch 0/10: Training Loss: 0.004389448150707658
Epoch 1/10: Training Loss: 0.0034632101939741974
Epoch 2/10: Training Loss: 0.0031257677989400875
Epoch 3/10: Training Loss: 0.0019809045609395217
Epoch 4/10: Training Loss: 0.0028880648552232486
Epoch 5/10: Training Loss: 0.00377765468731048
Epoch 6/10: Training Loss: 0.003781163768403849
Epoch 7/10: Training Loss: 0.0025582440719483005
Epoch 8/10: Training Loss: 0.0014204119990585717
Epoch 9/10: Training Loss: 0.0028618399504643337
Epoch 0/10: Training Loss: 0.004191324805581807
Epoch 1/10: Training Loss: 0.003379769277888418
Epoch 2/10: Training Loss: 0.00356995506791879
Epoch 3/10: Training Loss: 0.004102010205881485
Epoch 4/10: Training Loss: 0.003974675737469402
Epoch 5/10: Training Loss: 0.0038088374579979094
Epoch 6/10: Training Loss: 0.003902004649307554
Epoch 7/10: Training Loss: 0.0034582216218607313
Epoch 8/10: Training Loss: 0.003732063122932485
Epoch 9/10: Training Loss: 0.0034228665939230003
Epoch 0/10: Training Loss: 0.003774743995919133
Epoch 1/10: Training Loss: 0.0036305647022676782
Epoch 2/10: Training Loss: 0.003759069158541446
Epoch 3/10: Training Loss: 0.0034146182584446788
Epoch 4/10: Training Loss: 0.004073897734383084
Epoch 5/10: Training Loss: 0.004099900754082282
Epoch 6/10: Training Loss: 0.0035432790288862015
Epoch 7/10: Training Loss: 0.003203261372269384
Epoch 8/10: Training Loss: 0.003917953825944307
Epoch 9/10: Training Loss: 0.0035839495279931075
Epoch 0/10: Training Loss: 0.0040696188314071555
Epoch 1/10: Training Loss: 0.004015305579103382
Epoch 2/10: Training Loss: 0.0039012104470208782
Epoch 3/10: Training Loss: 0.0038746366437697253
Epoch 4/10: Training Loss: 0.003397281596202724
Epoch 5/10: Training Loss: 0.003404386785646148
Epoch 6/10: Training Loss: 0.003562241200579713
Epoch 7/10: Training Loss: 0.0035237255475378984
Epoch 8/10: Training Loss: 0.003491617985908559
Epoch 9/10: Training Loss: 0.0032307817446474998
Epoch 0/10: Training Loss: 0.002224246018073138
Epoch 1/10: Training Loss: 0.001409352263983558
Epoch 2/10: Training Loss: 0.0016348705572240493
Epoch 3/10: Training Loss: 0.0011601686477661134
Epoch 4/10: Training Loss: 0.0013771527830292198
Epoch 5/10: Training Loss: 0.0008439647800782148
Epoch 6/10: Training Loss: 0.0024232643492081585
Epoch 7/10: Training Loss: 0.0004492816241348491
Epoch 8/10: Training Loss: 0.00041617006063461305
Epoch 9/10: Training Loss: 0.0005048890324199901
Epoch 0/10: Training Loss: 0.002163961880347308
Epoch 1/10: Training Loss: 0.0017555625999675078
Epoch 2/10: Training Loss: 0.0018337547779083252
Epoch 3/10: Training Loss: 0.0018811359125025131
Epoch 4/10: Training Loss: 0.0009478157057481654
Epoch 5/10: Training Loss: 0.0011409803348429062
Epoch 6/10: Training Loss: 0.0009104985524626339
Epoch 7/10: Training Loss: 0.0010203946162672603
Epoch 8/10: Training Loss: 0.00046198911526623893
Epoch 9/10: Training Loss: 0.0009909519377876732
Epoch 0/10: Training Loss: 0.001968338209040025
Epoch 1/10: Training Loss: 0.0015202026156818165
Epoch 2/10: Training Loss: 0.0016769086613374598
Epoch 3/10: Training Loss: 0.0014199314748539645
Epoch 4/10: Training Loss: 0.00180683854748221
Epoch 5/10: Training Loss: 0.0008548598955659306
Epoch 6/10: Training Loss: 0.001213415668291204
Epoch 7/10: Training Loss: 0.001826993332189672
Epoch 8/10: Training Loss: 0.002020489818909589
Epoch 9/10: Training Loss: 0.0013932640061658972
dataset: capitals layer_num_from_end:-1 Avg_acc:tensor(0.7428) Avg_AUC:0.7936023628838073 Avg_threshold:0.45419299602508545
dataset: inventions layer_num_from_end:-1 Avg_acc:tensor(0.7207) Avg_AUC:0.79192298571588 Avg_threshold:0.3805712163448334
dataset: elements layer_num_from_end:-1 Avg_acc:tensor(0.6183) Avg_AUC:0.6955856168343161 Avg_threshold:0.4448348681131999
dataset: animals layer_num_from_end:-1 Avg_acc:tensor(0.6065) Avg_AUC:0.7812447509868145 Avg_threshold:0.5883885820706686
dataset: companies layer_num_from_end:-1 Avg_acc:tensor(0.5692) Avg_AUC:0.7161750000000001 Avg_threshold:0.6882504026095072
dataset: facts layer_num_from_end:-1 Avg_acc:tensor(0.6683) Avg_AUC:0.7373250316829708 Avg_threshold:0.7613506217797598


================layer -5================
Epoch 0/10: Training Loss: 0.00279765029053588
Epoch 1/10: Training Loss: 0.0020957609990259985
Epoch 2/10: Training Loss: 0.003824847561496121
Epoch 3/10: Training Loss: 0.0029396304300615002
Epoch 4/10: Training Loss: 0.0022509596147737305
Epoch 5/10: Training Loss: 0.0024437798069907235
Epoch 6/10: Training Loss: 0.0016853604074958321
Epoch 7/10: Training Loss: 0.002168818370445625
Epoch 8/10: Training Loss: 0.001597185547535236
Epoch 9/10: Training Loss: 0.0028599536085462235
Epoch 0/10: Training Loss: 0.003703655896486936
Epoch 1/10: Training Loss: 0.002383520553162048
Epoch 2/10: Training Loss: 0.0029215773085614185
Epoch 3/10: Training Loss: 0.0016279853098875993
Epoch 4/10: Training Loss: 0.002637922763824463
Epoch 5/10: Training Loss: 0.0022755594520302088
Epoch 6/10: Training Loss: 0.0009832566643094684
Epoch 7/10: Training Loss: 0.00214531413325063
Epoch 8/10: Training Loss: 0.002291548085379434
Epoch 9/10: Training Loss: 0.0021313533082708614
Epoch 0/10: Training Loss: 0.0026315865399954203
Epoch 1/10: Training Loss: 0.0015330160414422308
Epoch 2/10: Training Loss: 0.0017580033599079905
Epoch 3/10: Training Loss: 0.0011035920856715916
Epoch 4/10: Training Loss: 0.0038541547068349133
Epoch 5/10: Training Loss: 0.0018199592620342761
Epoch 6/10: Training Loss: 0.0008368962607183657
Epoch 7/10: Training Loss: 0.0029829138225608773
Epoch 8/10: Training Loss: 0.0030323562922177616
Epoch 9/10: Training Loss: 0.0028140188513935864
Epoch 0/10: Training Loss: 0.0034161959689087665
Epoch 1/10: Training Loss: 0.002311400840618859
Epoch 2/10: Training Loss: 0.0014650677245087419
Epoch 3/10: Training Loss: 0.001427051845503731
Epoch 4/10: Training Loss: 0.0011516173193059816
Epoch 5/10: Training Loss: 0.0015061426382123326
Epoch 6/10: Training Loss: 0.0003195740968171804
Epoch 7/10: Training Loss: 0.00020936236608247815
Epoch 8/10: Training Loss: 0.0004925602601349719
Epoch 9/10: Training Loss: 0.0006351871358836356
Epoch 0/10: Training Loss: 0.003008891102726474
Epoch 1/10: Training Loss: 0.0017965847363501238
Epoch 2/10: Training Loss: 0.0024833478079251715
Epoch 3/10: Training Loss: 0.001904584878792792
Epoch 4/10: Training Loss: 0.001217139187765999
Epoch 5/10: Training Loss: 0.0010331947744989687
Epoch 6/10: Training Loss: 0.00020297808698349935
Epoch 7/10: Training Loss: 0.0002398803784437706
Epoch 8/10: Training Loss: 0.0012991408263247437
Epoch 9/10: Training Loss: 0.0015483789283073752
Epoch 0/10: Training Loss: 0.00354134743930372
Epoch 1/10: Training Loss: 0.0030702996107698217
Epoch 2/10: Training Loss: 0.0020802562222158984
Epoch 3/10: Training Loss: 0.0017686413109668194
Epoch 4/10: Training Loss: 0.0014682705599837508
Epoch 5/10: Training Loss: 0.0010378375375197709
Epoch 6/10: Training Loss: 0.0007459883309580797
Epoch 7/10: Training Loss: 0.0016253736852868204
Epoch 8/10: Training Loss: 0.002121078090433694
Epoch 9/10: Training Loss: 0.003582834466103396
Epoch 0/10: Training Loss: 0.0033161547034978867
Epoch 1/10: Training Loss: 0.0035785593092441557
Epoch 2/10: Training Loss: 0.0029105879366397858
Epoch 3/10: Training Loss: 0.003128613904118538
Epoch 4/10: Training Loss: 0.003300235420465469
Epoch 5/10: Training Loss: 0.003391493111848831
Epoch 6/10: Training Loss: 0.0022472254931926727
Epoch 7/10: Training Loss: 0.002384067140519619
Epoch 8/10: Training Loss: 0.0022656215354800225
Epoch 9/10: Training Loss: 0.0015959134325385094
Epoch 0/10: Training Loss: 0.0037011131644248962
Epoch 1/10: Training Loss: 0.0024532534182071684
Epoch 2/10: Training Loss: 0.003250522166490555
Epoch 3/10: Training Loss: 0.0025536861270666123
Epoch 4/10: Training Loss: 0.002764572203159332
Epoch 5/10: Training Loss: 0.002823907323181629
Epoch 6/10: Training Loss: 0.00166751928627491
Epoch 7/10: Training Loss: 0.0018343880772590637
Epoch 8/10: Training Loss: 0.0020167829468846323
Epoch 9/10: Training Loss: 0.0016223253682255746
Epoch 0/10: Training Loss: 0.00392298586666584
Epoch 1/10: Training Loss: 0.0033940501511096955
Epoch 2/10: Training Loss: 0.0024520620703697205
Epoch 3/10: Training Loss: 0.0034640762954950334
Epoch 4/10: Training Loss: 0.0029609428718686106
Epoch 5/10: Training Loss: 0.002508603036403656
Epoch 6/10: Training Loss: 0.0023215644061565397
Epoch 7/10: Training Loss: 0.0018287155777215957
Epoch 8/10: Training Loss: 0.002154437080025673
Epoch 9/10: Training Loss: 0.0017224708572030067
Epoch 0/10: Training Loss: 0.002516706278369685
Epoch 1/10: Training Loss: 0.0017898618036015018
Epoch 2/10: Training Loss: 0.0015263081925689795
Epoch 3/10: Training Loss: 0.0015457110230330448
Epoch 4/10: Training Loss: 0.0011944580040160258
Epoch 5/10: Training Loss: 0.0005155392702977369
Epoch 6/10: Training Loss: 0.0013047733883948841
Epoch 7/10: Training Loss: 0.0011525244280031531
Epoch 8/10: Training Loss: 0.00038931599468182605
Epoch 9/10: Training Loss: 0.0009572894140413613
Epoch 0/10: Training Loss: 0.001977499313415236
Epoch 1/10: Training Loss: 0.0022187494927910483
Epoch 2/10: Training Loss: 0.0008059873892243501
Epoch 3/10: Training Loss: 0.0005713209604761403
Epoch 4/10: Training Loss: 0.0011715358419782797
Epoch 5/10: Training Loss: 0.0007238858824322938
Epoch 6/10: Training Loss: 0.0021653397447743993
Epoch 7/10: Training Loss: 0.0008548629132046062
Epoch 8/10: Training Loss: 0.0008529063053191847
Epoch 9/10: Training Loss: 0.0009222174905667639
Epoch 0/10: Training Loss: 0.0027693973225393115
Epoch 1/10: Training Loss: 0.000614172429036183
Epoch 2/10: Training Loss: 0.0023501230652924554
Epoch 3/10: Training Loss: 0.002806113404073533
Epoch 4/10: Training Loss: 0.0009886667986584317
Epoch 5/10: Training Loss: 0.0020580375270479044
Epoch 6/10: Training Loss: 0.0022315937242690163
Epoch 7/10: Training Loss: 0.001469065429298741
Epoch 8/10: Training Loss: 0.0005250141309325102
Epoch 9/10: Training Loss: 0.0011192641820117926
Epoch 0/10: Training Loss: 0.0035704997991094526
Epoch 1/10: Training Loss: 0.0035370114623316075
Epoch 2/10: Training Loss: 0.004903376497180256
Epoch 3/10: Training Loss: 0.0032123885802085826
Epoch 4/10: Training Loss: 0.004418510869638809
Epoch 5/10: Training Loss: 0.003966228061953917
Epoch 6/10: Training Loss: 0.003447805019403925
Epoch 7/10: Training Loss: 0.0038943156501315287
Epoch 8/10: Training Loss: 0.0045056939125061035
Epoch 9/10: Training Loss: 0.004345347944474378
Epoch 0/10: Training Loss: 0.003681702329622989
Epoch 1/10: Training Loss: 0.004309441080156541
Epoch 2/10: Training Loss: 0.0025844068716693396
Epoch 3/10: Training Loss: 0.00489744840078796
Epoch 4/10: Training Loss: 0.004364811426756398
Epoch 5/10: Training Loss: 0.00382880817185964
Epoch 6/10: Training Loss: 0.004076385734886523
Epoch 7/10: Training Loss: 0.004293521508475803
Epoch 8/10: Training Loss: 0.003902863587764715
Epoch 9/10: Training Loss: 0.004204240066326217
Epoch 0/10: Training Loss: 0.004051553097781756
Epoch 1/10: Training Loss: 0.003851679776678022
Epoch 2/10: Training Loss: 0.0020679265063330036
Epoch 3/10: Training Loss: 0.0045546040629709005
Epoch 4/10: Training Loss: 0.004768959733824067
Epoch 5/10: Training Loss: 0.004414933406754046
Epoch 6/10: Training Loss: 0.003858109578391574
Epoch 7/10: Training Loss: 0.0038538164650367586
Epoch 8/10: Training Loss: 0.0037904567276405184
Epoch 9/10: Training Loss: 0.0034959166255218305
Epoch 0/10: Training Loss: 0.0017389141461428474
Epoch 1/10: Training Loss: 0.0007827563320889192
Epoch 2/10: Training Loss: 0.0016505902304368861
Epoch 3/10: Training Loss: 0.001544895417550031
Epoch 4/10: Training Loss: 0.000625345330027973
Epoch 5/10: Training Loss: 0.00266723773058723
Epoch 6/10: Training Loss: 0.0019397349918589873
Epoch 7/10: Training Loss: 0.002282382460201488
Epoch 8/10: Training Loss: 0.0010292386307435878
Epoch 9/10: Training Loss: 0.0026794985813253067
Epoch 0/10: Training Loss: 0.0017741879996131448
Epoch 1/10: Training Loss: 0.001418986390618717
Epoch 2/10: Training Loss: 0.0012340375605751487
Epoch 3/10: Training Loss: 0.0008220494670026443
Epoch 4/10: Training Loss: 0.0017567964161143584
Epoch 5/10: Training Loss: 0.0007980735862956328
Epoch 6/10: Training Loss: 0.0006870835581246545
Epoch 7/10: Training Loss: 0.001966191389981438
Epoch 8/10: Training Loss: 0.0009947471758898568
Epoch 9/10: Training Loss: 0.0019285464988035314
Epoch 0/10: Training Loss: 0.0018416208379408892
Epoch 1/10: Training Loss: 0.0010514737928614896
Epoch 2/10: Training Loss: 0.001121206143323113
Epoch 3/10: Training Loss: 0.002968997815076043
Epoch 4/10: Training Loss: 0.002357567233197829
Epoch 5/10: Training Loss: 0.001603864221011891
Epoch 6/10: Training Loss: 0.0019171707770403693
Epoch 7/10: Training Loss: 0.0014589861035346984
Epoch 8/10: Training Loss: 0.001877866597736583
Epoch 9/10: Training Loss: 0.0016247673946268419
dataset: capitals layer_num_from_end:-5 Avg_acc:tensor(0.6795) Avg_AUC:0.8243696540788786 Avg_threshold:0.22431936860084534
dataset: inventions layer_num_from_end:-5 Avg_acc:tensor(0.7599) Avg_AUC:0.902726048990068 Avg_threshold:0.3528071939945221
dataset: elements layer_num_from_end:-5 Avg_acc:tensor(0.6330) Avg_AUC:0.6884587813620072 Avg_threshold:0.333438903093338
dataset: animals layer_num_from_end:-5 Avg_acc:tensor(0.6353) Avg_AUC:0.7819907669858067 Avg_threshold:0.3797268370787303
dataset: companies layer_num_from_end:-5 Avg_acc:tensor(0.6658) Avg_AUC:0.8593416666666668 Avg_threshold:0.3886972467104594
dataset: facts layer_num_from_end:-5 Avg_acc:tensor(0.7544) Avg_AUC:0.8348249248864397 Avg_threshold:0.561811625957489


================layer -9================
Epoch 0/10: Training Loss: 0.0037654971742963456
Epoch 1/10: Training Loss: 0.0025390549139543014
Epoch 2/10: Training Loss: 0.002107783214195625
Epoch 3/10: Training Loss: 0.00196788798679005
Epoch 4/10: Training Loss: 0.001727694502243629
Epoch 5/10: Training Loss: 0.0021428153231427384
Epoch 6/10: Training Loss: 0.0028192150842893376
Epoch 7/10: Training Loss: 0.0028252630800633997
Epoch 8/10: Training Loss: 0.0025802319283251994
Epoch 9/10: Training Loss: 0.0021178320154443485
Epoch 0/10: Training Loss: 0.00361870719002677
Epoch 1/10: Training Loss: 0.0027927418688794116
Epoch 2/10: Training Loss: 0.0032833791696108305
Epoch 3/10: Training Loss: 0.002456326793123792
Epoch 4/10: Training Loss: 0.003628138062003609
Epoch 5/10: Training Loss: 0.0023289418720698857
Epoch 6/10: Training Loss: 0.0025492664817329887
Epoch 7/10: Training Loss: 0.0024478681437619084
Epoch 8/10: Training Loss: 0.0026213953961859216
Epoch 9/10: Training Loss: 0.0018086016594946802
Epoch 0/10: Training Loss: 0.003524961171450315
Epoch 1/10: Training Loss: 0.0024336849476074007
Epoch 2/10: Training Loss: 0.0026279185618553963
Epoch 3/10: Training Loss: 0.003313914462402984
Epoch 4/10: Training Loss: 0.0020423552373072485
Epoch 5/10: Training Loss: 0.0035706195797953573
Epoch 6/10: Training Loss: 0.0014813629897324355
Epoch 7/10: Training Loss: 0.002696683982035497
Epoch 8/10: Training Loss: 0.0017567875085177122
Epoch 9/10: Training Loss: 0.0021409411113578955
Epoch 0/10: Training Loss: 0.002752552368889557
Epoch 1/10: Training Loss: 0.001897856136041185
Epoch 2/10: Training Loss: 0.0015527079441796051
Epoch 3/10: Training Loss: 0.000725638738439127
Epoch 4/10: Training Loss: 0.0005950342658107267
Epoch 5/10: Training Loss: 0.0005003427800956679
Epoch 6/10: Training Loss: 8.266360893205631e-05
Epoch 7/10: Training Loss: 0.0002419350931615186
Epoch 8/10: Training Loss: 0.0002708538589667689
Epoch 9/10: Training Loss: 0.0013909538282207185
Epoch 0/10: Training Loss: 0.003405487244845899
Epoch 1/10: Training Loss: 0.0024237076929010497
Epoch 2/10: Training Loss: 0.0012540779779293787
Epoch 3/10: Training Loss: 0.002000419457265936
Epoch 4/10: Training Loss: 0.0007033763121973518
Epoch 5/10: Training Loss: 0.0002720270968653673
Epoch 6/10: Training Loss: 0.0007973459600670937
Epoch 7/10: Training Loss: 0.00031621603329488835
Epoch 8/10: Training Loss: 0.0003792256619666983
Epoch 9/10: Training Loss: 0.0009928868592151104
Epoch 0/10: Training Loss: 0.002913284521161413
Epoch 1/10: Training Loss: 0.002346618409537099
Epoch 2/10: Training Loss: 0.001049562008834324
Epoch 3/10: Training Loss: 0.00036358869880255014
Epoch 4/10: Training Loss: 0.001586619330330129
Epoch 5/10: Training Loss: 0.0009404097232350542
Epoch 6/10: Training Loss: 0.00044027345677826303
Epoch 7/10: Training Loss: 0.0002143618884993477
Epoch 8/10: Training Loss: 0.0003530174219535172
Epoch 9/10: Training Loss: 0.0006961721492691274
Epoch 0/10: Training Loss: 0.002904854714870453
Epoch 1/10: Training Loss: 0.0015996413305401802
Epoch 2/10: Training Loss: 0.002204487845301628
Epoch 3/10: Training Loss: 0.0011551924981176853
Epoch 4/10: Training Loss: 0.0022358745336532593
Epoch 5/10: Training Loss: 0.002412429079413414
Epoch 6/10: Training Loss: 0.0019413486123085022
Epoch 7/10: Training Loss: 0.0013290675356984138
Epoch 8/10: Training Loss: 0.0008669527247548104
Epoch 9/10: Training Loss: 0.0018469654023647308
Epoch 0/10: Training Loss: 0.002869446575641632
Epoch 1/10: Training Loss: 0.0019343240186572076
Epoch 2/10: Training Loss: 0.002513224817812443
Epoch 3/10: Training Loss: 0.00275038443505764
Epoch 4/10: Training Loss: 0.002572431415319443
Epoch 5/10: Training Loss: 0.002306162565946579
Epoch 6/10: Training Loss: 0.002121162414550781
Epoch 7/10: Training Loss: 0.0014213216491043568
Epoch 8/10: Training Loss: 0.0013065239414572715
Epoch 9/10: Training Loss: 0.0019284015521407127
Epoch 0/10: Training Loss: 0.0027031991630792618
Epoch 1/10: Training Loss: 0.002259926125407219
Epoch 2/10: Training Loss: 0.002183108776807785
Epoch 3/10: Training Loss: 0.0020177558064460753
Epoch 4/10: Training Loss: 0.0026626216247677805
Epoch 5/10: Training Loss: 0.0009696235880255699
Epoch 6/10: Training Loss: 0.0019576286897063254
Epoch 7/10: Training Loss: 0.0006818966008722782
Epoch 8/10: Training Loss: 0.0021805521100759506
Epoch 9/10: Training Loss: 0.0010064429603517055
Epoch 0/10: Training Loss: 0.002271215816971603
Epoch 1/10: Training Loss: 0.0013261971769818835
Epoch 2/10: Training Loss: 0.0009765355450332545
Epoch 3/10: Training Loss: 0.0010576052650524553
Epoch 4/10: Training Loss: 0.0012859125046213722
Epoch 5/10: Training Loss: 0.0008738737577086041
Epoch 6/10: Training Loss: 0.0009171160732864575
Epoch 7/10: Training Loss: 0.0013932798318802171
Epoch 8/10: Training Loss: 0.0013132984660992957
Epoch 9/10: Training Loss: 0.001080283503623525
Epoch 0/10: Training Loss: 0.002743826929930669
Epoch 1/10: Training Loss: 0.0022734452964393954
Epoch 2/10: Training Loss: 0.0013846929665583714
Epoch 3/10: Training Loss: 0.002512976622125905
Epoch 4/10: Training Loss: 0.0015344549516204057
Epoch 5/10: Training Loss: 0.0009577895046039752
Epoch 6/10: Training Loss: 0.001473483765960499
Epoch 7/10: Training Loss: 0.0014711857601335853
Epoch 8/10: Training Loss: 0.0017234356540023902
Epoch 9/10: Training Loss: 0.0014044839865083147
Epoch 0/10: Training Loss: 0.0027065671932925083
Epoch 1/10: Training Loss: 0.0015564411878585815
Epoch 2/10: Training Loss: 0.00094455670399271
Epoch 3/10: Training Loss: 0.0013616655472737209
Epoch 4/10: Training Loss: 0.0023652098740741707
Epoch 5/10: Training Loss: 0.001748546863057811
Epoch 6/10: Training Loss: 0.0008648539045054441
Epoch 7/10: Training Loss: 0.0013277431961837088
Epoch 8/10: Training Loss: 0.0004111482838916171
Epoch 9/10: Training Loss: 0.0007674545049667358
Epoch 0/10: Training Loss: 0.003892097252094193
Epoch 1/10: Training Loss: 0.002479985652380432
Epoch 2/10: Training Loss: 0.002805360895118966
Epoch 3/10: Training Loss: 0.003112057384276232
Epoch 4/10: Training Loss: 0.002959919488982649
Epoch 5/10: Training Loss: 0.0038317145890747473
Epoch 6/10: Training Loss: 0.003357494508983284
Epoch 7/10: Training Loss: 0.0033346256672941296
Epoch 8/10: Training Loss: 0.0032055265461372225
Epoch 9/10: Training Loss: 0.003800591096183322
Epoch 0/10: Training Loss: 0.003972588390704022
Epoch 1/10: Training Loss: 0.0026752802709869993
Epoch 2/10: Training Loss: 0.002745470069102104
Epoch 3/10: Training Loss: 0.0037427066967187336
Epoch 4/10: Training Loss: 0.00410868829449281
Epoch 5/10: Training Loss: 0.0039030546384141935
Epoch 6/10: Training Loss: 0.0039688234297645016
Epoch 7/10: Training Loss: 0.0036353907048307507
Epoch 8/10: Training Loss: 0.0029024357432561204
Epoch 9/10: Training Loss: 0.0033527257426685057
Epoch 0/10: Training Loss: 0.0038227659977035015
Epoch 1/10: Training Loss: 0.003654151562823365
Epoch 2/10: Training Loss: 0.0038596320625962012
Epoch 3/10: Training Loss: 0.004109030527784335
Epoch 4/10: Training Loss: 0.0034829662335629495
Epoch 5/10: Training Loss: 0.002855410836390312
Epoch 6/10: Training Loss: 0.0027261640851860805
Epoch 7/10: Training Loss: 0.0033332602867227517
Epoch 8/10: Training Loss: 0.0031114102199377605
Epoch 9/10: Training Loss: 0.003059116815099653
Epoch 0/10: Training Loss: 0.0002891134689835941
Epoch 1/10: Training Loss: 0.0005986310979899238
Epoch 2/10: Training Loss: 0.0008646964150316575
Epoch 3/10: Training Loss: 0.0010615270803956424
Epoch 4/10: Training Loss: 0.0003347887712366441
Epoch 5/10: Training Loss: 0.002572729832985822
Epoch 6/10: Training Loss: 0.001485723607680377
Epoch 7/10: Training Loss: 0.0010937560130568113
Epoch 8/10: Training Loss: 0.0019741044324987074
Epoch 9/10: Training Loss: 0.0006989947574980119
Epoch 0/10: Training Loss: 0.000508436472976909
Epoch 1/10: Training Loss: 0.0002461669857011122
Epoch 2/10: Training Loss: 0.00024642676991574905
Epoch 3/10: Training Loss: 0.0014612367047983057
Epoch 4/10: Training Loss: 0.0009673148393630982
Epoch 5/10: Training Loss: 0.0008548549869481255
Epoch 6/10: Training Loss: 0.000611604268060011
Epoch 7/10: Training Loss: 0.00031288515995530524
Epoch 8/10: Training Loss: 0.0003614635590244742
Epoch 9/10: Training Loss: 0.0021241573726429657
Epoch 0/10: Training Loss: 0.0002266246168052449
Epoch 1/10: Training Loss: 0.0008450417834169724
Epoch 2/10: Training Loss: 0.0016516711782006656
Epoch 3/10: Training Loss: 0.000693382673403796
Epoch 4/10: Training Loss: 0.0008556000450078179
Epoch 5/10: Training Loss: 0.0008504232939551858
Epoch 6/10: Training Loss: 0.0021773592514150284
Epoch 7/10: Training Loss: 0.0018700736410477582
Epoch 8/10: Training Loss: 0.00040700724896262675
Epoch 9/10: Training Loss: 0.0014489684034796322
dataset: capitals layer_num_from_end:-9 Avg_acc:tensor(0.8025) Avg_AUC:0.8893489587743514 Avg_threshold:0.20033545792102814
dataset: inventions layer_num_from_end:-9 Avg_acc:tensor(0.7713) Avg_AUC:0.8852579929695348 Avg_threshold:0.375437597433726
dataset: elements layer_num_from_end:-9 Avg_acc:tensor(0.6161) Avg_AUC:0.6984391259105099 Avg_threshold:0.27802100280920666
dataset: animals layer_num_from_end:-9 Avg_acc:tensor(0.7007) Avg_AUC:0.7986950953220795 Avg_threshold:0.13132128367821375
dataset: companies layer_num_from_end:-9 Avg_acc:tensor(0.7414) Avg_AUC:0.9051731481481481 Avg_threshold:0.3667256434758504
dataset: facts layer_num_from_end:-9 Avg_acc:tensor(0.7533) Avg_AUC:0.8414231705754197 Avg_threshold:0.7660291790962219


================layer -13================
Epoch 0/10: Training Loss: 0.0032293448498199036
Epoch 1/10: Training Loss: 0.0012491178679299522
Epoch 2/10: Training Loss: 0.003185730922472227
Epoch 3/10: Training Loss: 0.0037496006572163188
Epoch 4/10: Training Loss: 0.002896396847038002
Epoch 5/10: Training Loss: 0.002260236473350258
Epoch 6/10: Training Loss: 0.0018682711191110677
Epoch 7/10: Training Loss: 0.0021272697648802
Epoch 8/10: Training Loss: 0.0013999272059727382
Epoch 9/10: Training Loss: 0.0018870809695103786
Epoch 0/10: Training Loss: 0.0025827284459467533
Epoch 1/10: Training Loss: 0.0018026032647886477
Epoch 2/10: Training Loss: 0.0034509386632825944
Epoch 3/10: Training Loss: 0.0024343906165836576
Epoch 4/10: Training Loss: 0.002277647907083685
Epoch 5/10: Training Loss: 0.0010491476192340985
Epoch 6/10: Training Loss: 0.001594104237489767
Epoch 7/10: Training Loss: 0.001712650894285082
Epoch 8/10: Training Loss: 0.002140701025516003
Epoch 9/10: Training Loss: 0.0015029771761460738
Epoch 0/10: Training Loss: 0.0024634937306384106
Epoch 1/10: Training Loss: 0.0024374410822674943
Epoch 2/10: Training Loss: 0.0029229357109203206
Epoch 3/10: Training Loss: 0.0021093527337054272
Epoch 4/10: Training Loss: 0.0025323812778179464
Epoch 5/10: Training Loss: 0.0030018249591747363
Epoch 6/10: Training Loss: 0.002125680029809058
Epoch 7/10: Training Loss: 0.002684407717698104
Epoch 8/10: Training Loss: 0.002648861883403538
Epoch 9/10: Training Loss: 0.0016046291464692228
Epoch 0/10: Training Loss: 0.0033065226911767128
Epoch 1/10: Training Loss: 0.001894910642705812
Epoch 2/10: Training Loss: 0.0011882588175908188
Epoch 3/10: Training Loss: 0.0015095438511093702
Epoch 4/10: Training Loss: 0.0014043219981749364
Epoch 5/10: Training Loss: 0.0007797748398927092
Epoch 6/10: Training Loss: 0.0012275370900616323
Epoch 7/10: Training Loss: 0.002431464341520532
Epoch 8/10: Training Loss: 0.00233056885333149
Epoch 9/10: Training Loss: 0.0015519848264799528
Epoch 0/10: Training Loss: 0.0030477035996372714
Epoch 1/10: Training Loss: 0.0021788050060623262
Epoch 2/10: Training Loss: 0.0022816113167745205
Epoch 3/10: Training Loss: 0.0011580411824711992
Epoch 4/10: Training Loss: 0.0009512774234900445
Epoch 5/10: Training Loss: 0.0007432116412677648
Epoch 6/10: Training Loss: 0.0006246174588525222
Epoch 7/10: Training Loss: 0.001616830657596237
Epoch 8/10: Training Loss: 0.0013657349384635503
Epoch 9/10: Training Loss: 0.0027508139610290527
Epoch 0/10: Training Loss: 0.0032934093036534598
Epoch 1/10: Training Loss: 0.002122545169175037
Epoch 2/10: Training Loss: 0.0010611379256277728
Epoch 3/10: Training Loss: 0.0013718091271406301
Epoch 4/10: Training Loss: 0.0013553845736146704
Epoch 5/10: Training Loss: 0.0017376582315363036
Epoch 6/10: Training Loss: 0.0012764150013952898
Epoch 7/10: Training Loss: 0.0018839916568592283
Epoch 8/10: Training Loss: 0.0018068881122612515
Epoch 9/10: Training Loss: 0.0016247535044430223
Epoch 0/10: Training Loss: 0.004193101078271866
Epoch 1/10: Training Loss: 0.0030429720878601073
Epoch 2/10: Training Loss: 0.0031961068511009217
Epoch 3/10: Training Loss: 0.002927059307694435
Epoch 4/10: Training Loss: 0.0025753237307071684
Epoch 5/10: Training Loss: 0.0025067538022994994
Epoch 6/10: Training Loss: 0.0025567546486854552
Epoch 7/10: Training Loss: 0.0018907610327005386
Epoch 8/10: Training Loss: 0.001462799683213234
Epoch 9/10: Training Loss: 0.0017291374504566192
Epoch 0/10: Training Loss: 0.0038295768201351164
Epoch 1/10: Training Loss: 0.0034192558377981186
Epoch 2/10: Training Loss: 0.0033325999975204468
Epoch 3/10: Training Loss: 0.003024575486779213
Epoch 4/10: Training Loss: 0.0033404212445020676
Epoch 5/10: Training Loss: 0.002538955956697464
Epoch 6/10: Training Loss: 0.002558276616036892
Epoch 7/10: Training Loss: 0.002039511315524578
Epoch 8/10: Training Loss: 0.0022429954260587692
Epoch 9/10: Training Loss: 0.0016597464680671691
Epoch 0/10: Training Loss: 0.00387374684214592
Epoch 1/10: Training Loss: 0.0028752079233527184
Epoch 2/10: Training Loss: 0.003276664763689041
Epoch 3/10: Training Loss: 0.00326494500041008
Epoch 4/10: Training Loss: 0.003034144639968872
Epoch 5/10: Training Loss: 0.0024498365819454193
Epoch 6/10: Training Loss: 0.002120616100728512
Epoch 7/10: Training Loss: 0.0020031342282891273
Epoch 8/10: Training Loss: 0.0016055108979344368
Epoch 9/10: Training Loss: 0.0014477544464170933
Epoch 0/10: Training Loss: 0.003037468263298083
Epoch 1/10: Training Loss: 0.002180666300901182
Epoch 2/10: Training Loss: 0.00176757536116679
Epoch 3/10: Training Loss: 0.002053870896624911
Epoch 4/10: Training Loss: 0.0024526119232177734
Epoch 5/10: Training Loss: 0.00234371594562652
Epoch 6/10: Training Loss: 0.0013984257628203956
Epoch 7/10: Training Loss: 0.0015678646837829783
Epoch 8/10: Training Loss: 0.0015179136186648325
Epoch 9/10: Training Loss: 0.0017155152597245137
Epoch 0/10: Training Loss: 0.0029884785603565777
Epoch 1/10: Training Loss: 0.002190496511520094
Epoch 2/10: Training Loss: 0.0031790539717218677
Epoch 3/10: Training Loss: 0.00215000825323117
Epoch 4/10: Training Loss: 0.003009775831441211
Epoch 5/10: Training Loss: 0.0016801323101019404
Epoch 6/10: Training Loss: 0.0015107797589271692
Epoch 7/10: Training Loss: 0.002005546146137699
Epoch 8/10: Training Loss: 0.001642489129570639
Epoch 9/10: Training Loss: 0.0018517538240760754
Epoch 0/10: Training Loss: 0.0024223297264925232
Epoch 1/10: Training Loss: 0.0020779329500380593
Epoch 2/10: Training Loss: 0.0013992184666311665
Epoch 3/10: Training Loss: 0.001994477335814458
Epoch 4/10: Training Loss: 0.0028588247906630206
Epoch 5/10: Training Loss: 0.0021451121302926614
Epoch 6/10: Training Loss: 0.0010071926435847192
Epoch 7/10: Training Loss: 0.0012421891757637073
Epoch 8/10: Training Loss: 0.001321153655932967
Epoch 9/10: Training Loss: 0.0014756752806863967
Epoch 0/10: Training Loss: 0.002888076352757334
Epoch 1/10: Training Loss: 0.0019269838238394025
Epoch 2/10: Training Loss: 0.003364060888227248
Epoch 3/10: Training Loss: 0.0030549210033669377
Epoch 4/10: Training Loss: 0.0029521493722271448
Epoch 5/10: Training Loss: 0.003239271853933271
Epoch 6/10: Training Loss: 0.0033327143713338487
Epoch 7/10: Training Loss: 0.002699180154610943
Epoch 8/10: Training Loss: 0.00273149652986337
Epoch 9/10: Training Loss: 0.0025087188411232653
Epoch 0/10: Training Loss: 0.002784689530631564
Epoch 1/10: Training Loss: 0.001847292808507452
Epoch 2/10: Training Loss: 0.0023704427757010554
Epoch 3/10: Training Loss: 0.003553801024986419
Epoch 4/10: Training Loss: 0.002925756751306799
Epoch 5/10: Training Loss: 0.003241252030757879
Epoch 6/10: Training Loss: 0.0028955016704584587
Epoch 7/10: Training Loss: 0.0031701890838067262
Epoch 8/10: Training Loss: 0.0032281891399661435
Epoch 9/10: Training Loss: 0.002771225196636276
Epoch 0/10: Training Loss: 0.002201201110486163
Epoch 1/10: Training Loss: 0.002199976649505413
Epoch 2/10: Training Loss: 0.0028476363775745924
Epoch 3/10: Training Loss: 0.0033314599106643375
Epoch 4/10: Training Loss: 0.003551050132473573
Epoch 5/10: Training Loss: 0.0036804502373499587
Epoch 6/10: Training Loss: 0.0032333305339939545
Epoch 7/10: Training Loss: 0.0027741655608676126
Epoch 8/10: Training Loss: 0.0021026452250828016
Epoch 9/10: Training Loss: 0.0022054375796918047
Epoch 0/10: Training Loss: 0.0009443374241099638
Epoch 1/10: Training Loss: 0.0004537937833982355
Epoch 2/10: Training Loss: 0.0009380600031684427
Epoch 3/10: Training Loss: 0.000662496203885359
Epoch 4/10: Training Loss: 0.0013000493540483362
Epoch 5/10: Training Loss: 0.0007203626282074872
Epoch 6/10: Training Loss: 0.0015529345063602224
Epoch 7/10: Training Loss: 0.00022607813863193286
Epoch 8/10: Training Loss: 0.00022736630019019633
Epoch 9/10: Training Loss: 0.000641497180742376
Epoch 0/10: Training Loss: 0.0010949360973694746
Epoch 1/10: Training Loss: 0.0012386161614866818
Epoch 2/10: Training Loss: 0.0007037058472633362
Epoch 3/10: Training Loss: 0.0006307947723304524
Epoch 4/10: Training Loss: 0.0006107427618082832
Epoch 5/10: Training Loss: 0.0006681723629727083
Epoch 6/10: Training Loss: 0.0005090174429556903
Epoch 7/10: Training Loss: 0.00031113617998712205
Epoch 8/10: Training Loss: 0.0003090682713424458
Epoch 9/10: Training Loss: 0.0019618721569285673
Epoch 0/10: Training Loss: 0.0010777026414871215
Epoch 1/10: Training Loss: 0.0017439176054561839
Epoch 2/10: Training Loss: 0.0008178674999405356
Epoch 3/10: Training Loss: 0.0006981581887778114
Epoch 4/10: Training Loss: 0.0013367048081229715
Epoch 5/10: Training Loss: 0.0005242897307171542
Epoch 6/10: Training Loss: 0.00040341865490464604
Epoch 7/10: Training Loss: 0.0013064523830133327
Epoch 8/10: Training Loss: 0.0007603954742936527
Epoch 9/10: Training Loss: 0.001613093474332024
dataset: capitals layer_num_from_end:-13 Avg_acc:tensor(0.8292) Avg_AUC:0.9681175019114696 Avg_threshold:0.39641860127449036
dataset: inventions layer_num_from_end:-13 Avg_acc:tensor(0.8212) Avg_AUC:0.927963885169066 Avg_threshold:0.23057190204660097
dataset: elements layer_num_from_end:-13 Avg_acc:tensor(0.6681) Avg_AUC:0.7533364165414115 Avg_threshold:0.3845781236886978
dataset: animals layer_num_from_end:-13 Avg_acc:tensor(0.7199) Avg_AUC:0.8123556521373981 Avg_threshold:0.20412200192610422
dataset: companies layer_num_from_end:-13 Avg_acc:tensor(0.6908) Avg_AUC:0.9128101851851852 Avg_threshold:0.27828409771124524
dataset: facts layer_num_from_end:-13 Avg_acc:tensor(0.7761) Avg_AUC:0.8722749085109717 Avg_threshold:0.647385835647583


================layer -17================
Epoch 0/10: Training Loss: 0.004142384429078002
Epoch 1/10: Training Loss: 0.0027402531016956677
Epoch 2/10: Training Loss: 0.002113862054331319
Epoch 3/10: Training Loss: 0.0025246126668436544
Epoch 4/10: Training Loss: 0.002021608861176284
Epoch 5/10: Training Loss: 0.001003362707324795
Epoch 6/10: Training Loss: 0.0021732820497526156
Epoch 7/10: Training Loss: 0.0021619217379109843
Epoch 8/10: Training Loss: 0.00109300121560797
Epoch 9/10: Training Loss: 0.0013723179593786493
Epoch 0/10: Training Loss: 0.00403008677742698
Epoch 1/10: Training Loss: 0.0024900219657204366
Epoch 2/10: Training Loss: 0.001988429706413429
Epoch 3/10: Training Loss: 0.0022607929223067277
Epoch 4/10: Training Loss: 0.0023187469769191077
Epoch 5/10: Training Loss: 0.001665944298664173
Epoch 6/10: Training Loss: 0.0021506868875943697
Epoch 7/10: Training Loss: 0.001517680662495273
Epoch 8/10: Training Loss: 0.0007968577784258169
Epoch 9/10: Training Loss: 0.0008542252691475661
Epoch 0/10: Training Loss: 0.003990015783510008
Epoch 1/10: Training Loss: 0.0031476616859436035
Epoch 2/10: Training Loss: 0.002704666836278422
Epoch 3/10: Training Loss: 0.00215111829184152
Epoch 4/10: Training Loss: 0.00219706367779445
Epoch 5/10: Training Loss: 0.0023206439051594767
Epoch 6/10: Training Loss: 0.0017657175764337286
Epoch 7/10: Training Loss: 0.0015324071779117719
Epoch 8/10: Training Loss: 0.0009467630953221888
Epoch 9/10: Training Loss: 0.0010589503950172371
Epoch 0/10: Training Loss: 0.0030460357666015625
Epoch 1/10: Training Loss: 0.0020456997894801976
Epoch 2/10: Training Loss: 0.001426915723853316
Epoch 3/10: Training Loss: 0.0015364238089579014
Epoch 4/10: Training Loss: 0.001348703185473483
Epoch 5/10: Training Loss: 0.0018871053596215745
Epoch 6/10: Training Loss: 0.0007379984745950056
Epoch 7/10: Training Loss: 0.0014842710246337704
Epoch 8/10: Training Loss: 0.0016937078508131344
Epoch 9/10: Training Loss: 0.0006655623302137925
Epoch 0/10: Training Loss: 0.003303751258030991
Epoch 1/10: Training Loss: 0.0022009033366946354
Epoch 2/10: Training Loss: 0.0012893690470537524
Epoch 3/10: Training Loss: 0.0007689913357693725
Epoch 4/10: Training Loss: 0.0005589946647363206
Epoch 5/10: Training Loss: 0.0009001782335386686
Epoch 6/10: Training Loss: 0.0013168733909817562
Epoch 7/10: Training Loss: 0.0012874283307900457
Epoch 8/10: Training Loss: 0.0009717757533664352
Epoch 9/10: Training Loss: 0.0004631873288768932
Epoch 0/10: Training Loss: 0.003171071684433639
Epoch 1/10: Training Loss: 0.0020740880556633138
Epoch 2/10: Training Loss: 0.0017822960768740601
Epoch 3/10: Training Loss: 0.0008640406321894172
Epoch 4/10: Training Loss: 0.001357226101167363
Epoch 5/10: Training Loss: 0.0010403613188515412
Epoch 6/10: Training Loss: 0.00123357352303581
Epoch 7/10: Training Loss: 0.0021316729074606867
Epoch 8/10: Training Loss: 0.0007004502909315145
Epoch 9/10: Training Loss: 0.0006719578994563752
Epoch 0/10: Training Loss: 0.003729372471570969
Epoch 1/10: Training Loss: 0.0025621687993407248
Epoch 2/10: Training Loss: 0.002618162706494331
Epoch 3/10: Training Loss: 0.0023246049880981446
Epoch 4/10: Training Loss: 0.0018062729388475419
Epoch 5/10: Training Loss: 0.0017656039446592331
Epoch 6/10: Training Loss: 0.0013490628451108932
Epoch 7/10: Training Loss: 0.0010636840015649796
Epoch 8/10: Training Loss: 0.0010263008996844291
Epoch 9/10: Training Loss: 0.0005664191208779812
Epoch 0/10: Training Loss: 0.0036122117191553114
Epoch 1/10: Training Loss: 0.00305386558175087
Epoch 2/10: Training Loss: 0.0023524411022663116
Epoch 3/10: Training Loss: 0.002303682640194893
Epoch 4/10: Training Loss: 0.0017813611775636673
Epoch 5/10: Training Loss: 0.0013875572010874748
Epoch 6/10: Training Loss: 0.0014125031419098377
Epoch 7/10: Training Loss: 0.0011756224557757378
Epoch 8/10: Training Loss: 0.000703367916867137
Epoch 9/10: Training Loss: 0.00034582477528601887
Epoch 0/10: Training Loss: 0.003456492722034454
Epoch 1/10: Training Loss: 0.002265046536922455
Epoch 2/10: Training Loss: 0.0016081798821687698
Epoch 3/10: Training Loss: 0.0024247577413916586
Epoch 4/10: Training Loss: 0.0010412189178168774
Epoch 5/10: Training Loss: 0.001249672845005989
Epoch 6/10: Training Loss: 0.001282692514359951
Epoch 7/10: Training Loss: 0.0006672993768006563
Epoch 8/10: Training Loss: 0.000544563727453351
Epoch 9/10: Training Loss: 0.00036859787069261075
Epoch 0/10: Training Loss: 0.003961017936657948
Epoch 1/10: Training Loss: 0.002736807249154255
Epoch 2/10: Training Loss: 0.002768210924355088
Epoch 3/10: Training Loss: 0.0029564093632303227
Epoch 4/10: Training Loss: 0.0026024587594779434
Epoch 5/10: Training Loss: 0.0024870079793747825
Epoch 6/10: Training Loss: 0.0019814425213321757
Epoch 7/10: Training Loss: 0.0018361753718868182
Epoch 8/10: Training Loss: 0.0016952787235284307
Epoch 9/10: Training Loss: 0.0012464916250508303
Epoch 0/10: Training Loss: 0.003933760390919485
Epoch 1/10: Training Loss: 0.002291162682187026
Epoch 2/10: Training Loss: 0.003254932582758035
Epoch 3/10: Training Loss: 0.003007837541543754
Epoch 4/10: Training Loss: 0.002509280374854993
Epoch 5/10: Training Loss: 0.0017957539315436295
Epoch 6/10: Training Loss: 0.0016870001319107737
Epoch 7/10: Training Loss: 0.0019102715382910077
Epoch 8/10: Training Loss: 0.001860162255111014
Epoch 9/10: Training Loss: 0.0014657000447534452
Epoch 0/10: Training Loss: 0.00357680639643578
Epoch 1/10: Training Loss: 0.002628829069198317
Epoch 2/10: Training Loss: 0.003147070574912296
Epoch 3/10: Training Loss: 0.0029484168359428455
Epoch 4/10: Training Loss: 0.0023059989236722325
Epoch 5/10: Training Loss: 0.002270544410511187
Epoch 6/10: Training Loss: 0.0020345724692010575
Epoch 7/10: Training Loss: 0.0018454733168243602
Epoch 8/10: Training Loss: 0.0018443953079782475
Epoch 9/10: Training Loss: 0.0016053187998996419
Epoch 0/10: Training Loss: 0.003393292821795735
Epoch 1/10: Training Loss: 0.0033107050602009754
Epoch 2/10: Training Loss: 0.0030455303113192124
Epoch 3/10: Training Loss: 0.002040334687327707
Epoch 4/10: Training Loss: 0.0020143133125557805
Epoch 5/10: Training Loss: 0.0024542121697735315
Epoch 6/10: Training Loss: 0.0026300963976525313
Epoch 7/10: Training Loss: 0.002204897387927731
Epoch 8/10: Training Loss: 0.0020455449622198444
Epoch 9/10: Training Loss: 0.0020570985923539726
Epoch 0/10: Training Loss: 0.0030959319594680078
Epoch 1/10: Training Loss: 0.0021045472448235317
Epoch 2/10: Training Loss: 0.0030293798209815627
Epoch 3/10: Training Loss: 0.0023122822606800406
Epoch 4/10: Training Loss: 0.002920084047001719
Epoch 5/10: Training Loss: 0.002681517837852832
Epoch 6/10: Training Loss: 0.0023813389784452933
Epoch 7/10: Training Loss: 0.002188388480255935
Epoch 8/10: Training Loss: 0.001867890160604818
Epoch 9/10: Training Loss: 0.001872124064047605
Epoch 0/10: Training Loss: 0.002621724708190817
Epoch 1/10: Training Loss: 0.0019561928629085717
Epoch 2/10: Training Loss: 0.002688237176036203
Epoch 3/10: Training Loss: 0.002646415438873089
Epoch 4/10: Training Loss: 0.0024574059524283503
Epoch 5/10: Training Loss: 0.0025321410981235127
Epoch 6/10: Training Loss: 0.0022620651500904008
Epoch 7/10: Training Loss: 0.0017982385805900523
Epoch 8/10: Training Loss: 0.0017673748218460588
Epoch 9/10: Training Loss: 0.0019338989494652149
Epoch 0/10: Training Loss: 0.0006477579036179711
Epoch 1/10: Training Loss: 0.0005501722149989184
Epoch 2/10: Training Loss: 0.0011444743941811954
Epoch 3/10: Training Loss: 0.0006335052935516133
Epoch 4/10: Training Loss: 0.0010708344333312091
Epoch 5/10: Training Loss: 0.0008140251916997573
Epoch 6/10: Training Loss: 0.0012643624754513012
Epoch 7/10: Training Loss: 0.0011019495480200823
Epoch 8/10: Training Loss: 0.000790407377130845
Epoch 9/10: Training Loss: 0.000673737859024721
Epoch 0/10: Training Loss: 0.000743807151037104
Epoch 1/10: Training Loss: 0.00035063773393630984
Epoch 2/10: Training Loss: 0.0004929217783843769
Epoch 3/10: Training Loss: 0.0003695367890245774
Epoch 4/10: Training Loss: 0.001203411291627323
Epoch 5/10: Training Loss: 0.0007368476075284621
Epoch 6/10: Training Loss: 0.0009066675515735851
Epoch 7/10: Training Loss: 0.0008629244916579302
Epoch 8/10: Training Loss: 0.0006737063474514905
Epoch 9/10: Training Loss: 0.0008267685770988464
Epoch 0/10: Training Loss: 0.000622591858401018
Epoch 1/10: Training Loss: 0.000541761692832498
Epoch 2/10: Training Loss: 0.000608547470148872
Epoch 3/10: Training Loss: 0.0006830596748520346
Epoch 4/10: Training Loss: 0.0003645083702662412
Epoch 5/10: Training Loss: 0.0003830726094105664
Epoch 6/10: Training Loss: 0.0007298450259601368
Epoch 7/10: Training Loss: 0.0007434353232383728
Epoch 8/10: Training Loss: 0.0011859578244826374
Epoch 9/10: Training Loss: 0.0008983529665890862
dataset: capitals layer_num_from_end:-17 Avg_acc:tensor(0.9273) Avg_AUC:0.9814736411630517 Avg_threshold:0.47904419898986816
dataset: inventions layer_num_from_end:-17 Avg_acc:tensor(0.8463) Avg_AUC:0.9444659496707956 Avg_threshold:0.6135138173898061
dataset: elements layer_num_from_end:-17 Avg_acc:tensor(0.6900) Avg_AUC:0.7824981693452037 Avg_threshold:0.3088616927464803
dataset: animals layer_num_from_end:-17 Avg_acc:tensor(0.7126) Avg_AUC:0.8395494771982867 Avg_threshold:0.4065061906973521
dataset: companies layer_num_from_end:-17 Avg_acc:tensor(0.8706) Avg_AUC:0.9405842592592593 Avg_threshold:0.3903193771839142
dataset: facts layer_num_from_end:-17 Avg_acc:tensor(0.7810) Avg_AUC:0.8561824511939852 Avg_threshold:0.6771365404129028


