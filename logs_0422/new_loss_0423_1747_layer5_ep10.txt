2024-04-24 00:37:59.388190: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-04-24 00:38:00.450191: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
================layer -5================
Epoch 0/10: Training Loss: 0.00333591462015272
Epoch 1/10: Training Loss: 0.002333303014715235
Epoch 2/10: Training Loss: 0.004051033433500703
Epoch 3/10: Training Loss: 0.003732260290559355
Epoch 4/10: Training Loss: 0.003517337612338833
Epoch 5/10: Training Loss: 0.004132220378288856
Epoch 6/10: Training Loss: 0.0033218443810522974
Epoch 7/10: Training Loss: 0.002530936594609614
Epoch 8/10: Training Loss: 0.0018495900647623555
Epoch 9/10: Training Loss: 0.003623096259323867
Epoch 0/10: Training Loss: 0.003633930966570661
Epoch 1/10: Training Loss: 0.0035815664104648405
Epoch 2/10: Training Loss: 0.0037717202326634546
Epoch 3/10: Training Loss: 0.0028679493840757784
Epoch 4/10: Training Loss: 0.0032042879741508642
Epoch 5/10: Training Loss: 0.0019631141966039486
Epoch 6/10: Training Loss: 0.0037739989640829447
Epoch 7/10: Training Loss: 0.001434045967522201
Epoch 8/10: Training Loss: 0.0011615803191711853
Epoch 9/10: Training Loss: 0.002961672894604556
Epoch 0/10: Training Loss: 0.003536068476163424
Epoch 1/10: Training Loss: 0.0035456794125216825
Epoch 2/10: Training Loss: 0.002556367770775215
Epoch 3/10: Training Loss: 0.003413474643147075
Epoch 4/10: Training Loss: 0.0021931160163212488
Epoch 5/10: Training Loss: 0.0012279729117880334
Epoch 6/10: Training Loss: 0.001859644076207301
Epoch 7/10: Training Loss: 0.0017862251171698938
Epoch 8/10: Training Loss: 0.0010887632420013
Epoch 9/10: Training Loss: 0.0013227408582513983
Epoch 0/10: Training Loss: 0.0032242393932459543
Epoch 1/10: Training Loss: 0.0024461439050779753
Epoch 2/10: Training Loss: 0.004058481725447017
Epoch 3/10: Training Loss: 0.0026838715456746105
Epoch 4/10: Training Loss: 0.002233782794577944
Epoch 5/10: Training Loss: 0.0017565117657550274
Epoch 6/10: Training Loss: 0.0007191815990611819
Epoch 7/10: Training Loss: 0.00033742029425556675
Epoch 8/10: Training Loss: 0.0009401304407353782
Epoch 9/10: Training Loss: 0.0015036818439975108
Epoch 0/10: Training Loss: 0.0040044104394737195
Epoch 1/10: Training Loss: 0.0027734163714332816
Epoch 2/10: Training Loss: 0.00394172134574937
Epoch 3/10: Training Loss: 0.0026091571234486584
Epoch 4/10: Training Loss: 0.0012657569229968487
Epoch 5/10: Training Loss: 0.0014204222000449712
Epoch 6/10: Training Loss: 0.0010428011783061584
Epoch 7/10: Training Loss: 0.001461632229799142
Epoch 8/10: Training Loss: 0.0015847092391523116
Epoch 9/10: Training Loss: 0.0021558425909171076
Epoch 0/10: Training Loss: 0.0037688338683426748
Epoch 1/10: Training Loss: 0.0033330628476991244
Epoch 2/10: Training Loss: 0.004387802872921061
Epoch 3/10: Training Loss: 0.003839171005904309
Epoch 4/10: Training Loss: 0.002350902996180248
Epoch 5/10: Training Loss: 0.0014228681845167663
Epoch 6/10: Training Loss: 0.0005233816665374428
Epoch 7/10: Training Loss: 0.00036666245182599027
Epoch 8/10: Training Loss: 0.0013867313145128496
Epoch 9/10: Training Loss: 0.001045225405254247
Epoch 0/10: Training Loss: 0.0015410391613841056
Epoch 1/10: Training Loss: 0.0015928527340292931
Epoch 2/10: Training Loss: 0.0009932915680110454
Epoch 3/10: Training Loss: 0.0015522408299148082
Epoch 4/10: Training Loss: 0.0028875052928924562
Epoch 5/10: Training Loss: 0.0016543075442314148
Epoch 6/10: Training Loss: 0.0020876606926321984
Epoch 7/10: Training Loss: 0.002259843982756138
Epoch 8/10: Training Loss: 0.0006056370213627816
Epoch 9/10: Training Loss: 0.0006667651236057281
Epoch 0/10: Training Loss: 0.001586860790848732
Epoch 1/10: Training Loss: 0.001895238645374775
Epoch 2/10: Training Loss: 0.0006200795061886311
Epoch 3/10: Training Loss: 0.0015573441982269288
Epoch 4/10: Training Loss: 0.001418109517544508
Epoch 5/10: Training Loss: 0.002120762877166271
Epoch 6/10: Training Loss: 0.0019070176407694817
Epoch 7/10: Training Loss: 0.0018167838454246521
Epoch 8/10: Training Loss: 0.0019034778699278832
Epoch 9/10: Training Loss: 0.0019919268786907198
Epoch 0/10: Training Loss: 0.0015259397216141224
Epoch 1/10: Training Loss: 0.0007339667994529009
Epoch 2/10: Training Loss: 0.0010313482955098153
Epoch 3/10: Training Loss: 0.0021608909592032433
Epoch 4/10: Training Loss: 0.00043190200813114643
Epoch 5/10: Training Loss: 0.0007908617146313191
Epoch 6/10: Training Loss: 0.002645132876932621
Epoch 7/10: Training Loss: 0.0021109685301780702
Epoch 8/10: Training Loss: 0.0012946877628564834
Epoch 9/10: Training Loss: 0.0013028664514422416
Epoch 0/10: Training Loss: 0.0038444684569243414
Epoch 1/10: Training Loss: 0.003043224667287936
Epoch 2/10: Training Loss: 0.003494472260687761
Epoch 3/10: Training Loss: 0.003241632015082487
Epoch 4/10: Training Loss: 0.0026679179471009854
Epoch 5/10: Training Loss: 0.0024594444378166444
Epoch 6/10: Training Loss: 0.002707050104809415
Epoch 7/10: Training Loss: 0.0029152958256423852
Epoch 8/10: Training Loss: 0.0023916470017402796
Epoch 9/10: Training Loss: 0.002102062580691781
Epoch 0/10: Training Loss: 0.003073492247587556
Epoch 1/10: Training Loss: 0.0024714177581155374
Epoch 2/10: Training Loss: 0.002294573814246305
Epoch 3/10: Training Loss: 0.002660926739880993
Epoch 4/10: Training Loss: 0.002827528555681751
Epoch 5/10: Training Loss: 0.0029536231308226373
Epoch 6/10: Training Loss: 0.0023297557405605436
Epoch 7/10: Training Loss: 0.002622379618845168
Epoch 8/10: Training Loss: 0.002195894718170166
Epoch 9/10: Training Loss: 0.0019372707339608746
Epoch 0/10: Training Loss: 0.0030828469498142314
Epoch 1/10: Training Loss: 0.003015333679830952
Epoch 2/10: Training Loss: 0.0032331552475121372
Epoch 3/10: Training Loss: 0.003724318401069398
Epoch 4/10: Training Loss: 0.003339888943228752
Epoch 5/10: Training Loss: 0.002959910471727894
Epoch 6/10: Training Loss: 0.0030664323241847338
Epoch 7/10: Training Loss: 0.0023357256962235565
Epoch 8/10: Training Loss: 0.0028476584109531087
Epoch 9/10: Training Loss: 0.0027228419188481227
Epoch 0/10: Training Loss: 0.004641948551531659
Epoch 1/10: Training Loss: 0.0055396592380195266
Epoch 2/10: Training Loss: 0.005417000378993963
Epoch 3/10: Training Loss: 0.004413547500079831
Epoch 4/10: Training Loss: 0.005759201697166393
Epoch 5/10: Training Loss: 0.004905617394984164
Epoch 6/10: Training Loss: 0.005202580761435806
Epoch 7/10: Training Loss: 0.0051016744398912845
Epoch 8/10: Training Loss: 0.005124388941076418
Epoch 9/10: Training Loss: 0.005481174055314222
Epoch 0/10: Training Loss: 0.0043068165021226896
Epoch 1/10: Training Loss: 0.004201708645220624
Epoch 2/10: Training Loss: 0.0049613027383160115
Epoch 3/10: Training Loss: 0.004949242073968546
Epoch 4/10: Training Loss: 0.004621329686499589
Epoch 5/10: Training Loss: 0.005436443729905893
Epoch 6/10: Training Loss: 0.004342070478477226
Epoch 7/10: Training Loss: 0.004407025330903514
Epoch 8/10: Training Loss: 0.0051334062159456165
Epoch 9/10: Training Loss: 0.004513365543441267
Epoch 0/10: Training Loss: 0.003471372143322269
Epoch 1/10: Training Loss: 0.004125978773003382
Epoch 2/10: Training Loss: 0.0028996562326191275
Epoch 3/10: Training Loss: 0.002466723026818787
Epoch 4/10: Training Loss: 0.005884680526935502
Epoch 5/10: Training Loss: 0.0053360245085709935
Epoch 6/10: Training Loss: 0.005390567495333438
Epoch 7/10: Training Loss: 0.004767871455640982
Epoch 8/10: Training Loss: 0.0048470741865650705
Epoch 9/10: Training Loss: 0.004536001098077029
Epoch 0/10: Training Loss: 0.003925553840749404
Epoch 1/10: Training Loss: 0.0034796564018025116
Epoch 2/10: Training Loss: 0.0023824011578279384
Epoch 3/10: Training Loss: 0.0018374975989846622
Epoch 4/10: Training Loss: 0.001998467305127312
Epoch 5/10: Training Loss: 0.0011992360739146962
Epoch 6/10: Training Loss: 0.002024723677074208
Epoch 7/10: Training Loss: 0.0018484259352964513
Epoch 8/10: Training Loss: 0.0017125962411656099
Epoch 9/10: Training Loss: 0.0017008578076082117
Epoch 0/10: Training Loss: 0.003793426471598008
Epoch 1/10: Training Loss: 0.0028491810840718884
Epoch 2/10: Training Loss: 0.003328086348140941
Epoch 3/10: Training Loss: 0.0015869708622203154
Epoch 4/10: Training Loss: 0.0024624233736711388
Epoch 5/10: Training Loss: 0.0009391651434056899
Epoch 6/10: Training Loss: 0.0010120667955454657
Epoch 7/10: Training Loss: 0.0012499656747369204
Epoch 8/10: Training Loss: 0.001862054011401008
Epoch 9/10: Training Loss: 0.0011618671171805437
Epoch 0/10: Training Loss: 0.004230739789850572
Epoch 1/10: Training Loss: 0.0032490954679601333
Epoch 2/10: Training Loss: 0.0015446005498661714
Epoch 3/10: Training Loss: 0.0013755851808716268
Epoch 4/10: Training Loss: 0.0016477665480445413
Epoch 5/10: Training Loss: 0.002080174929955426
Epoch 6/10: Training Loss: 0.0006785548785153558
Epoch 7/10: Training Loss: 0.0007272272425539354
Epoch 8/10: Training Loss: 0.0011659699327805464
Epoch 9/10: Training Loss: 0.00012060283299754648
dataset: capitals layer_num_from_end:-5 Avg_acc:tensor(0.7718) Avg_AUC:0.8909352119990741 Avg_threshold:0.4314543108145396
dataset: inventions layer_num_from_end:-5 Avg_acc:tensor(0.5982) Avg_AUC:0.7135992076777145 Avg_threshold:0.13855489219228426
dataset: elements layer_num_from_end:-5 Avg_acc:tensor(0.5875) Avg_AUC:0.6319790341850695 Avg_threshold:0.4790049393971761
dataset: animals layer_num_from_end:-5 Avg_acc:tensor(0.5605) Avg_AUC:0.7189048458889729 Avg_threshold:0.4362773497899373
dataset: companies layer_num_from_end:-5 Avg_acc:tensor(0.6169) Avg_AUC:0.8734782407407408 Avg_threshold:0.6567703286806742
dataset: facts layer_num_from_end:-5 Avg_acc:tensor(0.5964) Avg_AUC:0.6584451137027069 Avg_threshold:0.7137800852457682


