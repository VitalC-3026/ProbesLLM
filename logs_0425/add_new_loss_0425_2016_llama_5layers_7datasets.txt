2024-04-26 00:24:11.336780: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-04-26 00:24:12.459767: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
================layer -1================
Epoch 0/100: Training Loss: 0.005167053678096869
Epoch 1/100: Training Loss: 0.00544888850970146
Epoch 2/100: Training Loss: 0.005319802042765495
Epoch 3/100: Training Loss: 0.00510289348088778
Epoch 4/100: Training Loss: 0.005457876584468744
Epoch 5/100: Training Loss: 0.005299921219165509
Epoch 6/100: Training Loss: 0.0050421024744327254
Epoch 7/100: Training Loss: 0.0053020880008355165
Epoch 8/100: Training Loss: 0.005066813948826912
Epoch 9/100: Training Loss: 0.004951296708522699
Epoch 10/100: Training Loss: 0.004778812328974406
Epoch 11/100: Training Loss: 0.004826522790468656
Epoch 12/100: Training Loss: 0.004821452192771129
Epoch 13/100: Training Loss: 0.004856817615337861
Epoch 14/100: Training Loss: 0.004067169168056586
Epoch 15/100: Training Loss: 0.004479307012680249
Epoch 16/100: Training Loss: 0.0040745383653885275
Epoch 17/100: Training Loss: 0.0034153171074696076
Epoch 18/100: Training Loss: 0.00421368922942724
Epoch 19/100: Training Loss: 0.003979055163187859
Epoch 20/100: Training Loss: 0.004022272733541636
Epoch 21/100: Training Loss: 0.003485723565786313
Epoch 22/100: Training Loss: 0.0028928424685429302
Epoch 23/100: Training Loss: 0.0029437622198691736
Epoch 24/100: Training Loss: 0.002741847282800919
Epoch 25/100: Training Loss: 0.0024940627507674387
Epoch 26/100: Training Loss: 0.002996389682476337
Epoch 27/100: Training Loss: 0.0025205098283596528
Epoch 28/100: Training Loss: 0.0025521138539681067
Epoch 29/100: Training Loss: 0.0024809206907565775
Epoch 30/100: Training Loss: 0.0022232700616885456
Epoch 31/100: Training Loss: 0.0021610156847880436
Epoch 32/100: Training Loss: 0.0020491329905314324
Epoch 33/100: Training Loss: 0.0015031351492955135
Epoch 34/100: Training Loss: 0.0016753031657292293
Epoch 35/100: Training Loss: 0.0021506975858639446
Epoch 36/100: Training Loss: 0.0011966144427275045
Epoch 37/100: Training Loss: 0.0014558153656812815
Epoch 38/100: Training Loss: 0.002911242154928354
Epoch 39/100: Training Loss: 0.0009658535321553549
Epoch 40/100: Training Loss: 0.001770768601160783
Epoch 41/100: Training Loss: 0.0015655901187505478
Epoch 42/100: Training Loss: 0.0014019177701228703
Epoch 43/100: Training Loss: 0.0011344638008337754
Epoch 44/100: Training Loss: 0.0010444562977705246
Epoch 45/100: Training Loss: 0.0008763953661307311
Epoch 46/100: Training Loss: 0.003656531755740826
Epoch 47/100: Training Loss: 0.00243147978415856
Epoch 48/100: Training Loss: 0.0016014810938101548
Epoch 49/100: Training Loss: 0.00244481976215656
Epoch 50/100: Training Loss: 0.0016155183697358156
Epoch 51/100: Training Loss: 0.0010078621980471488
Epoch 52/100: Training Loss: 0.000986081571915211
Epoch 53/100: Training Loss: 0.0005569364875555038
Epoch 54/100: Training Loss: 0.0006913653551003871
Epoch 55/100: Training Loss: 0.001007540103716728
Epoch 56/100: Training Loss: 0.0020111587185126087
Epoch 57/100: Training Loss: 0.00268041438017136
Epoch 58/100: Training Loss: 0.0031044051433220888
Epoch 59/100: Training Loss: 0.0024403646970406557
Epoch 60/100: Training Loss: 0.001698004702727
Epoch 61/100: Training Loss: 0.0008189330498377482
Epoch 62/100: Training Loss: 0.0008351500026690654
Epoch 63/100: Training Loss: 0.0008395287470939832
Epoch 64/100: Training Loss: 0.0017504917505459909
Epoch 65/100: Training Loss: 0.002207143375506768
Epoch 66/100: Training Loss: 0.0013717386202934461
Epoch 67/100: Training Loss: 0.001499559061649518
Epoch 68/100: Training Loss: 0.0018202024392592602
Epoch 69/100: Training Loss: 0.0007087106888110822
Epoch 70/100: Training Loss: 0.0008371502925188113
Epoch 71/100: Training Loss: 0.000503773060746682
Epoch 72/100: Training Loss: 0.0005557496960346515
Epoch 73/100: Training Loss: 0.001014823237290749
Epoch 74/100: Training Loss: 0.0005626733868550032
Epoch 75/100: Training Loss: 0.001149122913678487
Epoch 76/100: Training Loss: 0.0005810949951410294
Epoch 77/100: Training Loss: 0.00038590291753793374
Epoch 78/100: Training Loss: 0.0003225977699726056
Epoch 79/100: Training Loss: 0.0006774313365801787
Epoch 80/100: Training Loss: 0.000324500438112479
Epoch 81/100: Training Loss: 7.690773464930363e-05
Epoch 82/100: Training Loss: 4.69178253880296e-05
Epoch 83/100: Training Loss: 0.0003555833529203366
Epoch 84/100: Training Loss: 0.0012813664208620023
Epoch 85/100: Training Loss: 0.0001001127291088685
Epoch 86/100: Training Loss: 9.08093120998297e-05
Epoch 87/100: Training Loss: 0.0009016812993929937
Epoch 88/100: Training Loss: 0.0005928232119633601
Epoch 89/100: Training Loss: 0.0002099941365229778
Epoch 90/100: Training Loss: 0.0005299971462824406
Epoch 91/100: Training Loss: 0.00030069072277118
Epoch 92/100: Training Loss: 0.00038408925040410116
Epoch 93/100: Training Loss: 0.00043783580454496236
Epoch 94/100: Training Loss: 0.0003352902638606536
Epoch 95/100: Training Loss: 0.00023886126776536307
Epoch 96/100: Training Loss: 0.000148541664179319
Epoch 97/100: Training Loss: 0.00023101580639680228
Epoch 98/100: Training Loss: 0.00030380286849462066
Epoch 99/100: Training Loss: 8.008736543930494e-05
Epoch 0/100: Training Loss: 0.0003563399539400109
Epoch 1/100: Training Loss: 0.00032197684049606323
Epoch 2/100: Training Loss: 0.00027004943433898445
Epoch 3/100: Training Loss: 0.00019928134743942808
Epoch 4/100: Training Loss: 0.00016448830542543008
Epoch 5/100: Training Loss: 0.00025156325756701655
Epoch 6/100: Training Loss: 0.00015253064143283486
Epoch 7/100: Training Loss: 0.00014730766749702763
Epoch 8/100: Training Loss: 8.066816280508255e-05
Epoch 9/100: Training Loss: 2.033398758135569e-05
Epoch 10/100: Training Loss: 1.691714046113694e-05
Epoch 11/100: Training Loss: 6.827556281747305e-05
Epoch 12/100: Training Loss: 6.808079110934596e-05
Epoch 13/100: Training Loss: 0.00015379764106241578
Epoch 14/100: Training Loss: 4.167792558536401e-05
Epoch 15/100: Training Loss: 4.493666086095331e-05
Epoch 16/100: Training Loss: 6.309596011456887e-05
Epoch 17/100: Training Loss: 3.464719606354632e-05
Epoch 18/100: Training Loss: 5.29329672522609e-05
Epoch 19/100: Training Loss: 3.297551380545569e-05
Epoch 20/100: Training Loss: 1.4223172976698041e-05
Epoch 21/100: Training Loss: 1.115492344604212e-05
Epoch 22/100: Training Loss: 2.090344330788728e-05
Epoch 23/100: Training Loss: 1.2563932018830638e-05
Epoch 24/100: Training Loss: 1.7857866052688503e-05
Epoch 25/100: Training Loss: 9.01484231343451e-06
Epoch 26/100: Training Loss: 5.3852448874725355e-06
Epoch 27/100: Training Loss: 2.4353630105743494e-05
Epoch 28/100: Training Loss: 4.434732578260482e-05
Epoch 29/100: Training Loss: 7.629717153083583e-06
Epoch 30/100: Training Loss: 3.67735266150915e-05
Epoch 31/100: Training Loss: 1.9371760132066874e-05
Epoch 32/100: Training Loss: 2.2600231600315583e-05
Epoch 33/100: Training Loss: 8.452166865826188e-06
Epoch 34/100: Training Loss: 2.5082960090027795e-05
Epoch 35/100: Training Loss: 6.069645344075066e-06
Epoch 36/100: Training Loss: 9.636972057779273e-06
Epoch 37/100: Training Loss: 7.768275922022326e-06
Epoch 38/100: Training Loss: 1.716064372738915e-05
Epoch 39/100: Training Loss: 4.319768532642869e-05
Epoch 40/100: Training Loss: 1.3477726517905034e-05
Epoch 41/100: Training Loss: 2.5322506279421495e-05
Epoch 42/100: Training Loss: 1.0961972619720103e-05
Epoch 43/100: Training Loss: 1.4428240393242494e-05
Epoch 44/100: Training Loss: 5.894197535287639e-06
Epoch 45/100: Training Loss: 5.30311048465192e-06
Epoch 46/100: Training Loss: 1.1192346534654164e-05
Epoch 47/100: Training Loss: 9.062077699755339e-06
Epoch 48/100: Training Loss: 7.586828286805495e-06
Epoch 49/100: Training Loss: 1.2563947680084695e-05
Epoch 50/100: Training Loss: 1.5843677388779787e-05
Epoch 51/100: Training Loss: 1.0034465126593017e-05
Epoch 52/100: Training Loss: 5.747049612581997e-06
Epoch 53/100: Training Loss: 4.903536557215746e-06
Epoch 54/100: Training Loss: 1.72092750534883e-05
Epoch 55/100: Training Loss: 1.2806667883634032e-05
Epoch 56/100: Training Loss: 1.028687233781868e-05
Epoch 57/100: Training Loss: 7.010758812685451e-06
Epoch 58/100: Training Loss: 8.120812316980598e-06
Epoch 59/100: Training Loss: 2.1100960955892443e-05
Epoch 60/100: Training Loss: 5.908983847285066e-06
Epoch 61/100: Training Loss: 3.6420501066964837e-05
Epoch 62/100: Training Loss: 1.572710858666309e-05
Epoch 63/100: Training Loss: 1.3847173412952722e-05
Epoch 64/100: Training Loss: 1.4494656639449265e-05
Epoch 65/100: Training Loss: 1.2776485514814544e-05
Epoch 66/100: Training Loss: 5.8950014796625876e-06
Epoch 67/100: Training Loss: 2.9668738155087013e-05
Epoch 68/100: Training Loss: 2.6043141135453108e-05
Epoch 69/100: Training Loss: 7.096931730885677e-06
Epoch 70/100: Training Loss: 1.5475115876628144e-05
Epoch 71/100: Training Loss: 1.5252252055056427e-05
Epoch 72/100: Training Loss: 1.0469791608875107e-05
Epoch 73/100: Training Loss: 3.062759739066988e-05
Epoch 74/100: Training Loss: 1.8796908581470695e-05
Epoch 75/100: Training Loss: 1.2488392570093608e-05
Epoch 76/100: Training Loss: 5.246839076781754e-06
Epoch 77/100: Training Loss: 9.568933305652152e-06
Epoch 78/100: Training Loss: 1.3738607511659374e-05
Epoch 79/100: Training Loss: 2.532793969049582e-05
Epoch 80/100: Training Loss: 1.3622276760604349e-05
Epoch 81/100: Training Loss: 1.4771799235333241e-05
Epoch 82/100: Training Loss: 2.4314405972914846e-05
Epoch 83/100: Training Loss: 1.5642949183692847e-05
Epoch 84/100: Training Loss: 7.301664205410021e-05
Epoch 85/100: Training Loss: 1.2054881749198575e-05
Epoch 86/100: Training Loss: 5.963235475423625e-06
Epoch 87/100: Training Loss: 1.0682833735985606e-05
Epoch 88/100: Training Loss: 2.422039877339329e-05
Epoch 89/100: Training Loss: 9.36070022762089e-06
Epoch 90/100: Training Loss: 2.942477392592371e-06
Epoch 91/100: Training Loss: 2.0255840011776295e-05
Epoch 92/100: Training Loss: 7.820588686908575e-06
Epoch 93/100: Training Loss: 1.4360008485648665e-05
Epoch 94/100: Training Loss: 7.989149720245146e-06
Epoch 95/100: Training Loss: 5.637107087057e-06
Epoch 96/100: Training Loss: 8.821234758525686e-06
Epoch 97/100: Training Loss: 1.0401370722148986e-05
Epoch 98/100: Training Loss: 1.2563233526899676e-05
Epoch 99/100: Training Loss: 1.4312576811610316e-05
Epoch 0/100: Training Loss: 0.00032560581265531514
Epoch 1/100: Training Loss: 0.0001045432262021492
Epoch 2/100: Training Loss: 0.0002611334439855895
Epoch 3/100: Training Loss: 0.0003357400519276097
Epoch 4/100: Training Loss: 3.5434417696290424e-05
Epoch 5/100: Training Loss: 0.00010036849058591403
Epoch 6/100: Training Loss: 0.0001615674921829776
Epoch 7/100: Training Loss: 0.00018236196148988887
Epoch 8/100: Training Loss: 3.978821166635099e-05
Epoch 9/100: Training Loss: 4.613241475766601e-05
Epoch 10/100: Training Loss: 2.1262966703235833e-05
Epoch 11/100: Training Loss: 0.0001394912870221548
Epoch 12/100: Training Loss: 0.00012879548130801362
Epoch 13/100: Training Loss: 0.00013414080933208379
Epoch 14/100: Training Loss: 0.0001840194628249466
Epoch 15/100: Training Loss: 8.22157072265763e-05
Epoch 16/100: Training Loss: 2.7921598623780644e-05
Epoch 17/100: Training Loss: 2.7885445608542515e-05
Epoch 18/100: Training Loss: 0.00011084402376170612
Epoch 19/100: Training Loss: 0.00018220720180558942
Epoch 20/100: Training Loss: 0.0003066755055841817
Epoch 21/100: Training Loss: 0.0003642544352630684
Epoch 22/100: Training Loss: 0.0002714897510153136
Epoch 23/100: Training Loss: 6.774203995099434e-05
Epoch 24/100: Training Loss: 0.00010450062135495751
Epoch 25/100: Training Loss: 0.00015440859675946818
Epoch 26/100: Training Loss: 0.00021293348046988922
Epoch 27/100: Training Loss: 2.819401469098497e-05
Epoch 28/100: Training Loss: 4.451371291104485e-05
Epoch 29/100: Training Loss: 3.992403726771946e-05
Epoch 30/100: Training Loss: 3.623315954909605e-05
Epoch 31/100: Training Loss: 2.0820217636915353e-05
Epoch 32/100: Training Loss: 1.5261391287340838e-05
Epoch 33/100: Training Loss: 1.698782208177569e-05
Epoch 34/100: Training Loss: 2.582674652203176e-05
Epoch 35/100: Training Loss: 9.189908734071847e-06
Epoch 36/100: Training Loss: 4.692810558086067e-05
Epoch 37/100: Training Loss: 2.2585840887343722e-05
Epoch 38/100: Training Loss: 2.5074701435965112e-05
Epoch 39/100: Training Loss: 3.4826059669795616e-05
Epoch 40/100: Training Loss: 8.772005120553582e-06
Epoch 41/100: Training Loss: 4.687067121267319e-05
Epoch 42/100: Training Loss: 6.770631361634753e-06
Epoch 43/100: Training Loss: 9.460485333838074e-06
Epoch 44/100: Training Loss: 4.241206657933703e-06
Epoch 45/100: Training Loss: 1.9023917632270182e-05
Epoch 46/100: Training Loss: 6.2056146014262645e-06
Epoch 47/100: Training Loss: 1.9267224601356153e-05
Epoch 48/100: Training Loss: 6.174303093984116e-06
Epoch 49/100: Training Loss: 1.892828850320022e-05
Epoch 50/100: Training Loss: 6.930791452628186e-06
Epoch 51/100: Training Loss: 2.8412489903188937e-05
Epoch 52/100: Training Loss: 5.3865790822252425e-06
Epoch 53/100: Training Loss: 2.68574730320349e-06
Epoch 54/100: Training Loss: 3.403437016236836e-05
Epoch 55/100: Training Loss: 5.894116804475698e-05
Epoch 56/100: Training Loss: 0.0001012202335428868
Epoch 57/100: Training Loss: 0.00016584013045103842
Epoch 58/100: Training Loss: 1.4954219188760308e-05
Epoch 59/100: Training Loss: 1.0976165415070175e-05
Epoch 60/100: Training Loss: 7.626707602410295e-05
Epoch 61/100: Training Loss: 3.3029924016193026e-05
Epoch 62/100: Training Loss: 2.8368785168250763e-05
Epoch 63/100: Training Loss: 1.5858752543435377e-05
Epoch 64/100: Training Loss: 9.265651597696192e-05
Epoch 65/100: Training Loss: 8.370571377859936e-06
Epoch 66/100: Training Loss: 8.656596975628607e-06
Epoch 67/100: Training Loss: 2.5565921417458565e-05
Epoch 68/100: Training Loss: 7.593197794420417e-06
Epoch 69/100: Training Loss: 1.0509807742896123e-05
Epoch 70/100: Training Loss: 7.560496677842615e-06
Epoch 71/100: Training Loss: 1.8640212731528605e-05
Epoch 72/100: Training Loss: 6.864881943658466e-06
Epoch 73/100: Training Loss: 6.366399963854125e-06
Epoch 74/100: Training Loss: 1.8401276077906606e-06
Epoch 75/100: Training Loss: 0.00020051850385255943
Epoch 76/100: Training Loss: 8.66764220851579e-06
Epoch 77/100: Training Loss: 7.320664785128103e-06
Epoch 78/100: Training Loss: 9.27503625293393e-06
Epoch 79/100: Training Loss: 4.5974259538685576e-06
Epoch 80/100: Training Loss: 4.146095340001097e-06
Epoch 81/100: Training Loss: 6.592850942518646e-06
Epoch 82/100: Training Loss: 2.9336047944574873e-05
Epoch 83/100: Training Loss: 2.1065454677219305e-05
Epoch 84/100: Training Loss: 5.207125653778266e-06
Epoch 85/100: Training Loss: 6.366428409226879e-06
Epoch 86/100: Training Loss: 5.241871149831228e-06
Epoch 87/100: Training Loss: 4.881210688383601e-06
Epoch 88/100: Training Loss: 8.01142694942687e-06
Epoch 89/100: Training Loss: 6.9622341248447e-06
Epoch 90/100: Training Loss: 1.0456929902010914e-05
Epoch 91/100: Training Loss: 0.000288543118610641
Epoch 92/100: Training Loss: 5.715961749265097e-06
Epoch 93/100: Training Loss: 5.294217483658866e-06
Epoch 94/100: Training Loss: 2.6379796208569366e-06
Epoch 95/100: Training Loss: 3.4054442060209506e-06
Epoch 96/100: Training Loss: 0.000380646083419679
Epoch 97/100: Training Loss: 1.2363962475126145e-05
Epoch 98/100: Training Loss: 4.119689605826706e-06
Epoch 99/100: Training Loss: 4.665186519131941e-06
Epoch 0/100: Training Loss: 0.000665377215905623
Epoch 1/100: Training Loss: 0.0004604845561764457
Epoch 2/100: Training Loss: 0.0002015305683016777
Epoch 3/100: Training Loss: 0.00032724470577456734
Epoch 4/100: Training Loss: 0.00017841947688297793
Epoch 5/100: Training Loss: 0.000706566260619597
Epoch 6/100: Training Loss: 0.00016171281987970525
Epoch 7/100: Training Loss: 0.000179846737195145
Epoch 8/100: Training Loss: 6.124594418162649e-05
Epoch 9/100: Training Loss: 9.255239062688567e-05
Epoch 10/100: Training Loss: 2.864740619605238e-05
Epoch 11/100: Training Loss: 0.0005661167204380035
Epoch 12/100: Training Loss: 0.00011507920412854714
Epoch 13/100: Training Loss: 0.00022003186697309667
Epoch 14/100: Training Loss: 7.425093684684147e-05
Epoch 15/100: Training Loss: 0.00012707106599753554
Epoch 16/100: Training Loss: 0.000632252340967005
Epoch 17/100: Training Loss: 0.0003530246290293607
Epoch 18/100: Training Loss: 0.0001318310218101198
Epoch 19/100: Training Loss: 5.063189362937754e-05
Epoch 20/100: Training Loss: 0.00031193650581619954
Epoch 21/100: Training Loss: 9.642422876574777e-05
Epoch 22/100: Training Loss: 3.529771400446242e-05
Epoch 23/100: Training Loss: 7.442146201025357e-05
Epoch 24/100: Training Loss: 0.00020142021504315462
Epoch 25/100: Training Loss: 6.827071563086726e-05
Epoch 26/100: Training Loss: 7.749657061966983e-05
Epoch 27/100: Training Loss: 7.498053495179523e-05
Epoch 28/100: Training Loss: 6.247128105976364e-05
Epoch 29/100: Training Loss: 3.8302469659935346e-05
Epoch 30/100: Training Loss: 4.672996953807094e-05
Epoch 31/100: Training Loss: 4.834793931381269e-05
Epoch 32/100: Training Loss: 0.00017961407588286833
Epoch 33/100: Training Loss: 2.5296708653596314e-05
Epoch 34/100: Training Loss: 5.071381191638383e-05
Epoch 35/100: Training Loss: 2.3046258667653258e-05
Epoch 36/100: Training Loss: 7.421251555735415e-05
Epoch 37/100: Training Loss: 6.13631849939173e-05
Epoch 38/100: Training Loss: 6.31098931824619e-05
Epoch 39/100: Training Loss: 3.5143313421444457e-05
Epoch 40/100: Training Loss: 3.146061419763348e-05
Epoch 41/100: Training Loss: 3.722570413215594e-05
Epoch 42/100: Training Loss: 3.851458260958845e-05
Epoch 43/100: Training Loss: 2.5890962305394086e-05
Epoch 44/100: Training Loss: 4.184084775095636e-05
Epoch 45/100: Training Loss: 9.403371844779362e-05
Epoch 46/100: Training Loss: 6.471083668822591e-05
Epoch 47/100: Training Loss: 3.416981886733662e-05
Epoch 48/100: Training Loss: 4.34074593199925e-05
Epoch 49/100: Training Loss: 0.0001018544151024385
Epoch 50/100: Training Loss: 8.633356880057942e-05
Epoch 51/100: Training Loss: 0.00011141637170856649
Epoch 52/100: Training Loss: 1.323303228921511e-05
Epoch 53/100: Training Loss: 4.822795109992677e-05
Epoch 54/100: Training Loss: 4.779623652046377e-05
Epoch 55/100: Training Loss: 1.557252835482359e-05
Epoch 56/100: Training Loss: 0.00011649907312609933
Epoch 57/100: Training Loss: 5.704105239022862e-05
Epoch 58/100: Training Loss: 5.65396367826245e-05
Epoch 59/100: Training Loss: 0.0002665462819012729
Epoch 60/100: Training Loss: 2.257045443085107e-05
Epoch 61/100: Training Loss: 1.4703695408322594e-05
Epoch 62/100: Training Loss: 5.1615916361862963e-05
Epoch 63/100: Training Loss: 0.000305062092163346
Epoch 64/100: Training Loss: 3.40247145769271e-05
Epoch 65/100: Training Loss: 2.5757556696507063e-05
Epoch 66/100: Training Loss: 3.272428981621157e-05
Epoch 67/100: Training Loss: 4.956938580355861e-05
Epoch 68/100: Training Loss: 4.346449436111884e-05
Epoch 69/100: Training Loss: 3.866289572959597e-05
Epoch 70/100: Training Loss: 1.1826198632744225e-05
Epoch 71/100: Training Loss: 1.4862230851907622e-05
Epoch 72/100: Training Loss: 2.0594122311608358e-05
Epoch 73/100: Training Loss: 1.111717958172614e-05
Epoch 74/100: Training Loss: 3.265585454011505e-05
Epoch 75/100: Training Loss: 1.5488250011747532e-05
Epoch 76/100: Training Loss: 3.364515829492699e-05
Epoch 77/100: Training Loss: 1.2702097989280115e-05
Epoch 78/100: Training Loss: 8.753366472030228e-06
Epoch 79/100: Training Loss: 2.968110652132468e-05
Epoch 80/100: Training Loss: 3.078588580882008e-05
Epoch 81/100: Training Loss: 9.961732747879895e-05
Epoch 82/100: Training Loss: 3.4095382910560476e-05
Epoch 83/100: Training Loss: 1.4758050780404697e-05
Epoch 84/100: Training Loss: 2.275406467643651e-05
Epoch 85/100: Training Loss: 1.688147471709685e-05
Epoch 86/100: Training Loss: 3.250824837860736e-05
Epoch 87/100: Training Loss: 2.5918182324279438e-05
Epoch 88/100: Training Loss: 3.283110404895111e-05
Epoch 89/100: Training Loss: 1.928542571311647e-05
Epoch 90/100: Training Loss: 1.1126216585663233e-05
Epoch 91/100: Training Loss: 2.7903074144639752e-05
Epoch 92/100: Training Loss: 3.0212684280493042e-05
Epoch 93/100: Training Loss: 0.0007149890742518685
Epoch 94/100: Training Loss: 1.5218204564668916e-05
Epoch 95/100: Training Loss: 9.390890640629964e-06
Epoch 96/100: Training Loss: 1.0607516477731141e-05
Epoch 97/100: Training Loss: 1.3006578030234033e-05
Epoch 98/100: Training Loss: 1.419287132607265e-05
Epoch 99/100: Training Loss: 1.0058838366107508e-05
Epoch 0/100: Training Loss: 0.0006406191169940931
Epoch 1/100: Training Loss: 0.0006342090769297516
Epoch 2/100: Training Loss: 0.0005658724203637119
Epoch 3/100: Training Loss: 0.0005550843916730398
Epoch 4/100: Training Loss: 0.0006619551764105872
Epoch 5/100: Training Loss: 0.0005269338321026569
Epoch 6/100: Training Loss: 0.0005818073238645281
Epoch 7/100: Training Loss: 0.0005699695873370368
Epoch 8/100: Training Loss: 0.0005167273607122184
Epoch 9/100: Training Loss: 0.0006471537225257416
Epoch 10/100: Training Loss: 0.0004987045779206237
Epoch 11/100: Training Loss: 0.0005515569366068335
Epoch 12/100: Training Loss: 0.0005510412328254242
Epoch 13/100: Training Loss: 0.0005430192579322147
Epoch 14/100: Training Loss: 0.0005634548614651376
Epoch 15/100: Training Loss: 0.0005147727335103646
Epoch 16/100: Training Loss: 0.0005094357487243442
Epoch 17/100: Training Loss: 0.0005024405119056526
Epoch 18/100: Training Loss: 0.0005677810050375451
Epoch 19/100: Training Loss: 0.0006335725158040973
Epoch 20/100: Training Loss: 0.00047534233826096705
Epoch 21/100: Training Loss: 0.0005061346951717605
Epoch 22/100: Training Loss: 0.0005491364249435987
Epoch 23/100: Training Loss: 0.00041292903060737295
Epoch 24/100: Training Loss: 0.0005214370890147125
Epoch 25/100: Training Loss: 0.00047475473809352117
Epoch 26/100: Training Loss: 0.0005033070109956275
Epoch 27/100: Training Loss: 0.000325448090030301
Epoch 28/100: Training Loss: 4.344227539229503e-05
Epoch 29/100: Training Loss: 0.00013935395992845985
Epoch 30/100: Training Loss: 3.4984999469348364e-05
Epoch 31/100: Training Loss: 2.250988637247393e-05
Epoch 32/100: Training Loss: 0.00020890425427168746
Epoch 33/100: Training Loss: 2.510241803623015e-05
Epoch 34/100: Training Loss: 1.406694401896769e-05
Epoch 35/100: Training Loss: 3.415678331654193e-05
Epoch 36/100: Training Loss: 1.8546417836220035e-05
Epoch 37/100: Training Loss: 9.610939959776566e-06
Epoch 38/100: Training Loss: 1.1727125877471564e-05
Epoch 39/100: Training Loss: 4.645780346909976e-06
Epoch 40/100: Training Loss: 7.72681001419296e-06
Epoch 41/100: Training Loss: 6.871788509293086e-05
Epoch 42/100: Training Loss: 1.783603222929113e-05
Epoch 43/100: Training Loss: 7.040348560613696e-06
Epoch 44/100: Training Loss: 7.348169448188923e-06
Epoch 45/100: Training Loss: 1.3474797752733054e-05
Epoch 46/100: Training Loss: 6.035085625877853e-06
Epoch 47/100: Training Loss: 1.8094295060717016e-05
Epoch 48/100: Training Loss: 3.6124784868502396e-06
Epoch 49/100: Training Loss: 1.2068615470957098e-05
Epoch 50/100: Training Loss: 1.5576284224261885e-05
Epoch 51/100: Training Loss: 5.2163634912758926e-06
Epoch 52/100: Training Loss: 7.619532490415233e-06
Epoch 53/100: Training Loss: 3.297950145423687e-05
Epoch 54/100: Training Loss: 1.1028833515633086e-05
Epoch 55/100: Training Loss: 0.0001468085748259373
Epoch 56/100: Training Loss: 1.0675735651485382e-05
Epoch 57/100: Training Loss: 9.675241989611481e-06
Epoch 58/100: Training Loss: 1.0754626613402148e-05
Epoch 59/100: Training Loss: 6.060457191846338e-06
Epoch 60/100: Training Loss: 1.674002542885767e-05
Epoch 61/100: Training Loss: 1.965834617546077e-05
Epoch 62/100: Training Loss: 2.338523088100319e-05
Epoch 63/100: Training Loss: 1.9506323859438917e-05
Epoch 64/100: Training Loss: 2.2008233974056858e-05
Epoch 65/100: Training Loss: 1.107039003019234e-05
Epoch 66/100: Training Loss: 1.536679887620535e-05
Epoch 67/100: Training Loss: 5.874864774228241e-05
Epoch 68/100: Training Loss: 3.4840672367042114e-05
Epoch 69/100: Training Loss: 1.1741639702894172e-05
Epoch 70/100: Training Loss: 1.1824225912445701e-05
Epoch 71/100: Training Loss: 1.9832836112119084e-05
Epoch 72/100: Training Loss: 9.34832931114232e-05
Epoch 73/100: Training Loss: 1.9315179557569565e-05
Epoch 74/100: Training Loss: 0.00039560825044658327
Epoch 75/100: Training Loss: 3.7949214676558145e-05
Epoch 76/100: Training Loss: 5.458189766802546e-05
Epoch 77/100: Training Loss: 1.7043345818115818e-05
Epoch 78/100: Training Loss: 9.471646076591882e-06
Epoch 79/100: Training Loss: 4.147912465756939e-05
Epoch 80/100: Training Loss: 1.6930420810039144e-05
Epoch 81/100: Training Loss: 2.72819210135717e-05
Epoch 82/100: Training Loss: 1.145852515087699e-05
Epoch 83/100: Training Loss: 2.956336089474265e-05
Epoch 84/100: Training Loss: 2.3858789097054213e-05
Epoch 85/100: Training Loss: 3.9699998113416855e-05
Epoch 86/100: Training Loss: 9.346856362259333e-05
Epoch 87/100: Training Loss: 1.418710072537721e-05
Epoch 88/100: Training Loss: 3.0799288182489334e-05
Epoch 89/100: Training Loss: 1.607308473249185e-05
Epoch 90/100: Training Loss: 1.4379709607101805e-05
Epoch 91/100: Training Loss: 2.1195442583154424e-05
Epoch 92/100: Training Loss: 1.0068531573024763e-05
Epoch 93/100: Training Loss: 5.899938381349986e-05
Epoch 94/100: Training Loss: 8.86339479671096e-06
Epoch 95/100: Training Loss: 1.94794936148527e-05
Epoch 96/100: Training Loss: 1.2408910868560664e-05
Epoch 97/100: Training Loss: 9.224310191133604e-06
Epoch 98/100: Training Loss: 8.500515363149105e-06
Epoch 99/100: Training Loss: 1.7346388312734767e-05
Epoch 0/100: Training Loss: 0.0007653082762144308
Epoch 1/100: Training Loss: 0.00036826597905791964
Epoch 2/100: Training Loss: 0.0005915427366189197
Epoch 3/100: Training Loss: 0.0006496136167408091
Epoch 4/100: Training Loss: 0.0005805723830661942
Epoch 5/100: Training Loss: 0.000461501852337238
Epoch 6/100: Training Loss: 0.00038518377505572494
Epoch 7/100: Training Loss: 0.0003875681256825945
Epoch 8/100: Training Loss: 0.00043862814133146166
Epoch 9/100: Training Loss: 0.0003154547143298968
Epoch 10/100: Training Loss: 0.000345194959535008
Epoch 11/100: Training Loss: 0.00039572367625953877
Epoch 12/100: Training Loss: 0.0003888877355946904
Epoch 13/100: Training Loss: 0.0004431378854587015
Epoch 14/100: Training Loss: 0.0005216008068713466
Epoch 15/100: Training Loss: 0.0005108221244495527
Epoch 16/100: Training Loss: 0.0004602318909843411
Epoch 17/100: Training Loss: 0.0005099265712552366
Epoch 18/100: Training Loss: 0.0004912197458005585
Epoch 19/100: Training Loss: 0.0005118811948109517
Epoch 20/100: Training Loss: 0.000494031449862286
Epoch 21/100: Training Loss: 0.00046401134634439925
Epoch 22/100: Training Loss: 0.0005188424107247749
Epoch 23/100: Training Loss: 0.0003896828048524603
Epoch 24/100: Training Loss: 0.0005102903639848253
Epoch 25/100: Training Loss: 0.0004267580477537307
Epoch 26/100: Training Loss: 0.0006622248782520801
Epoch 27/100: Training Loss: 0.00039967373672839814
Epoch 28/100: Training Loss: 0.00046848696944987877
Epoch 29/100: Training Loss: 0.0004902081811322575
Epoch 30/100: Training Loss: 0.0004093229968463425
Epoch 31/100: Training Loss: 0.00039712415464156496
Epoch 32/100: Training Loss: 0.0006133288936277406
Epoch 33/100: Training Loss: 0.0003754409211399281
Epoch 34/100: Training Loss: 0.0004011294680886564
Epoch 35/100: Training Loss: 0.00037336365970890077
Epoch 36/100: Training Loss: 0.0004470406380374875
Epoch 37/100: Training Loss: 0.00029910744818965944
Epoch 38/100: Training Loss: 0.0003959365782484544
Epoch 39/100: Training Loss: 0.0004832254046887423
Epoch 40/100: Training Loss: 0.0004176219875833629
Epoch 41/100: Training Loss: 0.0005479803786868542
Epoch 42/100: Training Loss: 0.00043343393280442837
Epoch 43/100: Training Loss: 0.00011580462971356062
Epoch 44/100: Training Loss: 0.0005115697209813954
Epoch 45/100: Training Loss: 0.0004888979998310055
Epoch 46/100: Training Loss: 0.0005685647504519572
Epoch 47/100: Training Loss: 0.0006091627003872289
Epoch 48/100: Training Loss: 0.0004599103951348668
Epoch 49/100: Training Loss: 0.0003969383846342036
Epoch 50/100: Training Loss: 0.00044163382422607555
Epoch 51/100: Training Loss: 0.0004928246206414383
Epoch 52/100: Training Loss: 0.00048455180583801947
Epoch 53/100: Training Loss: 0.0005342319805537705
Epoch 54/100: Training Loss: 0.0004948774863660863
Epoch 55/100: Training Loss: 0.0004078008696041276
Epoch 56/100: Training Loss: 0.00044501780540542266
Epoch 57/100: Training Loss: 0.00044371596479837874
Epoch 58/100: Training Loss: 0.00040209777982889023
Epoch 59/100: Training Loss: 0.00048542362266937186
Epoch 60/100: Training Loss: 0.0007076377494145284
Epoch 61/100: Training Loss: 0.00040586401535346446
Epoch 62/100: Training Loss: 0.0004530981578658112
Epoch 63/100: Training Loss: 0.0005074678137239101
Epoch 64/100: Training Loss: 0.0004351944456585741
Epoch 65/100: Training Loss: 0.00021569190167747767
Epoch 66/100: Training Loss: 0.0004797847529428195
Epoch 67/100: Training Loss: 0.0004644128205501928
Epoch 68/100: Training Loss: 0.00032536876676356896
Epoch 69/100: Training Loss: 0.0005356631843389663
Epoch 70/100: Training Loss: 0.0004754015038498735
Epoch 71/100: Training Loss: 0.00039332742448401663
Epoch 72/100: Training Loss: 0.00043192205835232693
Epoch 73/100: Training Loss: 0.00042906818927916803
Epoch 74/100: Training Loss: 0.0005064309962027896
Epoch 75/100: Training Loss: 0.00038130726434488214
Epoch 76/100: Training Loss: 0.0003074280983578842
Epoch 77/100: Training Loss: 0.00041710357882280267
Epoch 78/100: Training Loss: 0.00038445493684405774
Epoch 79/100: Training Loss: 0.00019897345816139627
Epoch 80/100: Training Loss: 0.0004336313402230761
Epoch 81/100: Training Loss: 0.0004048882200654629
Epoch 82/100: Training Loss: 0.0003433700617963234
Epoch 83/100: Training Loss: 0.00023525274169128554
Epoch 84/100: Training Loss: 9.264190376332376e-05
Epoch 85/100: Training Loss: 0.0003249872376960991
Epoch 86/100: Training Loss: 0.00030226616469104735
Epoch 87/100: Training Loss: 4.022692297214428e-06
Epoch 88/100: Training Loss: 8.614462660213488e-05
Epoch 89/100: Training Loss: 1.2946223419786026e-05
Epoch 90/100: Training Loss: 0.0004951314324826265
Epoch 91/100: Training Loss: 0.0002426494374475648
Epoch 92/100: Training Loss: 0.0002777695787691437
Epoch 93/100: Training Loss: 0.00040537939794295656
Epoch 94/100: Training Loss: 0.00038146079245921785
Epoch 95/100: Training Loss: 0.00026956444556734205
Epoch 96/100: Training Loss: 0.0006316130140186411
Epoch 97/100: Training Loss: 5.871410671192988e-05
Epoch 98/100: Training Loss: 0.000285905085306252
Epoch 99/100: Training Loss: 0.00029163955982807464
Epoch 0/100: Training Loss: 0.0005708828635279907
Epoch 1/100: Training Loss: 0.0011562590107254919
Epoch 2/100: Training Loss: 0.0011438222209434338
Epoch 3/100: Training Loss: 0.0008655815247463004
Epoch 4/100: Training Loss: 0.0005529301717142354
Epoch 5/100: Training Loss: 0.0003098155885533902
Epoch 6/100: Training Loss: 0.0004168994758161194
Epoch 7/100: Training Loss: 0.0005899482644726878
Epoch 8/100: Training Loss: 0.0004568191186729568
Epoch 9/100: Training Loss: 0.0008370641101101588
Epoch 10/100: Training Loss: 0.0004752953891796916
Epoch 11/100: Training Loss: 0.00035733261851451855
Epoch 12/100: Training Loss: 0.0001444920830662475
Epoch 13/100: Training Loss: 0.0002276548957076308
Epoch 14/100: Training Loss: 0.00048682444432391177
Epoch 15/100: Training Loss: 0.0008240500506798783
Epoch 16/100: Training Loss: 0.00016402023856949914
Epoch 17/100: Training Loss: 9.270859339312054e-05
Epoch 18/100: Training Loss: 0.00019240903881098656
Epoch 19/100: Training Loss: 9.252608757917123e-05
Epoch 20/100: Training Loss: 0.00022294439141525815
Epoch 21/100: Training Loss: 6.212115772235555e-05
Epoch 22/100: Training Loss: 3.4077399965759886e-05
Epoch 23/100: Training Loss: 0.00037220969061145867
Epoch 24/100: Training Loss: 7.607094576005978e-05
Epoch 25/100: Training Loss: 5.588592802729842e-05
Epoch 26/100: Training Loss: 0.00018008006527819441
Epoch 27/100: Training Loss: 8.40911449605574e-05
Epoch 28/100: Training Loss: 9.246271588075322e-05
Epoch 29/100: Training Loss: 4.111691440702019e-05
Epoch 30/100: Training Loss: 9.639562846833815e-05
Epoch 31/100: Training Loss: 0.00014430057307529876
Epoch 32/100: Training Loss: 8.374435885604722e-05
Epoch 33/100: Training Loss: 0.0002688479196330357
Epoch 34/100: Training Loss: 7.373939848801481e-05
Epoch 35/100: Training Loss: 4.7911376161960205e-05
Epoch 36/100: Training Loss: 2.8153067045414928e-05
Epoch 37/100: Training Loss: 3.105540020650278e-05
Epoch 38/100: Training Loss: 3.80319313484457e-05
Epoch 39/100: Training Loss: 0.00017193394124240618
Epoch 40/100: Training Loss: 1.0706254619386699e-05
Epoch 41/100: Training Loss: 7.723091637941219e-06
Epoch 42/100: Training Loss: 7.787362292342121e-06
Epoch 43/100: Training Loss: 2.0318789900418353e-05
Epoch 44/100: Training Loss: 1.208239857257749e-05
Epoch 45/100: Training Loss: 1.3367434746414557e-05
Epoch 46/100: Training Loss: 7.727877508364451e-05
Epoch 47/100: Training Loss: 1.4875109219764915e-05
Epoch 48/100: Training Loss: 9.201125621862476e-06
Epoch 49/100: Training Loss: 3.4506883756194945e-05
Epoch 50/100: Training Loss: 2.7879302059989338e-05
Epoch 51/100: Training Loss: 7.556165639528245e-06
Epoch 52/100: Training Loss: 1.4144822854899506e-05
Epoch 53/100: Training Loss: 5.931430078641984e-06
Epoch 54/100: Training Loss: 8.403528231141813e-06
Epoch 55/100: Training Loss: 1.2411587692028738e-05
Epoch 56/100: Training Loss: 1.676509474224574e-05
Epoch 57/100: Training Loss: 0.00010578258322226093
Epoch 58/100: Training Loss: 1.4039842336701706e-05
Epoch 59/100: Training Loss: 6.126383979357946e-06
Epoch 60/100: Training Loss: 2.6639060205007348e-05
Epoch 61/100: Training Loss: 4.254392611338953e-05
Epoch 62/100: Training Loss: 1.1878873267515891e-06
Epoch 63/100: Training Loss: 9.743935519134033e-08
Epoch 64/100: Training Loss: 7.145755820920181e-07
Epoch 65/100: Training Loss: 2.082481258862021e-06
Epoch 66/100: Training Loss: 4.524107178831849e-06
Epoch 67/100: Training Loss: 1.4184768449498399e-05
Epoch 68/100: Training Loss: 1.253554965014415e-05
Epoch 69/100: Training Loss: 1.8669851959553534e-05
Epoch 70/100: Training Loss: 0.0005240352164469492
Epoch 71/100: Training Loss: 3.638387565822612e-06
Epoch 72/100: Training Loss: 1.8787018499533425e-06
Epoch 73/100: Training Loss: 8.700274932302635e-07
Epoch 74/100: Training Loss: 1.3192218202145377e-06
Epoch 75/100: Training Loss: 9.617565675834904e-07
Epoch 76/100: Training Loss: 8.738127269786301e-06
Epoch 77/100: Training Loss: 3.6969933488652644e-07
Epoch 78/100: Training Loss: 5.636136872368142e-07
Epoch 79/100: Training Loss: 4.405489884684439e-06
Epoch 80/100: Training Loss: 4.4003608239806286e-07
Epoch 81/100: Training Loss: 1.718054317781543e-06
Epoch 82/100: Training Loss: 1.2944941356728028e-06
Epoch 83/100: Training Loss: 2.355237594271087e-07
Epoch 84/100: Training Loss: 2.2695372544120447e-07
Epoch 85/100: Training Loss: 5.260152252069403e-07
Epoch 86/100: Training Loss: 4.163144376638425e-07
Epoch 87/100: Training Loss: 8.925222744747596e-07
Epoch 88/100: Training Loss: 1.9783960097268926e-07
Epoch 89/100: Training Loss: 2.4319960106155864e-07
Epoch 90/100: Training Loss: 4.1503798020293625e-07
Epoch 91/100: Training Loss: 1.2926812150052043e-07
Epoch 92/100: Training Loss: 5.166298598092835e-07
Epoch 93/100: Training Loss: 1.011241888815345e-06
Epoch 94/100: Training Loss: 1.9988474867310826e-07
Epoch 95/100: Training Loss: 5.004348506370033e-07
Epoch 96/100: Training Loss: 6.100625523278331e-07
Epoch 97/100: Training Loss: 1.4818995076987575e-07
Epoch 98/100: Training Loss: 1.3583659501370427e-06
Epoch 99/100: Training Loss: 1.3634111089350013e-07
dataset: cities layer_num_from_end:-1 Avg_acc:tensor(0.7191) Avg_AUC:0.7961666479265053 Avg_threshold:0.9392133951187134
dataset: inventions layer_num_from_end:-1 Avg_acc:tensor(0.7135) Avg_AUC:0.7985750753264145 Avg_threshold:0.9125617742538452
dataset: elements layer_num_from_end:-1 Avg_acc:tensor(0.6376) Avg_AUC:0.7205526650479823 Avg_threshold:0.9817885160446167
dataset: animals layer_num_from_end:-1 Avg_acc:tensor(0.6002) Avg_AUC:0.736189846308894 Avg_threshold:0.9628689885139465
dataset: companies layer_num_from_end:-1 Avg_acc:tensor(0.6183) Avg_AUC:0.6885111111111112 Avg_threshold:0.3437240719795227
dataset: facts layer_num_from_end:-1 Avg_acc:tensor(0.7451) Avg_AUC:0.8324522619505318 Avg_threshold:0.6411822438240051
dataset: conj_neg_facts layer_num_from_end:-1 Avg_acc:tensor(0.7308) Avg_AUC:0.5487035393867692 Avg_threshold:0.8374865651130676


================layer -5================
Epoch 0/100: Training Loss: 0.00349541275929182
Epoch 1/100: Training Loss: 0.0026820447200383893
Epoch 2/100: Training Loss: 0.0031750118121122704
Epoch 3/100: Training Loss: 0.0024852494780833903
Epoch 4/100: Training Loss: 0.003353649989152566
Epoch 5/100: Training Loss: 0.0025012986018107487
Epoch 6/100: Training Loss: 0.0024706493967618696
Epoch 7/100: Training Loss: 0.0024650969948524083
Epoch 8/100: Training Loss: 0.0021588819531294014
Epoch 9/100: Training Loss: 0.0024030848573415708
Epoch 10/100: Training Loss: 0.002284092398790213
Epoch 11/100: Training Loss: 0.0015872172438181364
Epoch 12/100: Training Loss: 0.0022017485820330107
Epoch 13/100: Training Loss: 0.0015596386331778306
Epoch 14/100: Training Loss: 0.0016255237353153718
Epoch 15/100: Training Loss: 0.0013293043161049867
Epoch 16/100: Training Loss: 0.0018682346130028749
Epoch 17/100: Training Loss: 0.0019728759160408606
Epoch 18/100: Training Loss: 0.0008900100604081765
Epoch 19/100: Training Loss: 0.0015403980819078593
Epoch 20/100: Training Loss: 0.00042449119381415536
Epoch 21/100: Training Loss: 0.0002633029212936377
Epoch 22/100: Training Loss: 0.004232474626638951
Epoch 23/100: Training Loss: 0.00022210534184406965
Epoch 24/100: Training Loss: 0.00012202879700523156
Epoch 25/100: Training Loss: 0.0016010611867293334
Epoch 26/100: Training Loss: 0.0001188469573091238
Epoch 27/100: Training Loss: 0.00020882181632213102
Epoch 28/100: Training Loss: 0.00015890757099558145
Epoch 29/100: Training Loss: 0.00021839034385406054
Epoch 30/100: Training Loss: 0.00039314430875655933
Epoch 31/100: Training Loss: 0.0023134746230565584
Epoch 32/100: Training Loss: 0.0002617559944972014
Epoch 33/100: Training Loss: 0.0001770986291842583
Epoch 34/100: Training Loss: 0.00017528508145075577
Epoch 35/100: Training Loss: 0.0005985934955951495
Epoch 36/100: Training Loss: 6.115864007136761e-05
Epoch 37/100: Training Loss: 0.0003164971295075539
Epoch 38/100: Training Loss: 0.0007132159975858835
Epoch 39/100: Training Loss: 6.77560456097126e-05
Epoch 40/100: Training Loss: 0.00013643309760552185
Epoch 41/100: Training Loss: 3.654840521705456e-05
Epoch 42/100: Training Loss: 4.740111016405698e-05
Epoch 43/100: Training Loss: 0.0017103633055320154
Epoch 44/100: Training Loss: 0.0020258262371405577
Epoch 45/100: Training Loss: 1.2481713500351478e-05
Epoch 46/100: Training Loss: 0.00022288048878694192
Epoch 47/100: Training Loss: 5.239915126600327e-05
Epoch 48/100: Training Loss: 0.00015193195297167852
Epoch 49/100: Training Loss: 7.614242032361336e-05
Epoch 50/100: Training Loss: 7.31532211200549e-05
Epoch 51/100: Training Loss: 1.5654338475985405e-05
Epoch 52/100: Training Loss: 7.47332516580056e-05
Epoch 53/100: Training Loss: 6.447684240694612e-06
Epoch 54/100: Training Loss: 5.15160335896489e-06
Epoch 55/100: Training Loss: 0.0016387196687551646
Epoch 56/100: Training Loss: 1.5521060842543076e-05
Epoch 57/100: Training Loss: 8.271333242121797e-06
Epoch 58/100: Training Loss: 3.945139738229605e-05
Epoch 59/100: Training Loss: 4.2027799794689206e-05
Epoch 60/100: Training Loss: 8.09821992730483e-05
Epoch 61/100: Training Loss: 3.3136367654571164e-05
Epoch 62/100: Training Loss: 3.270700108259916e-05
Epoch 63/100: Training Loss: 0.00012913914636159554
Epoch 64/100: Training Loss: 6.683123632310293e-05
Epoch 65/100: Training Loss: 5.855478752308931e-05
Epoch 66/100: Training Loss: 7.159782585520775e-06
Epoch 67/100: Training Loss: 1.3358674597186156e-05
Epoch 68/100: Training Loss: 6.941953100837194e-06
Epoch 69/100: Training Loss: 2.2863103554416925e-06
Epoch 70/100: Training Loss: 6.2160259590317046e-06
Epoch 71/100: Training Loss: 2.6732122000211326e-05
Epoch 72/100: Training Loss: 3.7331748246334682e-06
Epoch 73/100: Training Loss: 1.797198097054393e-06
Epoch 74/100: Training Loss: 1.0381390800317511e-05
Epoch 75/100: Training Loss: 1.9301360365576467e-05
Epoch 76/100: Training Loss: 0.002683707918876257
Epoch 77/100: Training Loss: 2.2259264369495213e-06
Epoch 78/100: Training Loss: 0.003400480900055323
Epoch 79/100: Training Loss: 2.3220502323685932e-06
Epoch 80/100: Training Loss: 5.65110232669096e-06
Epoch 81/100: Training Loss: 2.19834775914653e-06
Epoch 82/100: Training Loss: 5.161816937825046e-06
Epoch 83/100: Training Loss: 1.8595243231035194e-05
Epoch 84/100: Training Loss: 4.100843514793385e-06
Epoch 85/100: Training Loss: 2.5364016227495785e-07
Epoch 86/100: Training Loss: 4.019003170423019e-06
Epoch 87/100: Training Loss: 6.11561699770391e-06
Epoch 88/100: Training Loss: 3.651207169661155e-05
Epoch 89/100: Training Loss: 4.9658433104363775e-06
Epoch 90/100: Training Loss: 6.4920675821411305e-06
Epoch 91/100: Training Loss: 3.744032045897956e-06
Epoch 92/100: Training Loss: 2.4132237647277993e-06
Epoch 93/100: Training Loss: 2.2929891248424656e-06
Epoch 94/100: Training Loss: 3.824525032168589e-06
Epoch 95/100: Training Loss: 2.8143743596350155e-06
Epoch 96/100: Training Loss: 7.472734955043937e-07
Epoch 97/100: Training Loss: 1.3635857239699899e-06
Epoch 98/100: Training Loss: 7.43044597001221e-06
Epoch 99/100: Training Loss: 1.7431684029407992e-05
Epoch 0/100: Training Loss: 0.00017113373886309396
Epoch 1/100: Training Loss: 2.6222986624379862e-05
Epoch 2/100: Training Loss: 6.22117580770778e-06
Epoch 3/100: Training Loss: 5.592381677719777e-06
Epoch 4/100: Training Loss: 1.533357885920948e-06
Epoch 5/100: Training Loss: 7.440779784134685e-06
Epoch 6/100: Training Loss: 6.654414122782213e-06
Epoch 7/100: Training Loss: 1.7011999632879223e-05
Epoch 8/100: Training Loss: 9.175427592121432e-06
Epoch 9/100: Training Loss: 9.127404966763317e-06
Epoch 10/100: Training Loss: 1.119739572296228e-05
Epoch 11/100: Training Loss: 5.041147298242212e-06
Epoch 12/100: Training Loss: 3.7122990272003707e-06
Epoch 13/100: Training Loss: 1.4237040821400223e-07
Epoch 14/100: Training Loss: 6.255461425090317e-06
Epoch 15/100: Training Loss: 7.700875104977145e-06
Epoch 16/100: Training Loss: 3.704248881593948e-06
Epoch 17/100: Training Loss: 1.5214125254095402e-06
Epoch 18/100: Training Loss: 4.938186886052145e-07
Epoch 19/100: Training Loss: 4.0162943440360007e-07
Epoch 20/100: Training Loss: 1.4986364680421727e-05
Epoch 21/100: Training Loss: 2.539209236344949e-06
Epoch 22/100: Training Loss: 9.550444803706e-07
Epoch 23/100: Training Loss: 1.7120018303853714e-07
Epoch 24/100: Training Loss: 1.5001910367475263e-07
Epoch 25/100: Training Loss: 1.3150602334801258e-06
Epoch 26/100: Training Loss: 7.025925693156473e-08
Epoch 27/100: Training Loss: 1.0511915944934293e-06
Epoch 28/100: Training Loss: 1.503412687217591e-08
Epoch 29/100: Training Loss: 2.019343464740047e-09
Epoch 30/100: Training Loss: 1.1909848891853456e-10
Epoch 31/100: Training Loss: 8.328663880337497e-08
Epoch 32/100: Training Loss: 1.9352189111189224e-07
Epoch 33/100: Training Loss: 2.0422043634845872e-07
Epoch 34/100: Training Loss: 6.237923169734099e-07
Epoch 35/100: Training Loss: 2.7290846165290496e-07
Epoch 36/100: Training Loss: 3.6306428146703093e-07
Epoch 37/100: Training Loss: 4.927519614382275e-07
Epoch 38/100: Training Loss: 1.213015808886744e-06
Epoch 39/100: Training Loss: 9.088018217913372e-07
Epoch 40/100: Training Loss: 9.190573068878575e-06
Epoch 41/100: Training Loss: 2.833580255775708e-07
Epoch 42/100: Training Loss: 2.582468687047171e-07
Epoch 43/100: Training Loss: 7.244817820699226e-07
Epoch 44/100: Training Loss: 8.765731959326e-08
Epoch 45/100: Training Loss: 9.564278656553509e-10
Epoch 46/100: Training Loss: 9.58289168994013e-10
Epoch 47/100: Training Loss: 2.088217780457584e-07
Epoch 48/100: Training Loss: 1.0485984823528695e-06
Epoch 49/100: Training Loss: 2.959992678109306e-07
Epoch 50/100: Training Loss: 9.051116714316667e-08
Epoch 51/100: Training Loss: 0.00012020948823257412
Epoch 52/100: Training Loss: 1.5540221928422562e-07
Epoch 53/100: Training Loss: 4.320329756129088e-09
Epoch 54/100: Training Loss: 7.642897533987871e-07
Epoch 55/100: Training Loss: 4.3349362614318194e-07
Epoch 56/100: Training Loss: 3.5449295542766693e-06
Epoch 57/100: Training Loss: 7.08286805525255e-07
Epoch 58/100: Training Loss: 2.5239923442693034e-05
Epoch 59/100: Training Loss: 8.131754443659176e-07
Epoch 60/100: Training Loss: 4.1087756806257864e-07
Epoch 61/100: Training Loss: 6.893083020927794e-07
Epoch 62/100: Training Loss: 3.08119634679527e-08
Epoch 63/100: Training Loss: 3.1194780799748185e-07
Epoch 64/100: Training Loss: 7.3095170714195e-07
Epoch 65/100: Training Loss: 3.030976333296252e-07
Epoch 66/100: Training Loss: 3.321311839087762e-07
Epoch 67/100: Training Loss: 1.5354975264502748e-05
Epoch 68/100: Training Loss: 3.1255579745655906e-06
Epoch 69/100: Training Loss: 1.4675798358170174e-06
Epoch 70/100: Training Loss: 1.1155291028725894e-07
Epoch 71/100: Training Loss: 3.5382001744290906e-06
Epoch 72/100: Training Loss: 5.171834280418917e-08
Epoch 73/100: Training Loss: 1.6726566491139042e-06
Epoch 74/100: Training Loss: 2.260016680187692e-07
Epoch 75/100: Training Loss: 4.2881322774050484e-07
Epoch 76/100: Training Loss: 1.1891222329478244e-07
Epoch 77/100: Training Loss: 1.4429344707419219e-07
Epoch 78/100: Training Loss: 6.458658104837609e-07
Epoch 79/100: Training Loss: 4.986326726108597e-08
Epoch 80/100: Training Loss: 4.281177049222916e-07
Epoch 81/100: Training Loss: 1.5211832185480492e-06
Epoch 82/100: Training Loss: 4.3192221506419404e-07
Epoch 83/100: Training Loss: 2.5504354842742194e-06
Epoch 84/100: Training Loss: 5.574818494867517e-07
Epoch 85/100: Training Loss: 1.1224561897388372e-07
Epoch 86/100: Training Loss: 5.074167682062584e-07
Epoch 87/100: Training Loss: 3.631219283408525e-09
Epoch 88/100: Training Loss: 5.446095711284321e-07
Epoch 89/100: Training Loss: 1.4579142318545704e-06
Epoch 90/100: Training Loss: 2.1777654052586733e-07
Epoch 91/100: Training Loss: 6.539385996533415e-07
Epoch 92/100: Training Loss: 1.2583927263927567e-06
Epoch 93/100: Training Loss: 0.00012697854952160018
Epoch 94/100: Training Loss: 0.00012563581624373192
Epoch 95/100: Training Loss: 0.000124476031471261
Epoch 96/100: Training Loss: 0.00012109507159267305
Epoch 97/100: Training Loss: 2.0713413109681299e-07
Epoch 98/100: Training Loss: 1.3781688228299666e-07
Epoch 99/100: Training Loss: 1.052012831503066e-06
Epoch 0/100: Training Loss: 0.00037756912848528693
Epoch 1/100: Training Loss: 0.0004176067955353681
Epoch 2/100: Training Loss: 0.0004678837853858913
Epoch 3/100: Training Loss: 0.0004057871405355531
Epoch 4/100: Training Loss: 0.00019460309684546286
Epoch 5/100: Training Loss: 0.00041504736939167007
Epoch 6/100: Training Loss: 0.0003899596800092119
Epoch 7/100: Training Loss: 0.00010308330010504744
Epoch 8/100: Training Loss: 0.00013266044470789206
Epoch 9/100: Training Loss: 0.00023490883308837856
Epoch 10/100: Training Loss: 0.00016830591864175926
Epoch 11/100: Training Loss: 0.00040298740788282853
Epoch 12/100: Training Loss: 0.00020079415847812842
Epoch 13/100: Training Loss: 0.00012548654698408567
Epoch 14/100: Training Loss: 0.00011193705090570234
Epoch 15/100: Training Loss: 9.652528473559547e-06
Epoch 16/100: Training Loss: 0.00015157341485109803
Epoch 17/100: Training Loss: 6.37370705334849e-05
Epoch 18/100: Training Loss: 7.111226878554573e-05
Epoch 19/100: Training Loss: 4.372564544774828e-05
Epoch 20/100: Training Loss: 2.8122494595622585e-05
Epoch 21/100: Training Loss: 2.9630933749190282e-05
Epoch 22/100: Training Loss: 2.7165497438265727e-05
Epoch 23/100: Training Loss: 0.00016126839014200063
Epoch 24/100: Training Loss: 0.0004091696029874534
Epoch 25/100: Training Loss: 0.0002934050101500291
Epoch 26/100: Training Loss: 3.406640228592028e-06
Epoch 27/100: Training Loss: 1.2237153003388401e-05
Epoch 28/100: Training Loss: 0.00027228671029142664
Epoch 29/100: Training Loss: 0.0001486878656693713
Epoch 30/100: Training Loss: 9.642005371292253e-05
Epoch 31/100: Training Loss: 7.272154362492971e-05
Epoch 32/100: Training Loss: 0.00012335549551167638
Epoch 33/100: Training Loss: 0.00019418768483589138
Epoch 34/100: Training Loss: 0.00023703194514119248
Epoch 35/100: Training Loss: 0.00036713905733634986
Epoch 36/100: Training Loss: 7.767255557068872e-05
Epoch 37/100: Training Loss: 2.713055387813581e-05
Epoch 38/100: Training Loss: 0.00011040313312640556
Epoch 39/100: Training Loss: 0.000544308436132664
Epoch 40/100: Training Loss: 0.0001086805066371935
Epoch 41/100: Training Loss: 1.8956950903613103e-05
Epoch 42/100: Training Loss: 9.146554403984709e-05
Epoch 43/100: Training Loss: 0.0003426176795053266
Epoch 44/100: Training Loss: 0.0002761716486641724
Epoch 45/100: Training Loss: 3.4897710403166206e-05
Epoch 46/100: Training Loss: 0.00010575858109137592
Epoch 47/100: Training Loss: 0.0003694126980876491
Epoch 48/100: Training Loss: 8.313384589768643e-06
Epoch 49/100: Training Loss: 9.150597439632156e-05
Epoch 50/100: Training Loss: 5.153180584648615e-06
Epoch 51/100: Training Loss: 5.658238081101379e-05
Epoch 52/100: Training Loss: 5.789096066854658e-05
Epoch 53/100: Training Loss: 8.459247615002939e-05
Epoch 54/100: Training Loss: 0.00020800520796581632
Epoch 55/100: Training Loss: 5.399733697531989e-05
Epoch 56/100: Training Loss: 4.76072462300909e-05
Epoch 57/100: Training Loss: 3.6994066949074084e-05
Epoch 58/100: Training Loss: 3.576597274698283e-05
Epoch 59/100: Training Loss: 4.337784914274561e-05
Epoch 60/100: Training Loss: 7.40029865968551e-06
Epoch 61/100: Training Loss: 3.0442406486601853e-05
Epoch 62/100: Training Loss: 7.454898393801435e-05
Epoch 63/100: Training Loss: 0.00037926637748787306
Epoch 64/100: Training Loss: 5.0916778960379e-05
Epoch 65/100: Training Loss: 0.00024810015346130097
Epoch 66/100: Training Loss: 0.0013921842046452863
Epoch 67/100: Training Loss: 0.000219917202966785
Epoch 68/100: Training Loss: 3.087112671649294e-05
Epoch 69/100: Training Loss: 4.6267587168993456e-05
Epoch 70/100: Training Loss: 2.4873537866912817e-05
Epoch 71/100: Training Loss: 2.8669720355471873e-05
Epoch 72/100: Training Loss: 1.4124826920045986e-05
Epoch 73/100: Training Loss: 4.84600342911293e-05
Epoch 74/100: Training Loss: 6.31093085486425e-05
Epoch 75/100: Training Loss: 6.369790020167019e-05
Epoch 76/100: Training Loss: 6.367345403761885e-05
Epoch 77/100: Training Loss: 8.212829775669995e-05
Epoch 78/100: Training Loss: 6.935533506972757e-05
Epoch 79/100: Training Loss: 0.0001246700425762936
Epoch 80/100: Training Loss: 0.00015872201094260582
Epoch 81/100: Training Loss: 4.8284667514568e-05
Epoch 82/100: Training Loss: 6.740492224963002e-05
Epoch 83/100: Training Loss: 6.846459877437057e-05
Epoch 84/100: Training Loss: 3.407818025053896e-05
Epoch 85/100: Training Loss: 5.5811473282214206e-05
Epoch 86/100: Training Loss: 1.6491433470699583e-05
Epoch 87/100: Training Loss: 6.0286372900009155e-05
Epoch 88/100: Training Loss: 4.252041447739121e-06
Epoch 89/100: Training Loss: 6.006588965519521e-05
Epoch 90/100: Training Loss: 2.1264218299637012e-05
Epoch 91/100: Training Loss: 4.241209291764514e-05
Epoch 92/100: Training Loss: 1.0388157418950111e-05
Epoch 93/100: Training Loss: 0.00012193015055958502
Epoch 94/100: Training Loss: 6.348181229371291e-05
Epoch 95/100: Training Loss: 0.00012519687616447517
Epoch 96/100: Training Loss: 7.108394140841196e-05
Epoch 97/100: Training Loss: 6.270832571778362e-05
Epoch 98/100: Training Loss: 0.000121660244006377
Epoch 99/100: Training Loss: 0.00016376622257189514
Epoch 0/100: Training Loss: 0.000266882066022266
Epoch 1/100: Training Loss: 0.00016896184533834457
Epoch 2/100: Training Loss: 0.00011412745172327215
Epoch 3/100: Training Loss: 0.00026863586496223105
Epoch 4/100: Training Loss: 0.000589948757128282
Epoch 5/100: Training Loss: 0.00029048384590582414
Epoch 6/100: Training Loss: 0.0005026070909066633
Epoch 7/100: Training Loss: 0.00027280397374521603
Epoch 8/100: Training Loss: 0.00037042508748444646
Epoch 9/100: Training Loss: 0.000468490103429014
Epoch 10/100: Training Loss: 0.00044521879066120496
Epoch 11/100: Training Loss: 0.00022321089424870232
Epoch 12/100: Training Loss: 0.00015323673459616575
Epoch 13/100: Training Loss: 0.0001970778134736148
Epoch 14/100: Training Loss: 2.0403027619150552e-05
Epoch 15/100: Training Loss: 0.00012108424509113485
Epoch 16/100: Training Loss: 0.0001554370773109523
Epoch 17/100: Training Loss: 7.028444881804965e-06
Epoch 18/100: Training Loss: 0.00010258739983493632
Epoch 19/100: Training Loss: 1.2415759688751263e-05
Epoch 20/100: Training Loss: 1.3000633440573107e-05
Epoch 21/100: Training Loss: 0.0005832645026120273
Epoch 22/100: Training Loss: 0.0003116258504715833
Epoch 23/100: Training Loss: 0.00030086626383391294
Epoch 24/100: Training Loss: 0.00020334144884889775
Epoch 25/100: Training Loss: 4.131633111021736e-05
Epoch 26/100: Training Loss: 0.00021210143511945552
Epoch 27/100: Training Loss: 0.00022398395971818405
Epoch 28/100: Training Loss: 0.0001986073838038878
Epoch 29/100: Training Loss: 9.394890882752158e-05
Epoch 30/100: Training Loss: 1.5018390364606272e-05
Epoch 31/100: Training Loss: 1.5894431536170568e-05
Epoch 32/100: Training Loss: 1.0543608699332584e-05
Epoch 33/100: Training Loss: 7.104986368424513e-06
Epoch 34/100: Training Loss: 0.00020571510222825137
Epoch 35/100: Training Loss: 0.0002657756040042097
Epoch 36/100: Training Loss: 0.0001779663630507209
Epoch 37/100: Training Loss: 2.3820077661763538e-05
Epoch 38/100: Training Loss: 8.839187635616823e-05
Epoch 39/100: Training Loss: 2.5107584555040707e-05
Epoch 40/100: Training Loss: 9.040466763756493e-06
Epoch 41/100: Training Loss: 9.358186401765455e-06
Epoch 42/100: Training Loss: 1.5050728424367579e-05
Epoch 43/100: Training Loss: 7.777752629904585e-06
Epoch 44/100: Training Loss: 3.2723521475087512e-06
Epoch 45/100: Training Loss: 3.6722102033143695e-06
Epoch 46/100: Training Loss: 0.00012703841890801082
Epoch 47/100: Training Loss: 0.0001348848200657151
Epoch 48/100: Training Loss: 0.0001033183102580634
Epoch 49/100: Training Loss: 0.00011649135161529888
Epoch 50/100: Training Loss: 2.2568533578040927e-06
Epoch 51/100: Training Loss: 8.73396596447988e-05
Epoch 52/100: Training Loss: 4.776314811103723e-06
Epoch 53/100: Training Loss: 1.0987863457888702e-06
Epoch 54/100: Training Loss: 2.3173014845021746e-06
Epoch 55/100: Training Loss: 5.522153911773454e-06
Epoch 56/100: Training Loss: 0.00020561797375028785
Epoch 57/100: Training Loss: 0.0004775393415581096
Epoch 58/100: Training Loss: 1.9391499121080744e-05
Epoch 59/100: Training Loss: 1.479345162145116e-05
Epoch 60/100: Training Loss: 9.709199551831592e-05
Epoch 61/100: Training Loss: 9.787944568829103e-05
Epoch 62/100: Training Loss: 2.123102207075466e-05
Epoch 63/100: Training Loss: 9.955953679640184e-06
Epoch 64/100: Training Loss: 2.3780591701242056e-06
Epoch 65/100: Training Loss: 1.594010761684992e-05
Epoch 66/100: Training Loss: 1.7022429330443793e-05
Epoch 67/100: Training Loss: 1.3670413119887764e-05
Epoch 68/100: Training Loss: 6.236862497065555e-06
Epoch 69/100: Training Loss: 2.011894883418625e-06
Epoch 70/100: Training Loss: 7.642223923043771e-05
Epoch 71/100: Training Loss: 7.491088048978285e-05
Epoch 72/100: Training Loss: 0.0001062053848396648
Epoch 73/100: Training Loss: 4.470865242183209e-05
Epoch 74/100: Training Loss: 7.00413715094328e-05
Epoch 75/100: Training Loss: 0.0005938787351955067
Epoch 76/100: Training Loss: 1.5031124084171924e-05
Epoch 77/100: Training Loss: 3.7876516580581664e-05
Epoch 78/100: Training Loss: 7.535191252827644e-05
Epoch 79/100: Training Loss: 4.070914424532516e-06
Epoch 80/100: Training Loss: 6.33871140466495e-05
Epoch 81/100: Training Loss: 3.281263475814326e-06
Epoch 82/100: Training Loss: 6.345551968975501e-05
Epoch 83/100: Training Loss: 8.565032410181381e-06
Epoch 84/100: Training Loss: 0.00010784886438738216
Epoch 85/100: Training Loss: 1.2912323952398518e-05
Epoch 86/100: Training Loss: 6.8617365534671326e-06
Epoch 87/100: Training Loss: 7.92399218136614e-05
Epoch 88/100: Training Loss: 6.97095412760973e-05
Epoch 89/100: Training Loss: 7.047850786792961e-06
Epoch 90/100: Training Loss: 1.9629263657737863e-06
Epoch 91/100: Training Loss: 5.8928898281671786e-05
Epoch 92/100: Training Loss: 3.269206288016655e-06
Epoch 93/100: Training Loss: 4.682481415908445e-06
Epoch 94/100: Training Loss: 2.812262920832092e-05
Epoch 95/100: Training Loss: 5.6578079238533974e-05
Epoch 96/100: Training Loss: 5.9418693523515355e-05
Epoch 97/100: Training Loss: 4.948019896718589e-05
Epoch 98/100: Training Loss: 6.684577092528344e-05
Epoch 99/100: Training Loss: 0.00010218274864283476
Epoch 0/100: Training Loss: 0.00046848783844626995
Epoch 1/100: Training Loss: 0.0006243312551129249
Epoch 2/100: Training Loss: 0.0005614748954223598
Epoch 3/100: Training Loss: 0.00044278539545525053
Epoch 4/100: Training Loss: 0.00022023064749581472
Epoch 5/100: Training Loss: 0.00040961922176422613
Epoch 6/100: Training Loss: 0.0003155985941535317
Epoch 7/100: Training Loss: 0.00039380905540308095
Epoch 8/100: Training Loss: 0.0003992512066792783
Epoch 9/100: Training Loss: 6.29491657705351e-05
Epoch 10/100: Training Loss: 0.00028844194890167307
Epoch 11/100: Training Loss: 0.0001693760057748188
Epoch 12/100: Training Loss: 7.050464354661478e-06
Epoch 13/100: Training Loss: 0.0001457697510169948
Epoch 14/100: Training Loss: 0.00016793840423157689
Epoch 15/100: Training Loss: 0.00015303386109215871
Epoch 16/100: Training Loss: 0.00013779092597247268
Epoch 17/100: Training Loss: 0.00015164987282818913
Epoch 18/100: Training Loss: 0.00016403625706373822
Epoch 19/100: Training Loss: 5.559110799418067e-05
Epoch 20/100: Training Loss: 0.00015408614401443764
Epoch 21/100: Training Loss: 2.0211114020254206e-05
Epoch 22/100: Training Loss: 3.4518739474671225e-05
Epoch 23/100: Training Loss: 1.3016015162459716e-05
Epoch 24/100: Training Loss: 4.368633340855348e-05
Epoch 25/100: Training Loss: 8.881093788256843e-05
Epoch 26/100: Training Loss: 1.3350569192440279e-05
Epoch 27/100: Training Loss: 1.7136383655777175e-06
Epoch 28/100: Training Loss: 6.259870093127e-05
Epoch 29/100: Training Loss: 3.1976632276026335e-05
Epoch 30/100: Training Loss: 1.481379176305461e-05
Epoch 31/100: Training Loss: 1.7450933563544453e-05
Epoch 32/100: Training Loss: 8.044120628163562e-05
Epoch 33/100: Training Loss: 2.0963165582874403e-05
Epoch 34/100: Training Loss: 1.3554150084128029e-05
Epoch 35/100: Training Loss: 2.7456490348698358e-05
Epoch 36/100: Training Loss: 1.4775369342090347e-05
Epoch 37/100: Training Loss: 5.892039306702152e-05
Epoch 38/100: Training Loss: 4.73762935345074e-06
Epoch 39/100: Training Loss: 4.5459716439178465e-06
Epoch 40/100: Training Loss: 3.1100937985055455e-05
Epoch 41/100: Training Loss: 1.8774183608660225e-06
Epoch 42/100: Training Loss: 8.287498392679724e-06
Epoch 43/100: Training Loss: 5.2125689955556995e-06
Epoch 44/100: Training Loss: 2.595391296159287e-05
Epoch 45/100: Training Loss: 5.0541454462617776e-06
Epoch 46/100: Training Loss: 3.045143973734659e-06
Epoch 47/100: Training Loss: 1.1434028577488688e-06
Epoch 48/100: Training Loss: 3.3981696818418766e-06
Epoch 49/100: Training Loss: 3.1697042372232206e-05
Epoch 50/100: Training Loss: 2.2740373151883277e-06
Epoch 51/100: Training Loss: 7.552286279537986e-06
Epoch 52/100: Training Loss: 2.780970409169175e-05
Epoch 53/100: Training Loss: 1.4074640305635566e-05
Epoch 54/100: Training Loss: 6.132619133571051e-06
Epoch 55/100: Training Loss: 7.064062281198422e-07
Epoch 56/100: Training Loss: 1.5939123153875362e-06
Epoch 57/100: Training Loss: 9.429024791018823e-07
Epoch 58/100: Training Loss: 7.40522556307335e-07
Epoch 59/100: Training Loss: 1.4786596714694928e-06
Epoch 60/100: Training Loss: 1.3080952380041374e-05
Epoch 61/100: Training Loss: 5.539998622430909e-07
Epoch 62/100: Training Loss: 4.966366482997304e-07
Epoch 63/100: Training Loss: 4.858486161219635e-07
Epoch 64/100: Training Loss: 1.6885263199407246e-06
Epoch 65/100: Training Loss: 8.122205802921876e-07
Epoch 66/100: Training Loss: 5.725857024917954e-06
Epoch 67/100: Training Loss: 2.383148514403863e-06
Epoch 68/100: Training Loss: 6.254311318510353e-08
Epoch 69/100: Training Loss: 2.3588480230437997e-06
Epoch 70/100: Training Loss: 5.161728292021231e-07
Epoch 71/100: Training Loss: 1.0256476197091322e-06
Epoch 72/100: Training Loss: 4.802731805682731e-06
Epoch 73/100: Training Loss: 2.9434088519924593e-07
Epoch 74/100: Training Loss: 4.381899537372699e-07
Epoch 75/100: Training Loss: 1.638107085471741e-07
Epoch 76/100: Training Loss: 1.8051110341080597e-06
Epoch 77/100: Training Loss: 2.800456402728909e-07
Epoch 78/100: Training Loss: 3.0548456752638455e-07
Epoch 79/100: Training Loss: 5.0386249568970006e-08
Epoch 80/100: Training Loss: 6.900651247373649e-06
Epoch 81/100: Training Loss: 1.1855721621761263e-06
Epoch 82/100: Training Loss: 9.723805955478123e-06
Epoch 83/100: Training Loss: 1.311156692730117e-05
Epoch 84/100: Training Loss: 1.1295711080874166e-05
Epoch 85/100: Training Loss: 6.2741883461737e-07
Epoch 86/100: Training Loss: 7.505696814095233e-07
Epoch 87/100: Training Loss: 1.854596800097878e-06
Epoch 88/100: Training Loss: 1.5760756908885895e-05
Epoch 89/100: Training Loss: 4.076153068901962e-07
Epoch 90/100: Training Loss: 1.7803944719736442e-07
Epoch 91/100: Training Loss: 3.2297187900033344e-07
Epoch 92/100: Training Loss: 7.156729816205902e-07
Epoch 93/100: Training Loss: 2.5806916354026662e-08
Epoch 94/100: Training Loss: 1.2268472447066486e-07
Epoch 95/100: Training Loss: 2.9511143258141893e-07
Epoch 96/100: Training Loss: 5.5811016637701024e-08
Epoch 97/100: Training Loss: 3.9324497206947736e-08
Epoch 98/100: Training Loss: 4.19655608491809e-08
Epoch 99/100: Training Loss: 3.272472249884759e-07
Epoch 0/100: Training Loss: 0.00037203745636264833
Epoch 1/100: Training Loss: 0.0003780179319128526
Epoch 2/100: Training Loss: 0.00040091969798096514
Epoch 3/100: Training Loss: 0.0005309796372873593
Epoch 4/100: Training Loss: 0.00038449149743645595
Epoch 5/100: Training Loss: 0.0003896676399539002
Epoch 6/100: Training Loss: 0.0003771900273529829
Epoch 7/100: Training Loss: 0.00036735470052314017
Epoch 8/100: Training Loss: 0.0005003631576500108
Epoch 9/100: Training Loss: 0.0005923803664941703
Epoch 10/100: Training Loss: 0.0003581720577404562
Epoch 11/100: Training Loss: 0.000720635583970399
Epoch 12/100: Training Loss: 0.0006587030613316899
Epoch 13/100: Training Loss: 0.0006091754257151511
Epoch 14/100: Training Loss: 0.00068078143934233
Epoch 15/100: Training Loss: 0.0004242811582784737
Epoch 16/100: Training Loss: 0.000438700207566793
Epoch 17/100: Training Loss: 0.000593723844637913
Epoch 18/100: Training Loss: 0.0007554555756855855
Epoch 19/100: Training Loss: 0.0006370467290414118
Epoch 20/100: Training Loss: 0.0004934974146627747
Epoch 21/100: Training Loss: 0.0006379005128303461
Epoch 22/100: Training Loss: 0.00035213669185089853
Epoch 23/100: Training Loss: 0.0003890892309425151
Epoch 24/100: Training Loss: 0.0005436277231283947
Epoch 25/100: Training Loss: 0.00039728968280606565
Epoch 26/100: Training Loss: 0.0003334297015603665
Epoch 27/100: Training Loss: 0.0003883131837422869
Epoch 28/100: Training Loss: 0.00036178641350923386
Epoch 29/100: Training Loss: 0.00040740018661043287
Epoch 30/100: Training Loss: 0.00019688391645925234
Epoch 31/100: Training Loss: 0.0001847033380670885
Epoch 32/100: Training Loss: 3.253729535942584e-05
Epoch 33/100: Training Loss: 3.734646381530087e-05
Epoch 34/100: Training Loss: 1.722757922963495e-05
Epoch 35/100: Training Loss: 3.4526736883496025e-06
Epoch 36/100: Training Loss: 1.538722297852546e-05
Epoch 37/100: Training Loss: 2.060057512954273e-05
Epoch 38/100: Training Loss: 0.00024543498205927623
Epoch 39/100: Training Loss: 1.2896475959074709e-05
Epoch 40/100: Training Loss: 3.2899781764872304e-05
Epoch 41/100: Training Loss: 2.342746469431219e-05
Epoch 42/100: Training Loss: 9.229674572702003e-06
Epoch 43/100: Training Loss: 5.968335389563468e-06
Epoch 44/100: Training Loss: 4.2464335313050355e-06
Epoch 45/100: Training Loss: 0.00015149521023298786
Epoch 46/100: Training Loss: 4.547968737583245e-05
Epoch 47/100: Training Loss: 7.440632753140104e-05
Epoch 48/100: Training Loss: 1.2756876474394735e-05
Epoch 49/100: Training Loss: 1.0018126701161398e-05
Epoch 50/100: Training Loss: 2.635801025559153e-06
Epoch 51/100: Training Loss: 4.1873214881768267e-05
Epoch 52/100: Training Loss: 6.844885482461052e-05
Epoch 53/100: Training Loss: 2.768142581546465e-06
Epoch 54/100: Training Loss: 4.0095103224657015e-06
Epoch 55/100: Training Loss: 5.425121923661338e-06
Epoch 56/100: Training Loss: 1.6100221494855607e-05
Epoch 57/100: Training Loss: 0.0004128860227302112
Epoch 58/100: Training Loss: 2.554566568521931e-06
Epoch 59/100: Training Loss: 8.627382906532921e-06
Epoch 60/100: Training Loss: 9.452202975486232e-06
Epoch 61/100: Training Loss: 4.645582778890312e-06
Epoch 62/100: Training Loss: 1.177707464491899e-05
Epoch 63/100: Training Loss: 0.0001227642055106374
Epoch 64/100: Training Loss: 1.116756419857255e-05
Epoch 65/100: Training Loss: 7.674786719633678e-06
Epoch 66/100: Training Loss: 8.805312084413208e-05
Epoch 67/100: Training Loss: 3.582565113902092e-06
Epoch 68/100: Training Loss: 1.6253039269683371e-06
Epoch 69/100: Training Loss: 0.00010272337997381666
Epoch 70/100: Training Loss: 5.50479275809057e-06
Epoch 71/100: Training Loss: 9.656161795147752e-05
Epoch 72/100: Training Loss: 0.00010125969528360704
Epoch 73/100: Training Loss: 2.7348465117238527e-06
Epoch 74/100: Training Loss: 3.7966732376675427e-06
Epoch 75/100: Training Loss: 7.683230436250435e-06
Epoch 76/100: Training Loss: 2.17861436159078e-06
Epoch 77/100: Training Loss: 1.1475966930125667e-05
Epoch 78/100: Training Loss: 3.3901764318940385e-06
Epoch 79/100: Training Loss: 5.924606497968193e-06
Epoch 80/100: Training Loss: 0.00010999456971092562
Epoch 81/100: Training Loss: 1.760614434240666e-05
Epoch 82/100: Training Loss: 6.436456027811608e-05
Epoch 83/100: Training Loss: 1.1494714948060237e-05
Epoch 84/100: Training Loss: 2.167435915135177e-06
Epoch 85/100: Training Loss: 5.4921678770168695e-06
Epoch 86/100: Training Loss: 7.385733347814695e-05
Epoch 87/100: Training Loss: 3.8005963309907017e-06
Epoch 88/100: Training Loss: 5.515733222782084e-06
Epoch 89/100: Training Loss: 1.3093168402206054e-05
Epoch 90/100: Training Loss: 3.3057657939967063e-06
Epoch 91/100: Training Loss: 7.67203241553718e-06
Epoch 92/100: Training Loss: 5.272768779896793e-06
Epoch 93/100: Training Loss: 9.442130474587984e-06
Epoch 94/100: Training Loss: 3.8367577115551825e-06
Epoch 95/100: Training Loss: 4.5294868265303365e-06
Epoch 96/100: Training Loss: 2.4041886218881186e-06
Epoch 97/100: Training Loss: 4.002147053360148e-06
Epoch 98/100: Training Loss: 6.309491520401387e-06
Epoch 99/100: Training Loss: 1.4729989461798583e-05
Epoch 0/100: Training Loss: 0.0005852621499733005
Epoch 1/100: Training Loss: 0.0010343701850138438
Epoch 2/100: Training Loss: 0.00032851210223185107
Epoch 3/100: Training Loss: 0.00075447305435557
Epoch 4/100: Training Loss: 6.351552302260036e-05
Epoch 5/100: Training Loss: 0.0001116271377144373
Epoch 6/100: Training Loss: 0.0005712116913945151
Epoch 7/100: Training Loss: 0.000716340782396462
Epoch 8/100: Training Loss: 0.0001827621406503857
Epoch 9/100: Training Loss: 3.116293037686113e-05
Epoch 10/100: Training Loss: 3.1272041289795674e-05
Epoch 11/100: Training Loss: 0.000436329788156689
Epoch 12/100: Training Loss: 7.484580135398916e-06
Epoch 13/100: Training Loss: 1.3614324236814884e-06
Epoch 14/100: Training Loss: 4.84071561289876e-06
Epoch 15/100: Training Loss: 0.00033351932673176307
Epoch 16/100: Training Loss: 0.0002916401811779347
Epoch 17/100: Training Loss: 4.982753268405461e-06
Epoch 18/100: Training Loss: 1.1055625874899963e-05
Epoch 19/100: Training Loss: 2.6797604300248782e-05
Epoch 20/100: Training Loss: 0.0003283190887605128
Epoch 21/100: Training Loss: 1.2412380412504946e-06
Epoch 22/100: Training Loss: 3.4850894686486153e-06
Epoch 23/100: Training Loss: 1.5316766829254948e-07
Epoch 24/100: Training Loss: 5.872167632267748e-07
Epoch 25/100: Training Loss: 2.4265729751220732e-06
Epoch 26/100: Training Loss: 1.3836603110547558e-05
Epoch 27/100: Training Loss: 6.027229030036071e-05
Epoch 28/100: Training Loss: 2.862482056055101e-06
Epoch 29/100: Training Loss: 5.787465566844416e-06
Epoch 30/100: Training Loss: 8.026797286832011e-06
Epoch 31/100: Training Loss: 4.4983713989430044e-08
Epoch 32/100: Training Loss: 5.689400327579857e-05
Epoch 33/100: Training Loss: 3.055387334229901e-07
Epoch 34/100: Training Loss: 1.0880163579452531e-06
Epoch 35/100: Training Loss: 8.935117133529731e-05
Epoch 36/100: Training Loss: 1.328503253614956e-05
Epoch 37/100: Training Loss: 6.426944326392203e-05
Epoch 38/100: Training Loss: 4.532744360444524e-06
Epoch 39/100: Training Loss: 9.401217980034683e-06
Epoch 40/100: Training Loss: 8.79903440885747e-06
Epoch 41/100: Training Loss: 9.17467167559226e-06
Epoch 42/100: Training Loss: 2.1630432456731796e-05
Epoch 43/100: Training Loss: 1.496174283985533e-06
Epoch 44/100: Training Loss: 3.3534472315434384e-06
Epoch 45/100: Training Loss: 1.3557417243050888e-05
Epoch 46/100: Training Loss: 1.9343110885822962e-08
Epoch 47/100: Training Loss: 6.62584835428519e-08
Epoch 48/100: Training Loss: 1.2438880559474631e-06
Epoch 49/100: Training Loss: 5.705580282371675e-05
Epoch 50/100: Training Loss: 6.136410314205516e-06
Epoch 51/100: Training Loss: 4.2457062664395827e-07
Epoch 52/100: Training Loss: 3.7645191296667796e-06
Epoch 53/100: Training Loss: 1.1278959534093404e-05
Epoch 54/100: Training Loss: 1.92955002656432e-06
Epoch 55/100: Training Loss: 2.8888246769110586e-07
Epoch 56/100: Training Loss: 1.7518950250383986e-05
Epoch 57/100: Training Loss: 2.1102696222842008e-05
Epoch 58/100: Training Loss: 1.13507820452008e-05
Epoch 59/100: Training Loss: 8.242119137863859e-08
Epoch 60/100: Training Loss: 1.804850684403705e-06
Epoch 61/100: Training Loss: 1.2713263120351885e-05
Epoch 62/100: Training Loss: 5.597689276719842e-05
Epoch 63/100: Training Loss: 5.671200593056433e-06
Epoch 64/100: Training Loss: 8.029601695391896e-06
Epoch 65/100: Training Loss: 4.6829332419015435e-07
Epoch 66/100: Training Loss: 3.8128200685605407e-07
Epoch 67/100: Training Loss: 2.199973161394954e-06
Epoch 68/100: Training Loss: 4.519568025364202e-06
Epoch 69/100: Training Loss: 7.966621528325327e-07
Epoch 70/100: Training Loss: 7.087186254527536e-06
Epoch 71/100: Training Loss: 2.6597721802880948e-06
Epoch 72/100: Training Loss: 2.4227005669183094e-07
Epoch 73/100: Training Loss: 4.794455074387782e-07
Epoch 74/100: Training Loss: 6.629192847716768e-07
Epoch 75/100: Training Loss: 7.986647573888568e-06
Epoch 76/100: Training Loss: 4.783400539058206e-05
Epoch 77/100: Training Loss: 3.0941908318766565e-06
Epoch 78/100: Training Loss: 2.7175726159684563e-07
Epoch 79/100: Training Loss: 2.587642284442207e-07
Epoch 80/100: Training Loss: 1.5615085985578362e-07
Epoch 81/100: Training Loss: 0.00013299140193804497
Epoch 82/100: Training Loss: 6.056375955043553e-07
Epoch 83/100: Training Loss: 1.7493451640611273e-05
Epoch 84/100: Training Loss: 3.7374350789416533e-06
Epoch 85/100: Training Loss: 3.8045038774008174e-06
Epoch 86/100: Training Loss: 9.635004482227859e-07
Epoch 87/100: Training Loss: 2.3688437655200604e-06
Epoch 88/100: Training Loss: 1.6682623948739015e-07
Epoch 89/100: Training Loss: 1.7947350559372982e-08
Epoch 90/100: Training Loss: 2.0964307623216882e-08
Epoch 91/100: Training Loss: 1.1810271754326308e-06
Epoch 92/100: Training Loss: 9.579612584283467e-07
Epoch 93/100: Training Loss: 1.8218008596700671e-06
Epoch 94/100: Training Loss: 1.5717648928191495e-06
Epoch 95/100: Training Loss: 7.834205103227911e-06
Epoch 96/100: Training Loss: 1.0423677568344791e-05
Epoch 97/100: Training Loss: 1.725847436106793e-05
Epoch 98/100: Training Loss: 5.872509830668902e-06
Epoch 99/100: Training Loss: 7.490822189224408e-06
dataset: cities layer_num_from_end:-5 Avg_acc:tensor(0.6776) Avg_AUC:0.7873076770913221 Avg_threshold:0.35431766510009766
dataset: inventions layer_num_from_end:-5 Avg_acc:tensor(0.6564) Avg_AUC:0.8441161700703046 Avg_threshold:0.009480226784944534
dataset: elements layer_num_from_end:-5 Avg_acc:tensor(0.6118) Avg_AUC:0.6806335992600301 Avg_threshold:0.7941304445266724
dataset: animals layer_num_from_end:-5 Avg_acc:tensor(0.6835) Avg_AUC:0.76104261148904 Avg_threshold:0.8426349759101868
dataset: companies layer_num_from_end:-5 Avg_acc:tensor(0.6900) Avg_AUC:0.8417361111111111 Avg_threshold:0.9978792667388916
dataset: facts layer_num_from_end:-5 Avg_acc:tensor(0.7435) Avg_AUC:0.8591086761501986 Avg_threshold:0.868720531463623
dataset: conj_neg_facts layer_num_from_end:-5 Avg_acc:tensor(0.7291) Avg_AUC:0.5816819481415755 Avg_threshold:0.9859822988510132


================layer -9================
Epoch 0/100: Training Loss: 0.0031212201485267053
Epoch 1/100: Training Loss: 0.00356542108914791
Epoch 2/100: Training Loss: 0.0037636818029941656
Epoch 3/100: Training Loss: 0.0031396270944521977
Epoch 4/100: Training Loss: 0.0034777353971432415
Epoch 5/100: Training Loss: 0.0023192734672473026
Epoch 6/100: Training Loss: 0.0018669512027349228
Epoch 7/100: Training Loss: 0.0017648472999915099
Epoch 8/100: Training Loss: 0.0020321393624330177
Epoch 9/100: Training Loss: 0.00108491151760786
Epoch 10/100: Training Loss: 0.0009544382874782269
Epoch 11/100: Training Loss: 0.0021131974764359305
Epoch 12/100: Training Loss: 0.0016031742860109378
Epoch 13/100: Training Loss: 0.0007516333403495642
Epoch 14/100: Training Loss: 0.0022273420905455565
Epoch 15/100: Training Loss: 0.0007072868637549571
Epoch 16/100: Training Loss: 0.0009613109704775688
Epoch 17/100: Training Loss: 0.0005563688583863088
Epoch 18/100: Training Loss: 0.001234780137355511
Epoch 19/100: Training Loss: 0.00042533521086741716
Epoch 20/100: Training Loss: 0.0005799162273223584
Epoch 21/100: Training Loss: 0.0023855788585467217
Epoch 22/100: Training Loss: 0.0005750320374201506
Epoch 23/100: Training Loss: 0.0011871816256107427
Epoch 24/100: Training Loss: 0.0006932662561153754
Epoch 25/100: Training Loss: 0.00047523977282719733
Epoch 26/100: Training Loss: 0.0010840570888458155
Epoch 27/100: Training Loss: 0.0009492805752998743
Epoch 28/100: Training Loss: 0.0005021058022975922
Epoch 29/100: Training Loss: 6.777545413336693e-05
Epoch 30/100: Training Loss: 0.0005733186426835182
Epoch 31/100: Training Loss: 0.003129513217852666
Epoch 32/100: Training Loss: 0.0006013741860022912
Epoch 33/100: Training Loss: 0.00032252125824109104
Epoch 34/100: Training Loss: 0.0006628638276687035
Epoch 35/100: Training Loss: 0.0008011566331753364
Epoch 36/100: Training Loss: 0.0008866126911762434
Epoch 37/100: Training Loss: 0.0002613169117233692
Epoch 38/100: Training Loss: 0.0001748231454537465
Epoch 39/100: Training Loss: 0.0009230942680285528
Epoch 40/100: Training Loss: 0.0007137576930033854
Epoch 41/100: Training Loss: 0.0006971193525271538
Epoch 42/100: Training Loss: 0.0006662175441399599
Epoch 43/100: Training Loss: 9.247513177494208e-05
Epoch 44/100: Training Loss: 0.0005651344664585896
Epoch 45/100: Training Loss: 0.0009311088957847693
Epoch 46/100: Training Loss: 0.0010397239373280453
Epoch 47/100: Training Loss: 0.0018895828188993991
Epoch 48/100: Training Loss: 0.0004261688162118961
Epoch 49/100: Training Loss: 0.0005090406689888391
Epoch 50/100: Training Loss: 0.0032895375520755085
Epoch 51/100: Training Loss: 0.0002022405417683797
Epoch 52/100: Training Loss: 0.0005872389062857017
Epoch 53/100: Training Loss: 0.001359890477779584
Epoch 54/100: Training Loss: 0.0008273166723740407
Epoch 55/100: Training Loss: 0.0005795432206911918
Epoch 56/100: Training Loss: 0.0015515511234601338
Epoch 57/100: Training Loss: 0.0014312569147501236
Epoch 58/100: Training Loss: 0.0011869283058704473
Epoch 59/100: Training Loss: 0.0010120565883624249
Epoch 60/100: Training Loss: 0.001393242046619073
Epoch 61/100: Training Loss: 0.0005228194670799451
Epoch 62/100: Training Loss: 0.0008157653113206228
Epoch 63/100: Training Loss: 0.0010363278098595447
Epoch 64/100: Training Loss: 0.0003762769106871043
Epoch 65/100: Training Loss: 0.00027507534011816367
Epoch 66/100: Training Loss: 0.0003569560746351878
Epoch 67/100: Training Loss: 0.0006833740820487341
Epoch 68/100: Training Loss: 0.00038454999239780963
Epoch 69/100: Training Loss: 7.749390071974351e-05
Epoch 70/100: Training Loss: 0.0009431885794187203
Epoch 71/100: Training Loss: 0.00013910759335909135
Epoch 72/100: Training Loss: 0.000612927075379934
Epoch 73/100: Training Loss: 0.0005567347965179345
Epoch 74/100: Training Loss: 0.0009495837566180107
Epoch 75/100: Training Loss: 0.0006363461606013469
Epoch 76/100: Training Loss: 0.0007243034167167468
Epoch 77/100: Training Loss: 0.0005977194851789719
Epoch 78/100: Training Loss: 0.00017219605163121835
Epoch 79/100: Training Loss: 0.00038177789881443366
Epoch 80/100: Training Loss: 0.0004835881006259185
Epoch 81/100: Training Loss: 0.0001227069192398817
Epoch 82/100: Training Loss: 0.0004990404615035424
Epoch 83/100: Training Loss: 0.00047875186189627036
Epoch 84/100: Training Loss: 0.0005110623076176032
Epoch 85/100: Training Loss: 6.54968898743391e-05
Epoch 86/100: Training Loss: 2.1195457054254337e-05
Epoch 87/100: Training Loss: 0.0004702923962703118
Epoch 88/100: Training Loss: 0.00046631092062363256
Epoch 89/100: Training Loss: 0.0007925641078215379
Epoch 90/100: Training Loss: 0.00040424662904861645
Epoch 91/100: Training Loss: 7.962088028971966e-05
Epoch 92/100: Training Loss: 0.00016190345661762432
Epoch 93/100: Training Loss: 0.0001275245076379715
Epoch 94/100: Training Loss: 0.0004463054908391757
Epoch 95/100: Training Loss: 0.00039534156139080343
Epoch 96/100: Training Loss: 0.00045279069588734553
Epoch 97/100: Training Loss: 0.0004792267886491922
Epoch 98/100: Training Loss: 0.00045524728603852104
Epoch 99/100: Training Loss: 0.0004470622501312158
Epoch 0/100: Training Loss: 0.00021696307867631785
Epoch 1/100: Training Loss: 0.0002881238412429399
Epoch 2/100: Training Loss: 0.0001817805350094098
Epoch 3/100: Training Loss: 0.0001313568431165721
Epoch 4/100: Training Loss: 0.00010266769760927277
Epoch 5/100: Training Loss: 0.00021446198893234868
Epoch 6/100: Training Loss: 0.00025960664377618797
Epoch 7/100: Training Loss: 9.256953816242816e-06
Epoch 8/100: Training Loss: 2.3176194577908033e-06
Epoch 9/100: Training Loss: 0.00016213049736258162
Epoch 10/100: Training Loss: 8.19590594201879e-05
Epoch 11/100: Training Loss: 1.25369539425913e-06
Epoch 12/100: Training Loss: 2.426203682751399e-05
Epoch 13/100: Training Loss: 3.863785378300823e-06
Epoch 14/100: Training Loss: 2.9949726110883894e-05
Epoch 15/100: Training Loss: 4.569102797005743e-05
Epoch 16/100: Training Loss: 2.7339976498097047e-05
Epoch 17/100: Training Loss: 0.00011014448888098712
Epoch 18/100: Training Loss: 0.00021824203932766423
Epoch 19/100: Training Loss: 6.301186962111649e-05
Epoch 20/100: Training Loss: 3.733617909286054e-05
Epoch 21/100: Training Loss: 9.172270283303454e-06
Epoch 22/100: Training Loss: 2.3647221932897653e-05
Epoch 23/100: Training Loss: 8.206545571575251e-05
Epoch 24/100: Training Loss: 4.28763111727521e-06
Epoch 25/100: Training Loss: 1.13349391205963e-05
Epoch 26/100: Training Loss: 2.756563950781063e-06
Epoch 27/100: Training Loss: 2.2216844100989567e-05
Epoch 28/100: Training Loss: 2.449171022760511e-07
Epoch 29/100: Training Loss: 6.384292989133987e-06
Epoch 30/100: Training Loss: 9.558494557781069e-06
Epoch 31/100: Training Loss: 2.671072884429363e-07
Epoch 32/100: Training Loss: 2.267624297484154e-06
Epoch 33/100: Training Loss: 2.006889893067792e-05
Epoch 34/100: Training Loss: 0.0001400533913229613
Epoch 35/100: Training Loss: 3.2331821646524655e-05
Epoch 36/100: Training Loss: 8.938644047943466e-06
Epoch 37/100: Training Loss: 4.4738961542401075e-06
Epoch 38/100: Training Loss: 8.550042395099931e-05
Epoch 39/100: Training Loss: 0.00015378220533041677
Epoch 40/100: Training Loss: 1.1799233180424825e-06
Epoch 41/100: Training Loss: 0.00010625419996244491
Epoch 42/100: Training Loss: 1.3887645225687946e-05
Epoch 43/100: Training Loss: 0.00013132146121140552
Epoch 44/100: Training Loss: 8.975288459003774e-05
Epoch 45/100: Training Loss: 5.924482093789853e-07
Epoch 46/100: Training Loss: 2.031775860683867e-06
Epoch 47/100: Training Loss: 9.568332076634409e-08
Epoch 48/100: Training Loss: 4.3861240914901674e-07
Epoch 49/100: Training Loss: 5.1883391768675746e-05
Epoch 50/100: Training Loss: 1.7500411240893496e-06
Epoch 51/100: Training Loss: 0.00012528954800468923
Epoch 52/100: Training Loss: 1.8841935730969426e-05
Epoch 53/100: Training Loss: 1.7396927220776108e-07
Epoch 54/100: Training Loss: 1.956524056979693e-06
Epoch 55/100: Training Loss: 2.422960184831924e-06
Epoch 56/100: Training Loss: 9.611967299551173e-05
Epoch 57/100: Training Loss: 1.7171746773453171e-06
Epoch 58/100: Training Loss: 9.422576798318213e-06
Epoch 59/100: Training Loss: 5.9390498007827275e-06
Epoch 60/100: Training Loss: 0.00014909796182884763
Epoch 61/100: Training Loss: 2.7620486263603375e-05
Epoch 62/100: Training Loss: 1.2778334586876926e-05
Epoch 63/100: Training Loss: 2.482068367853694e-06
Epoch 64/100: Training Loss: 1.4424944221305206e-05
Epoch 65/100: Training Loss: 5.2714093918223015e-05
Epoch 66/100: Training Loss: 8.115987606647303e-05
Epoch 67/100: Training Loss: 3.1998035426965743e-06
Epoch 68/100: Training Loss: 5.4949643967397546e-05
Epoch 69/100: Training Loss: 4.3843551267422905e-05
Epoch 70/100: Training Loss: 2.0739372617109873e-06
Epoch 71/100: Training Loss: 0.00011268592201540823
Epoch 72/100: Training Loss: 6.838803985594634e-06
Epoch 73/100: Training Loss: 0.0001035752818990716
Epoch 74/100: Training Loss: 2.5016410201119734e-05
Epoch 75/100: Training Loss: 0.0001002274904684101
Epoch 76/100: Training Loss: 5.513704862161602e-05
Epoch 77/100: Training Loss: 5.026503503656708e-06
Epoch 78/100: Training Loss: 5.317035427676188e-06
Epoch 79/100: Training Loss: 9.878730660329485e-06
Epoch 80/100: Training Loss: 9.996686214288788e-05
Epoch 81/100: Training Loss: 9.114082251161738e-05
Epoch 82/100: Training Loss: 2.3940519634383677e-05
Epoch 83/100: Training Loss: 8.755277491471159e-05
Epoch 84/100: Training Loss: 1.6082391443541233e-05
Epoch 85/100: Training Loss: 9.417023317629447e-06
Epoch 86/100: Training Loss: 9.129044386838049e-05
Epoch 87/100: Training Loss: 8.680455954620122e-05
Epoch 88/100: Training Loss: 8.886241491867288e-05
Epoch 89/100: Training Loss: 8.91803091416979e-05
Epoch 90/100: Training Loss: 9.852436458850656e-06
Epoch 91/100: Training Loss: 6.9292586906541624e-06
Epoch 92/100: Training Loss: 4.920377625745508e-06
Epoch 93/100: Training Loss: 8.306947991986981e-05
Epoch 94/100: Training Loss: 3.0066016142678367e-05
Epoch 95/100: Training Loss: 1.4687113614226671e-05
Epoch 96/100: Training Loss: 8.38409073550605e-05
Epoch 97/100: Training Loss: 2.7906070275424308e-05
Epoch 98/100: Training Loss: 4.313508624987752e-05
Epoch 99/100: Training Loss: 5.1252076176784495e-05
Epoch 0/100: Training Loss: 0.0009595398449789884
Epoch 1/100: Training Loss: 0.0011254761418605821
Epoch 2/100: Training Loss: 0.0007644097459801721
Epoch 3/100: Training Loss: 0.0008153450165399059
Epoch 4/100: Training Loss: 0.001015688634026644
Epoch 5/100: Training Loss: 0.0008024002227308524
Epoch 6/100: Training Loss: 0.0008000090920547555
Epoch 7/100: Training Loss: 0.0007250897620058707
Epoch 8/100: Training Loss: 0.0007355207771197703
Epoch 9/100: Training Loss: 0.0007316480260089512
Epoch 10/100: Training Loss: 0.0009583186914478492
Epoch 11/100: Training Loss: 0.0007214916372730721
Epoch 12/100: Training Loss: 0.0008045473384641414
Epoch 13/100: Training Loss: 0.0005777611047434051
Epoch 14/100: Training Loss: 0.0007049191861131073
Epoch 15/100: Training Loss: 0.0009545068935031804
Epoch 16/100: Training Loss: 0.0008964982912011816
Epoch 17/100: Training Loss: 0.0008511804887072533
Epoch 18/100: Training Loss: 0.001034015578921564
Epoch 19/100: Training Loss: 0.0006852728479048785
Epoch 20/100: Training Loss: 0.0006773057836213263
Epoch 21/100: Training Loss: 0.0007873880108017728
Epoch 22/100: Training Loss: 0.0006417394763204307
Epoch 23/100: Training Loss: 0.000741812033890599
Epoch 24/100: Training Loss: 0.00048003268187941467
Epoch 25/100: Training Loss: 0.0003512193916609924
Epoch 26/100: Training Loss: 0.0006530605829679049
Epoch 27/100: Training Loss: 0.0003844887625038354
Epoch 28/100: Training Loss: 0.0003318126519880683
Epoch 29/100: Training Loss: 0.0006145795815670652
Epoch 30/100: Training Loss: 0.0008848269061265488
Epoch 31/100: Training Loss: 0.000733496107127332
Epoch 32/100: Training Loss: 0.0005851072423598345
Epoch 33/100: Training Loss: 0.00024811766738265886
Epoch 34/100: Training Loss: 0.000412342741208918
Epoch 35/100: Training Loss: 0.0003978013318048883
Epoch 36/100: Training Loss: 0.00019772113111224109
Epoch 37/100: Training Loss: 0.00016095000424536104
Epoch 38/100: Training Loss: 0.00027528954711974476
Epoch 39/100: Training Loss: 0.00023708027277596935
Epoch 40/100: Training Loss: 0.000189077699076536
Epoch 41/100: Training Loss: 0.00032329451444461875
Epoch 42/100: Training Loss: 0.0004798294934212352
Epoch 43/100: Training Loss: 0.00011746438717410575
Epoch 44/100: Training Loss: 0.0005520746044443743
Epoch 45/100: Training Loss: 0.0001421224207899689
Epoch 46/100: Training Loss: 0.00042394848700562214
Epoch 47/100: Training Loss: 0.00045816637407061203
Epoch 48/100: Training Loss: 0.00022573469046553875
Epoch 49/100: Training Loss: 9.080551868110761e-05
Epoch 50/100: Training Loss: 3.247990007449059e-05
Epoch 51/100: Training Loss: 0.00017791726875089414
Epoch 52/100: Training Loss: 6.691785739962331e-05
Epoch 53/100: Training Loss: 5.277850542942323e-05
Epoch 54/100: Training Loss: 0.00016900701490462635
Epoch 55/100: Training Loss: 9.170883415241587e-05
Epoch 56/100: Training Loss: 0.00011985234882497141
Epoch 57/100: Training Loss: 0.00013884350792315212
Epoch 58/100: Training Loss: 0.00033516453671779027
Epoch 59/100: Training Loss: 0.0002164717308536374
Epoch 60/100: Training Loss: 0.00011558781854167783
Epoch 61/100: Training Loss: 7.598144234035888e-05
Epoch 62/100: Training Loss: 0.0005309757064370548
Epoch 63/100: Training Loss: 0.0004035181521829976
Epoch 64/100: Training Loss: 0.00025465595641287203
Epoch 65/100: Training Loss: 5.981286911807988e-05
Epoch 66/100: Training Loss: 7.505128286542936e-05
Epoch 67/100: Training Loss: 0.00023386270077519828
Epoch 68/100: Training Loss: 1.6702274787682212e-05
Epoch 69/100: Training Loss: 1.0125717249323879e-05
Epoch 70/100: Training Loss: 7.82425249863534e-05
Epoch 71/100: Training Loss: 5.332227138911976e-05
Epoch 72/100: Training Loss: 3.0271519329483153e-05
Epoch 73/100: Training Loss: 8.18829174359999e-05
Epoch 74/100: Training Loss: 3.964359117713989e-05
Epoch 75/100: Training Loss: 5.827832659411485e-06
Epoch 76/100: Training Loss: 2.059853336525179e-05
Epoch 77/100: Training Loss: 0.0001976172949663654
Epoch 78/100: Training Loss: 4.5577165295635416e-05
Epoch 79/100: Training Loss: 0.00015990672068358548
Epoch 80/100: Training Loss: 5.677180170878026e-06
Epoch 81/100: Training Loss: 0.00029054142501019783
Epoch 82/100: Training Loss: 2.6769904478777588e-05
Epoch 83/100: Training Loss: 3.541954181987236e-05
Epoch 84/100: Training Loss: 2.140869550845202e-05
Epoch 85/100: Training Loss: 2.303400721687537e-05
Epoch 86/100: Training Loss: 4.8339189919411326e-05
Epoch 87/100: Training Loss: 4.393192707683166e-05
Epoch 88/100: Training Loss: 5.457621927444752e-05
Epoch 89/100: Training Loss: 4.647810399802022e-05
Epoch 90/100: Training Loss: 3.176109393325327e-05
Epoch 91/100: Training Loss: 3.311616617225414e-05
Epoch 92/100: Training Loss: 0.00024291866235603575
Epoch 93/100: Training Loss: 2.451456990514406e-05
Epoch 94/100: Training Loss: 2.9462267438211052e-05
Epoch 95/100: Training Loss: 4.762170912183787e-05
Epoch 96/100: Training Loss: 4.380895456981875e-05
Epoch 97/100: Training Loss: 6.773447980303571e-05
Epoch 98/100: Training Loss: 3.552952638039222e-05
Epoch 99/100: Training Loss: 0.00011341096791206982
Epoch 0/100: Training Loss: 0.000808044590733268
Epoch 1/100: Training Loss: 0.0006929200481284748
Epoch 2/100: Training Loss: 0.0008509221402081576
Epoch 3/100: Training Loss: 0.0006059878251769326
Epoch 4/100: Training Loss: 0.0005404707383025776
Epoch 5/100: Training Loss: 0.0007179457355629314
Epoch 6/100: Training Loss: 0.0005024952305988832
Epoch 7/100: Training Loss: 0.00028054900467395784
Epoch 8/100: Training Loss: 0.0004986185919154774
Epoch 9/100: Training Loss: 0.0006002103063193234
Epoch 10/100: Training Loss: 0.0004341943697495894
Epoch 11/100: Training Loss: 0.00032479566606608305
Epoch 12/100: Training Loss: 0.0001288577914237976
Epoch 13/100: Training Loss: 0.00027092620730400084
Epoch 14/100: Training Loss: 0.0003381034867330031
Epoch 15/100: Training Loss: 0.0001921348443085497
Epoch 16/100: Training Loss: 0.0001502412963997234
Epoch 17/100: Training Loss: 1.0534249965778807e-05
Epoch 18/100: Training Loss: 0.00016378404741937465
Epoch 19/100: Training Loss: 0.0005868663842027838
Epoch 20/100: Training Loss: 0.00033354975960471413
Epoch 21/100: Training Loss: 5.2203026346184994e-05
Epoch 22/100: Training Loss: 6.826814602721821e-05
Epoch 23/100: Training Loss: 0.0005245768549767408
Epoch 24/100: Training Loss: 4.056304777887734e-05
Epoch 25/100: Training Loss: 0.0002199797975746068
Epoch 26/100: Training Loss: 0.00020809200677004728
Epoch 27/100: Training Loss: 0.0002253296361728148
Epoch 28/100: Training Loss: 0.0001624090089039369
Epoch 29/100: Training Loss: 0.00019170108505270697
Epoch 30/100: Training Loss: 0.00013123146173628892
Epoch 31/100: Training Loss: 1.2357440904121507e-05
Epoch 32/100: Training Loss: 0.00016701431436972185
Epoch 33/100: Training Loss: 0.00017534370788119057
Epoch 34/100: Training Loss: 0.0002867804332212968
Epoch 35/100: Training Loss: 0.00011578385125507008
Epoch 36/100: Training Loss: 8.701727810231122e-05
Epoch 37/100: Training Loss: 0.00018782197413119402
Epoch 38/100: Training Loss: 7.066611962562258e-05
Epoch 39/100: Training Loss: 0.0001126980832354589
Epoch 40/100: Training Loss: 0.0005766371434385127
Epoch 41/100: Training Loss: 0.0003178532150658694
Epoch 42/100: Training Loss: 9.491179477084767e-05
Epoch 43/100: Training Loss: 0.00011327684941616926
Epoch 44/100: Training Loss: 2.8719949874688278e-05
Epoch 45/100: Training Loss: 7.873842154036868e-05
Epoch 46/100: Training Loss: 4.175171594728123e-05
Epoch 47/100: Training Loss: 1.5748692253096537e-05
Epoch 48/100: Training Loss: 0.0001367781480604952
Epoch 49/100: Training Loss: 2.7712184766476805e-05
Epoch 50/100: Training Loss: 0.00020217919214205309
Epoch 51/100: Training Loss: 2.7075327340174807e-05
Epoch 52/100: Training Loss: 2.9160696166482838e-05
Epoch 53/100: Training Loss: 1.439359885725108e-05
Epoch 54/100: Training Loss: 0.00019870600239797073
Epoch 55/100: Training Loss: 0.0005733503536744551
Epoch 56/100: Training Loss: 0.0001738326454704458
Epoch 57/100: Training Loss: 0.0001001096245917407
Epoch 58/100: Training Loss: 5.428375760939988e-05
Epoch 59/100: Training Loss: 9.28244638172063e-05
Epoch 60/100: Training Loss: 2.81815565275875e-05
Epoch 61/100: Training Loss: 4.8963759433139454e-05
Epoch 62/100: Training Loss: 1.485107508911328e-05
Epoch 63/100: Training Loss: 0.00011782611466266892
Epoch 64/100: Training Loss: 0.00012491765347394075
Epoch 65/100: Training Loss: 5.6822153485634114e-05
Epoch 66/100: Training Loss: 4.126685671508312e-05
Epoch 67/100: Training Loss: 5.903095006942749e-05
Epoch 68/100: Training Loss: 1.2740395455197854e-05
Epoch 69/100: Training Loss: 1.3994446701624177e-05
Epoch 70/100: Training Loss: 3.411497825502672e-06
Epoch 71/100: Training Loss: 0.00011186003684997559
Epoch 72/100: Training Loss: 6.048314125192436e-06
Epoch 73/100: Training Loss: 4.541324663229964e-06
Epoch 74/100: Training Loss: 9.721693667498502e-05
Epoch 75/100: Training Loss: 9.788290851495483e-05
Epoch 76/100: Training Loss: 3.782914030704309e-06
Epoch 77/100: Training Loss: 0.00010421878912232139
Epoch 78/100: Training Loss: 5.419811085713181e-06
Epoch 79/100: Training Loss: 0.00013048366050828587
Epoch 80/100: Training Loss: 1.7059084282002666e-05
Epoch 81/100: Training Loss: 0.0001998071304776452
Epoch 82/100: Training Loss: 7.951691408048977e-05
Epoch 83/100: Training Loss: 3.0279002914374524e-05
Epoch 84/100: Training Loss: 4.2400200089270416e-05
Epoch 85/100: Training Loss: 6.399078040637753e-05
Epoch 86/100: Training Loss: 5.630593408237804e-05
Epoch 87/100: Training Loss: 2.8598520227453925e-05
Epoch 88/100: Training Loss: 8.948728950186209e-05
Epoch 89/100: Training Loss: 5.127042426135052e-06
Epoch 90/100: Training Loss: 6.591826677322387e-05
Epoch 91/100: Training Loss: 3.994480198757215e-05
Epoch 92/100: Training Loss: 0.0006961731748147444
Epoch 93/100: Training Loss: 7.554838772524487e-05
Epoch 94/100: Training Loss: 4.985456524247473e-05
Epoch 95/100: Training Loss: 9.595424106175249e-05
Epoch 96/100: Training Loss: 0.0001621961085633798
Epoch 97/100: Training Loss: 5.752174607054754e-05
Epoch 98/100: Training Loss: 9.452406613325531e-06
Epoch 99/100: Training Loss: 5.737397481094707e-05
Epoch 0/100: Training Loss: 0.00046876536398988717
Epoch 1/100: Training Loss: 0.0004557998086999638
Epoch 2/100: Training Loss: 7.99642060346867e-05
Epoch 3/100: Training Loss: 3.385733306613935e-05
Epoch 4/100: Training Loss: 0.0002315981734183527
Epoch 5/100: Training Loss: 0.00012315305009964975
Epoch 6/100: Training Loss: 1.5860177739630645e-05
Epoch 7/100: Training Loss: 8.91804368951903e-05
Epoch 8/100: Training Loss: 0.00023322818534714834
Epoch 9/100: Training Loss: 4.1638571937452607e-07
Epoch 10/100: Training Loss: 0.00036164644401743667
Epoch 11/100: Training Loss: 0.00025554334375715474
Epoch 12/100: Training Loss: 0.00015452253516368603
Epoch 13/100: Training Loss: 1.1983480999752673e-05
Epoch 14/100: Training Loss: 0.00037365647092942267
Epoch 15/100: Training Loss: 0.00022117261375699724
Epoch 16/100: Training Loss: 0.00013402919821475508
Epoch 17/100: Training Loss: 1.4426388395821444e-05
Epoch 18/100: Training Loss: 0.0002672340944066026
Epoch 19/100: Training Loss: 0.00012032419855144167
Epoch 20/100: Training Loss: 4.6164011968995016e-05
Epoch 21/100: Training Loss: 5.03737091039595e-06
Epoch 22/100: Training Loss: 1.6858286228597437e-05
Epoch 23/100: Training Loss: 0.0002674176464981747
Epoch 24/100: Training Loss: 0.00021935101546999498
Epoch 25/100: Training Loss: 0.0005024834986655942
Epoch 26/100: Training Loss: 0.0003402882732004614
Epoch 27/100: Training Loss: 0.0002116551322321738
Epoch 28/100: Training Loss: 3.918216505821812e-06
Epoch 29/100: Training Loss: 0.0004723118746884957
Epoch 30/100: Training Loss: 4.337066655746803e-05
Epoch 31/100: Training Loss: 2.348032878703236e-05
Epoch 32/100: Training Loss: 0.0005295525910118208
Epoch 33/100: Training Loss: 5.296301654612963e-06
Epoch 34/100: Training Loss: 3.0500316468801366e-05
Epoch 35/100: Training Loss: 0.00046309166102914765
Epoch 36/100: Training Loss: 7.234929206733879e-05
Epoch 37/100: Training Loss: 0.0007412630704141432
Epoch 38/100: Training Loss: 0.0005124913794653756
Epoch 39/100: Training Loss: 0.00018775507739062682
Epoch 40/100: Training Loss: 6.682885926611687e-06
Epoch 41/100: Training Loss: 3.1350210549369934e-05
Epoch 42/100: Training Loss: 0.0008408650412537536
Epoch 43/100: Training Loss: 0.0001949792221394552
Epoch 44/100: Training Loss: 0.00012605044183917858
Epoch 45/100: Training Loss: 0.00038079477949625883
Epoch 46/100: Training Loss: 0.0018173294133305
Epoch 47/100: Training Loss: 0.00034679757422565865
Epoch 48/100: Training Loss: 1.2243303903786269e-05
Epoch 49/100: Training Loss: 0.00020568958625266079
Epoch 50/100: Training Loss: 0.00024002498608031032
Epoch 51/100: Training Loss: 0.00025045171311374086
Epoch 52/100: Training Loss: 4.999534321373783e-06
Epoch 53/100: Training Loss: 0.0003570794441183591
Epoch 54/100: Training Loss: 0.0003493134327198503
Epoch 55/100: Training Loss: 0.0002443744770942196
Epoch 56/100: Training Loss: 2.217865312978419e-06
Epoch 57/100: Training Loss: 3.4607024133754764e-05
Epoch 58/100: Training Loss: 0.00039727793585869574
Epoch 59/100: Training Loss: 0.00022877778531768905
Epoch 60/100: Training Loss: 0.00019918696328242254
Epoch 61/100: Training Loss: 0.00010259612570709896
Epoch 62/100: Training Loss: 6.943943691418468e-05
Epoch 63/100: Training Loss: 0.0001281009738071723
Epoch 64/100: Training Loss: 3.8817409031127455e-05
Epoch 65/100: Training Loss: 9.711129668121513e-05
Epoch 66/100: Training Loss: 9.690640571479972e-05
Epoch 67/100: Training Loss: 0.00013071928636818986
Epoch 68/100: Training Loss: 8.920447102614812e-05
Epoch 69/100: Training Loss: 0.00014096786898951376
Epoch 70/100: Training Loss: 0.00021086020335074393
Epoch 71/100: Training Loss: 5.324290687656073e-06
Epoch 72/100: Training Loss: 0.00010924434400923241
Epoch 73/100: Training Loss: 0.00025406157091465966
Epoch 74/100: Training Loss: 5.1785835541338415e-05
Epoch 75/100: Training Loss: 0.0006677972556259226
Epoch 76/100: Training Loss: 0.00031066050727246544
Epoch 77/100: Training Loss: 5.1067515452336607e-05
Epoch 78/100: Training Loss: 0.00014882442039278795
Epoch 79/100: Training Loss: 3.3238299152371794e-06
Epoch 80/100: Training Loss: 4.5392653703140225e-05
Epoch 81/100: Training Loss: 0.0001564279610660219
Epoch 82/100: Training Loss: 3.161549156162596e-05
Epoch 83/100: Training Loss: 9.845501919221218e-05
Epoch 84/100: Training Loss: 3.3181386008378e-05
Epoch 85/100: Training Loss: 0.00017100960565602174
Epoch 86/100: Training Loss: 0.00046094846890269334
Epoch 87/100: Training Loss: 2.6292772749052618e-05
Epoch 88/100: Training Loss: 0.00012123155566404492
Epoch 89/100: Training Loss: 2.2474451050642998e-05
Epoch 90/100: Training Loss: 7.212532830128472e-05
Epoch 91/100: Training Loss: 7.124668339155786e-05
Epoch 92/100: Training Loss: 0.00022733063879100957
Epoch 93/100: Training Loss: 0.0001478245318759971
Epoch 94/100: Training Loss: 0.0001543584335509533
Epoch 95/100: Training Loss: 0.00020643684171861218
Epoch 96/100: Training Loss: 0.00016839187678104173
Epoch 97/100: Training Loss: 0.00010683069924055706
Epoch 98/100: Training Loss: 4.928276341940675e-06
Epoch 99/100: Training Loss: 1.1194151856245533e-05
Epoch 0/100: Training Loss: 0.0007766104113739149
Epoch 1/100: Training Loss: 0.0006260499358177185
Epoch 2/100: Training Loss: 0.0002536097854639577
Epoch 3/100: Training Loss: 0.00042073257201540786
Epoch 4/100: Training Loss: 0.0002536220162843181
Epoch 5/100: Training Loss: 0.0001734835713838054
Epoch 6/100: Training Loss: 0.0001407759181693592
Epoch 7/100: Training Loss: 0.00032785851343543126
Epoch 8/100: Training Loss: 9.184706527574927e-05
Epoch 9/100: Training Loss: 9.296137213179495e-05
Epoch 10/100: Training Loss: 5.800297599186939e-05
Epoch 11/100: Training Loss: 0.00018448448550384658
Epoch 12/100: Training Loss: 6.48039137631391e-05
Epoch 13/100: Training Loss: 7.852861674222271e-05
Epoch 14/100: Training Loss: 0.0001647622382218859
Epoch 15/100: Training Loss: 0.00024420359229619524
Epoch 16/100: Training Loss: 1.8181188641923718e-05
Epoch 17/100: Training Loss: 2.1833683306400756e-05
Epoch 18/100: Training Loss: 4.595233770334615e-05
Epoch 19/100: Training Loss: 6.383973116104582e-05
Epoch 20/100: Training Loss: 4.862827883489364e-05
Epoch 21/100: Training Loss: 1.6825440529305325e-05
Epoch 22/100: Training Loss: 2.5858472109632155e-05
Epoch 23/100: Training Loss: 0.00013823421523634312
Epoch 24/100: Training Loss: 7.964402799848962e-05
Epoch 25/100: Training Loss: 0.0002404587030147029
Epoch 26/100: Training Loss: 4.90881379001436e-05
Epoch 27/100: Training Loss: 6.570802193827334e-05
Epoch 28/100: Training Loss: 0.00018569241912491554
Epoch 29/100: Training Loss: 7.420588713304132e-05
Epoch 30/100: Training Loss: 0.00023439941944274227
Epoch 31/100: Training Loss: 4.3010763593215856e-05
Epoch 32/100: Training Loss: 3.7462990401329194e-05
Epoch 33/100: Training Loss: 1.2009004872720853e-05
Epoch 34/100: Training Loss: 4.616989630513487e-05
Epoch 35/100: Training Loss: 1.3962316953173254e-05
Epoch 36/100: Training Loss: 2.220590532946903e-05
Epoch 37/100: Training Loss: 6.547103743110083e-05
Epoch 38/100: Training Loss: 4.8225807372183926e-05
Epoch 39/100: Training Loss: 1.9402729462733312e-05
Epoch 40/100: Training Loss: 2.7048092467331253e-05
Epoch 41/100: Training Loss: 3.4307529408056126e-05
Epoch 42/100: Training Loss: 2.1939406962653177e-05
Epoch 43/100: Training Loss: 0.00014628443570263619
Epoch 44/100: Training Loss: 3.998105175199762e-05
Epoch 45/100: Training Loss: 9.907514219526696e-05
Epoch 46/100: Training Loss: 4.3518598133747554e-05
Epoch 47/100: Training Loss: 3.388882367418403e-05
Epoch 48/100: Training Loss: 1.9470732494265633e-05
Epoch 49/100: Training Loss: 1.4097080565989017e-05
Epoch 50/100: Training Loss: 5.1416168236626985e-05
Epoch 51/100: Training Loss: 9.349914910519017e-05
Epoch 52/100: Training Loss: 0.00010281183089302705
Epoch 53/100: Training Loss: 0.00011210565545917612
Epoch 54/100: Training Loss: 0.00020046769517712888
Epoch 55/100: Training Loss: 1.0016458768364603e-05
Epoch 56/100: Training Loss: 0.00013231655863006557
Epoch 57/100: Training Loss: 1.5010143448887146e-05
Epoch 58/100: Training Loss: 2.1865298823181506e-05
Epoch 59/100: Training Loss: 2.322107784252251e-05
Epoch 60/100: Training Loss: 1.7196887823860202e-05
Epoch 61/100: Training Loss: 0.00011317531652418913
Epoch 62/100: Training Loss: 0.00010487697688879165
Epoch 63/100: Training Loss: 1.315130497765752e-05
Epoch 64/100: Training Loss: 3.3840116739800547e-05
Epoch 65/100: Training Loss: 1.2726502377638775e-05
Epoch 66/100: Training Loss: 1.2208752960494135e-05
Epoch 67/100: Training Loss: 0.00010460775048859352
Epoch 68/100: Training Loss: 1.540271342789705e-05
Epoch 69/100: Training Loss: 1.2336496626381326e-05
Epoch 70/100: Training Loss: 1.1030043706100072e-05
Epoch 71/100: Training Loss: 0.00010259711570971834
Epoch 72/100: Training Loss: 6.816879046701751e-05
Epoch 73/100: Training Loss: 1.6051177677195682e-05
Epoch 74/100: Training Loss: 4.446015701489111e-05
Epoch 75/100: Training Loss: 1.8587226740422502e-05
Epoch 76/100: Training Loss: 5.054177996595349e-05
Epoch 77/100: Training Loss: 2.074397202020725e-05
Epoch 78/100: Training Loss: 1.6184616421835613e-05
Epoch 79/100: Training Loss: 1.0222342870799842e-05
Epoch 80/100: Training Loss: 8.250864436932369e-05
Epoch 81/100: Training Loss: 1.7104736339729442e-05
Epoch 82/100: Training Loss: 1.4156645032025017e-05
Epoch 83/100: Training Loss: 2.9915101018495264e-06
Epoch 84/100: Training Loss: 9.543550060649889e-05
Epoch 85/100: Training Loss: 9.891623219030093e-05
Epoch 86/100: Training Loss: 1.6036114770820184e-05
Epoch 87/100: Training Loss: 5.616147104090294e-05
Epoch 88/100: Training Loss: 6.063145366950636e-06
Epoch 89/100: Training Loss: 5.433293661119136e-06
Epoch 90/100: Training Loss: 8.065104319722252e-05
Epoch 91/100: Training Loss: 9.980728536580517e-05
Epoch 92/100: Training Loss: 0.0003413730094918108
Epoch 93/100: Training Loss: 2.1949398075848553e-05
Epoch 94/100: Training Loss: 3.0101913612632625e-05
Epoch 95/100: Training Loss: 1.0980312728974144e-05
Epoch 96/100: Training Loss: 1.3295323093446483e-05
Epoch 97/100: Training Loss: 2.1103472836249698e-05
Epoch 98/100: Training Loss: 5.195723471553188e-06
Epoch 99/100: Training Loss: 1.4329768008494799e-05
Epoch 0/100: Training Loss: 0.0011475500504532202
Epoch 1/100: Training Loss: 0.0005992453194519864
Epoch 2/100: Training Loss: 0.000271491923540697
Epoch 3/100: Training Loss: 0.00038669645919928104
Epoch 4/100: Training Loss: 0.000657082405860114
Epoch 5/100: Training Loss: 0.0005182285078972445
Epoch 6/100: Training Loss: 0.00012192501782569115
Epoch 7/100: Training Loss: 0.0006747576554259912
Epoch 8/100: Training Loss: 0.0001786021273499647
Epoch 9/100: Training Loss: 0.00025168338565013866
Epoch 10/100: Training Loss: 0.0004136945901964812
Epoch 11/100: Training Loss: 0.0002518700176824903
Epoch 12/100: Training Loss: 0.00025389931528023007
Epoch 13/100: Training Loss: 0.0004673822711936027
Epoch 14/100: Training Loss: 0.00031283658181605317
Epoch 15/100: Training Loss: 0.0004454740895284131
Epoch 16/100: Training Loss: 0.0004704197025085244
Epoch 17/100: Training Loss: 0.0005145323517076638
Epoch 18/100: Training Loss: 0.00043325767656078254
Epoch 19/100: Training Loss: 0.0004083615447908239
Epoch 20/100: Training Loss: 0.00040988410267594685
Epoch 21/100: Training Loss: 0.0003828207874511924
Epoch 22/100: Training Loss: 0.00025924255094186073
Epoch 23/100: Training Loss: 7.09284247304292e-05
Epoch 24/100: Training Loss: 0.0004651474979426294
Epoch 25/100: Training Loss: 0.0004057761933236913
Epoch 26/100: Training Loss: 0.0002610823431891711
Epoch 27/100: Training Loss: 0.0001695439586992221
Epoch 28/100: Training Loss: 1.6283952827344026e-05
Epoch 29/100: Training Loss: 0.00025259265717904127
Epoch 30/100: Training Loss: 0.0003737283394475689
Epoch 31/100: Training Loss: 0.0003071940533248833
Epoch 32/100: Training Loss: 0.0003681281088713573
Epoch 33/100: Training Loss: 0.00020659130250391938
Epoch 34/100: Training Loss: 5.8880526856456634e-05
Epoch 35/100: Training Loss: 0.00028739582262766203
Epoch 36/100: Training Loss: 7.174545773743514e-05
Epoch 37/100: Training Loss: 0.00029399660269775735
Epoch 38/100: Training Loss: 0.00011339533923720031
Epoch 39/100: Training Loss: 0.00031183152188100086
Epoch 40/100: Training Loss: 4.594744133007099e-06
Epoch 41/100: Training Loss: 3.8487076625695676e-05
Epoch 42/100: Training Loss: 5.5290098748934107e-05
Epoch 43/100: Training Loss: 0.00030335246046562364
Epoch 44/100: Training Loss: 9.54493713218535e-05
Epoch 45/100: Training Loss: 0.00012984344810915635
Epoch 46/100: Training Loss: 0.00021420308960927442
Epoch 47/100: Training Loss: 0.0003518810654434923
Epoch 48/100: Training Loss: 0.00014496382509646393
Epoch 49/100: Training Loss: 3.428737770749314e-05
Epoch 50/100: Training Loss: 9.324451733063154e-06
Epoch 51/100: Training Loss: 3.2723284088442676e-05
Epoch 52/100: Training Loss: 0.00013177831043309694
Epoch 53/100: Training Loss: 0.00022044346872466562
Epoch 54/100: Training Loss: 5.996775617113028e-05
Epoch 55/100: Training Loss: 1.5734796174238082e-05
Epoch 56/100: Training Loss: 0.00011856397070959545
Epoch 57/100: Training Loss: 1.7234406145832464e-06
Epoch 58/100: Training Loss: 3.2251972220671015e-05
Epoch 59/100: Training Loss: 0.0002710626130681401
Epoch 60/100: Training Loss: 3.9303240955143235e-05
Epoch 61/100: Training Loss: 6.55434090193077e-05
Epoch 62/100: Training Loss: 0.00016185941984835227
Epoch 63/100: Training Loss: 0.00020369706934343006
Epoch 64/100: Training Loss: 5.243392452397154e-05
Epoch 65/100: Training Loss: 0.00016182173260658847
Epoch 66/100: Training Loss: 3.8342191232159536e-05
Epoch 67/100: Training Loss: 4.903298923787515e-05
Epoch 68/100: Training Loss: 0.00010131755551415173
Epoch 69/100: Training Loss: 0.00011168046115225206
Epoch 70/100: Training Loss: 1.2375301610461266e-05
Epoch 71/100: Training Loss: 0.00023744145049107984
Epoch 72/100: Training Loss: 0.00011190392845414679
Epoch 73/100: Training Loss: 0.00017375716179475657
Epoch 74/100: Training Loss: 5.6977768015166566e-05
Epoch 75/100: Training Loss: 0.00011029592276688649
Epoch 76/100: Training Loss: 0.00017108671333757752
Epoch 77/100: Training Loss: 0.00010567896836541693
Epoch 78/100: Training Loss: 6.548073059240265e-05
Epoch 79/100: Training Loss: 0.0002558038240056401
Epoch 80/100: Training Loss: 8.97723713664196e-05
Epoch 81/100: Training Loss: 5.945908699201362e-05
Epoch 82/100: Training Loss: 9.705774452654235e-05
Epoch 83/100: Training Loss: 5.882691107523281e-05
Epoch 84/100: Training Loss: 9.642154888788681e-06
Epoch 85/100: Training Loss: 7.931100446814379e-05
Epoch 86/100: Training Loss: 1.903882066431067e-05
Epoch 87/100: Training Loss: 7.814939881386778e-05
Epoch 88/100: Training Loss: 3.4337349636939606e-05
Epoch 89/100: Training Loss: 6.532714171794498e-05
Epoch 90/100: Training Loss: 0.00016183249084404232
Epoch 91/100: Training Loss: 0.00010151140425237306
Epoch 92/100: Training Loss: 1.8975624371938107e-05
Epoch 93/100: Training Loss: 4.6220633113598075e-05
Epoch 94/100: Training Loss: 0.00010511729314990107
Epoch 95/100: Training Loss: 9.01353031797794e-05
Epoch 96/100: Training Loss: 0.00020155253832650292
Epoch 97/100: Training Loss: 0.0001939574732641468
Epoch 98/100: Training Loss: 2.8564993525968005e-05
Epoch 99/100: Training Loss: 3.970622799188032e-05
dataset: cities layer_num_from_end:-9 Avg_acc:tensor(0.7796) Avg_AUC:0.8703695784072338 Avg_threshold:0.24604825675487518
dataset: inventions layer_num_from_end:-9 Avg_acc:tensor(0.6541) Avg_AUC:0.8826058754603281 Avg_threshold:0.11316221207380295
dataset: elements layer_num_from_end:-9 Avg_acc:tensor(0.6505) Avg_AUC:0.6838385940571164 Avg_threshold:0.804347813129425
dataset: animals layer_num_from_end:-9 Avg_acc:tensor(0.6141) Avg_AUC:0.7657588498362309 Avg_threshold:0.8406447172164917
dataset: companies layer_num_from_end:-9 Avg_acc:tensor(0.7508) Avg_AUC:0.8525277777777778 Avg_threshold:0.43471625447273254
dataset: facts layer_num_from_end:-9 Avg_acc:tensor(0.7451) Avg_AUC:0.8683465761032083 Avg_threshold:0.9365063905715942
dataset: conj_neg_facts layer_num_from_end:-9 Avg_acc:tensor(0.7451) Avg_AUC:0.611505471753919 Avg_threshold:0.2934129536151886


================layer -13================
Epoch 0/100: Training Loss: 0.005100160837173462
Epoch 1/100: Training Loss: 0.0047943401031005075
Epoch 2/100: Training Loss: 0.004844279243395879
Epoch 3/100: Training Loss: 0.004668377912961519
Epoch 4/100: Training Loss: 0.0037851287768437313
Epoch 5/100: Training Loss: 0.004328223757254772
Epoch 6/100: Training Loss: 0.0030796871735499455
Epoch 7/100: Training Loss: 0.0024630650877952576
Epoch 8/100: Training Loss: 0.002703695725171994
Epoch 9/100: Training Loss: 0.002068109237230741
Epoch 10/100: Training Loss: 0.0017656227334951742
Epoch 11/100: Training Loss: 0.001360672597701733
Epoch 12/100: Training Loss: 0.0016973821016458364
Epoch 13/100: Training Loss: 0.0013039994698304397
Epoch 14/100: Training Loss: 0.004433389657582992
Epoch 15/100: Training Loss: 0.001300737643853212
Epoch 16/100: Training Loss: 0.0008129626512527466
Epoch 17/100: Training Loss: 0.0006244984479286732
Epoch 18/100: Training Loss: 0.0007687840515222305
Epoch 19/100: Training Loss: 0.0009480538085485116
Epoch 20/100: Training Loss: 0.0005220001420149436
Epoch 21/100: Training Loss: 0.0015638430531208331
Epoch 22/100: Training Loss: 0.00205325736449315
Epoch 23/100: Training Loss: 0.0002876046615151259
Epoch 24/100: Training Loss: 0.0003817901731683658
Epoch 25/100: Training Loss: 0.0004700193993556194
Epoch 26/100: Training Loss: 0.0003743216825219301
Epoch 27/100: Training Loss: 0.000258058739396242
Epoch 28/100: Training Loss: 0.0006019301139391386
Epoch 29/100: Training Loss: 0.0004823714590225464
Epoch 30/100: Training Loss: 0.00032344374519128067
Epoch 31/100: Training Loss: 0.00023081607352464626
Epoch 32/100: Training Loss: 0.0001170031774120453
Epoch 33/100: Training Loss: 0.0002016063588551986
Epoch 34/100: Training Loss: 0.00019822325796270982
Epoch 35/100: Training Loss: 0.00013381403942520803
Epoch 36/100: Training Loss: 0.00019127371697089612
Epoch 37/100: Training Loss: 0.0005100713803982123
Epoch 38/100: Training Loss: 7.058713489618056e-05
Epoch 39/100: Training Loss: 0.00011644266450252288
Epoch 40/100: Training Loss: 0.00021048468083907396
Epoch 41/100: Training Loss: 3.300287211552644e-05
Epoch 42/100: Training Loss: 0.00014122107472175206
Epoch 43/100: Training Loss: 0.0001386010493987646
Epoch 44/100: Training Loss: 2.183538013830399e-05
Epoch 45/100: Training Loss: 3.464056107287224e-05
Epoch 46/100: Training Loss: 3.872519263472313e-05
Epoch 47/100: Training Loss: 0.00016100697505932587
Epoch 48/100: Training Loss: 5.189504307241012e-05
Epoch 49/100: Training Loss: 0.003991943139296312
Epoch 50/100: Training Loss: 3.4073075184073205e-05
Epoch 51/100: Training Loss: 3.3202276636774724e-05
Epoch 52/100: Training Loss: 1.820899510326294e-05
Epoch 53/100: Training Loss: 3.515700332056254e-05
Epoch 54/100: Training Loss: 4.545552059052846e-05
Epoch 55/100: Training Loss: 9.244242205451697e-05
Epoch 56/100: Training Loss: 9.475605419048897e-05
Epoch 57/100: Training Loss: 4.550625379077899e-05
Epoch 58/100: Training Loss: 0.00010414355888198584
Epoch 59/100: Training Loss: 3.7014403810294774e-05
Epoch 60/100: Training Loss: 7.036879945259828e-05
Epoch 61/100: Training Loss: 7.660730550877559e-05
Epoch 62/100: Training Loss: 4.793836687428829e-05
Epoch 63/100: Training Loss: 0.00017846634802527918
Epoch 64/100: Training Loss: 2.9835485829374728e-05
Epoch 65/100: Training Loss: 7.767269077400366e-06
Epoch 66/100: Training Loss: 2.91953000645989e-05
Epoch 67/100: Training Loss: 0.004363801234807723
Epoch 68/100: Training Loss: 0.0019801555153651116
Epoch 69/100: Training Loss: 5.18626318528102e-05
Epoch 70/100: Training Loss: 0.0001753082689948571
Epoch 71/100: Training Loss: 7.204108465558443e-05
Epoch 72/100: Training Loss: 8.285224724274415e-05
Epoch 73/100: Training Loss: 4.473931562059965e-05
Epoch 74/100: Training Loss: 3.975061460947379e-05
Epoch 75/100: Training Loss: 5.1711578495227375e-05
Epoch 76/100: Training Loss: 9.191506661665745e-05
Epoch 77/100: Training Loss: 2.259243942367343e-05
Epoch 78/100: Training Loss: 2.432023682512152e-05
Epoch 79/100: Training Loss: 2.96675380713378e-06
Epoch 80/100: Training Loss: 1.0840514173301366e-05
Epoch 81/100: Training Loss: 3.454567960057503e-06
Epoch 82/100: Training Loss: 4.019949418038894e-05
Epoch 83/100: Training Loss: 6.309215528651689e-05
Epoch 84/100: Training Loss: 1.9733482100165042e-05
Epoch 85/100: Training Loss: 1.1395760035763184e-05
Epoch 86/100: Training Loss: 1.137715224057245e-05
Epoch 87/100: Training Loss: 0.0002671094276966193
Epoch 88/100: Training Loss: 5.760734217265286e-06
Epoch 89/100: Training Loss: 0.00019075036144409425
Epoch 90/100: Training Loss: 3.07602085507451e-05
Epoch 91/100: Training Loss: 4.875076075012867e-05
Epoch 92/100: Training Loss: 7.908442057669163e-06
Epoch 93/100: Training Loss: 1.916663351062781e-05
Epoch 94/100: Training Loss: 1.4673122421253281e-06
Epoch 95/100: Training Loss: 0.0005830685393168376
Epoch 96/100: Training Loss: 0.00021467985919652841
Epoch 97/100: Training Loss: 0.0005297173196688676
Epoch 98/100: Training Loss: 1.4641762484247103e-05
Epoch 99/100: Training Loss: 1.7538668217662817e-05
Epoch 0/100: Training Loss: 0.0002726274688682214
Epoch 1/100: Training Loss: 0.000504990196014199
Epoch 2/100: Training Loss: 0.0002700095754033247
Epoch 3/100: Training Loss: 0.0005390731609455674
Epoch 4/100: Training Loss: 0.0002986732285653529
Epoch 5/100: Training Loss: 0.0002886150115808564
Epoch 6/100: Training Loss: 1.993595576540237e-05
Epoch 7/100: Training Loss: 0.00010337242898385087
Epoch 8/100: Training Loss: 6.559625635499912e-05
Epoch 9/100: Training Loss: 0.0002563747122148762
Epoch 10/100: Training Loss: 0.00021848708992581732
Epoch 11/100: Training Loss: 0.0002790153728204992
Epoch 12/100: Training Loss: 6.980209448479217e-06
Epoch 13/100: Training Loss: 0.0001857294429578054
Epoch 14/100: Training Loss: 5.4247702384208884e-05
Epoch 15/100: Training Loss: 7.681750730014168e-05
Epoch 16/100: Training Loss: 0.0002677667501795987
Epoch 17/100: Training Loss: 2.5034376791774425e-05
Epoch 18/100: Training Loss: 0.0002140788854237629
Epoch 19/100: Training Loss: 0.00018954183488683316
Epoch 20/100: Training Loss: 1.1835739563632706e-06
Epoch 21/100: Training Loss: 0.00013917362977303733
Epoch 22/100: Training Loss: 4.084759512957971e-05
Epoch 23/100: Training Loss: 1.9407672431704176e-05
Epoch 24/100: Training Loss: 0.0001792826192795963
Epoch 25/100: Training Loss: 0.00018778520181040058
Epoch 26/100: Training Loss: 0.00013438406747018274
Epoch 27/100: Training Loss: 0.00020785922092707168
Epoch 28/100: Training Loss: 0.00019719267907164023
Epoch 29/100: Training Loss: 6.18625966823689e-05
Epoch 30/100: Training Loss: 7.12880071235879e-05
Epoch 31/100: Training Loss: 0.00021311580600225338
Epoch 32/100: Training Loss: 3.37095087552819e-05
Epoch 33/100: Training Loss: 0.00022432874604190947
Epoch 34/100: Training Loss: 5.819521126057535e-05
Epoch 35/100: Training Loss: 1.3003319986331624e-05
Epoch 36/100: Training Loss: 0.00029890062162159806
Epoch 37/100: Training Loss: 0.00010045762320003168
Epoch 38/100: Training Loss: 2.3899345153383074e-05
Epoch 39/100: Training Loss: 3.7172178616705495e-05
Epoch 40/100: Training Loss: 0.00017919979421547177
Epoch 41/100: Training Loss: 0.0020417873100314976
Epoch 42/100: Training Loss: 0.00014946397577700592
Epoch 43/100: Training Loss: 0.00016452256807297335
Epoch 44/100: Training Loss: 0.0001175930397793851
Epoch 45/100: Training Loss: 8.264136387895576e-05
Epoch 46/100: Training Loss: 9.161172092228193e-05
Epoch 47/100: Training Loss: 7.971390380185816e-05
Epoch 48/100: Training Loss: 1.276760349559677e-05
Epoch 49/100: Training Loss: 0.00020751664456765214
Epoch 50/100: Training Loss: 2.975253630513033e-05
Epoch 51/100: Training Loss: 4.788285388607081e-06
Epoch 52/100: Training Loss: 6.127079756671538e-05
Epoch 53/100: Training Loss: 5.564336234812245e-05
Epoch 54/100: Training Loss: 2.544463482672858e-05
Epoch 55/100: Training Loss: 3.596880961827633e-05
Epoch 56/100: Training Loss: 2.6964699440205577e-05
Epoch 57/100: Training Loss: 0.00010546906579770314
Epoch 58/100: Training Loss: 6.928831660460196e-06
Epoch 59/100: Training Loss: 5.177901264263375e-05
Epoch 60/100: Training Loss: 7.372959245480764e-05
Epoch 61/100: Training Loss: 6.539433893868741e-05
Epoch 62/100: Training Loss: 0.0001841164757852597
Epoch 63/100: Training Loss: 7.57645949253587e-05
Epoch 64/100: Training Loss: 0.00015348580252429295
Epoch 65/100: Training Loss: 0.000130400003138679
Epoch 66/100: Training Loss: 5.501284861243894e-05
Epoch 67/100: Training Loss: 0.00012988000774062801
Epoch 68/100: Training Loss: 1.0656064476467033e-05
Epoch 69/100: Training Loss: 0.0001703436933290798
Epoch 70/100: Training Loss: 9.83017910221767e-05
Epoch 71/100: Training Loss: 0.0001801203919633087
Epoch 72/100: Training Loss: 3.068915864811884e-05
Epoch 73/100: Training Loss: 7.482666538969818e-05
Epoch 74/100: Training Loss: 0.0001349098262097269
Epoch 75/100: Training Loss: 0.00010275929297566948
Epoch 76/100: Training Loss: 0.00015478350656449526
Epoch 77/100: Training Loss: 0.00013329340938495414
Epoch 78/100: Training Loss: 0.0001064157322249605
Epoch 79/100: Training Loss: 0.0001059776514501315
Epoch 80/100: Training Loss: 0.0003594967281871847
Epoch 81/100: Training Loss: 0.0001975830159914333
Epoch 82/100: Training Loss: 0.000842937974117262
Epoch 83/100: Training Loss: 6.787268436543076e-05
Epoch 84/100: Training Loss: 0.0001078302984547722
Epoch 85/100: Training Loss: 7.343437572765778e-05
Epoch 86/100: Training Loss: 2.713807578712301e-05
Epoch 87/100: Training Loss: 5.283651063260476e-05
Epoch 88/100: Training Loss: 3.417027537865489e-05
Epoch 89/100: Training Loss: 5.8862259569724045e-05
Epoch 90/100: Training Loss: 1.6499592634460853e-05
Epoch 91/100: Training Loss: 2.99169794727334e-05
Epoch 92/100: Training Loss: 0.00011149917482795203
Epoch 93/100: Training Loss: 2.20083907213179e-05
Epoch 94/100: Training Loss: 9.511790871035492e-07
Epoch 95/100: Training Loss: 5.3654804892604124e-05
Epoch 96/100: Training Loss: 3.252486017586939e-05
Epoch 97/100: Training Loss: 0.00014156968470646126
Epoch 98/100: Training Loss: 6.235722920151569e-05
Epoch 99/100: Training Loss: 0.00016750939892011906
Epoch 0/100: Training Loss: 0.0007362473604366251
Epoch 1/100: Training Loss: 0.0008248288976660681
Epoch 2/100: Training Loss: 0.0008201009817252871
Epoch 3/100: Training Loss: 0.0008216500282287598
Epoch 4/100: Training Loss: 0.0007839326944825875
Epoch 5/100: Training Loss: 0.0008771422222189234
Epoch 6/100: Training Loss: 0.0007843667835132029
Epoch 7/100: Training Loss: 0.0008453471897953775
Epoch 8/100: Training Loss: 0.0008269399404525757
Epoch 9/100: Training Loss: 0.0007470540061795334
Epoch 10/100: Training Loss: 0.0008483000470502344
Epoch 11/100: Training Loss: 0.0006411659501796395
Epoch 12/100: Training Loss: 0.0005893435548333561
Epoch 13/100: Training Loss: 0.0001611989412911877
Epoch 14/100: Training Loss: 0.00011049077858752255
Epoch 15/100: Training Loss: 0.00014559718952998857
Epoch 16/100: Training Loss: 0.0006418226126632
Epoch 17/100: Training Loss: 0.00042270737535813274
Epoch 18/100: Training Loss: 0.00013026659170426935
Epoch 19/100: Training Loss: 7.846336221802827e-05
Epoch 20/100: Training Loss: 0.0003915906491862163
Epoch 21/100: Training Loss: 0.00025182551118583163
Epoch 22/100: Training Loss: 0.00013374652696680699
Epoch 23/100: Training Loss: 0.000265642629759344
Epoch 24/100: Training Loss: 0.00019206892176451188
Epoch 25/100: Training Loss: 0.00027342801450064817
Epoch 26/100: Training Loss: 0.00016327992890754976
Epoch 27/100: Training Loss: 4.581814606534949e-05
Epoch 28/100: Training Loss: 7.707897017444421e-05
Epoch 29/100: Training Loss: 0.00019139037952164178
Epoch 30/100: Training Loss: 6.61076699716473e-05
Epoch 31/100: Training Loss: 4.666135119636674e-05
Epoch 32/100: Training Loss: 2.674869265896163e-05
Epoch 33/100: Training Loss: 0.00031588405235860145
Epoch 34/100: Training Loss: 4.352572715390322e-05
Epoch 35/100: Training Loss: 2.7728001523880935e-05
Epoch 36/100: Training Loss: 0.00012583192016474263
Epoch 37/100: Training Loss: 0.00014495104551315308
Epoch 38/100: Training Loss: 6.0829283391458415e-05
Epoch 39/100: Training Loss: 1.6606768921894184e-05
Epoch 40/100: Training Loss: 0.00010067895391947543
Epoch 41/100: Training Loss: 9.95180455807647e-05
Epoch 42/100: Training Loss: 0.00010134365696173448
Epoch 43/100: Training Loss: 1.6104288786919407e-05
Epoch 44/100: Training Loss: 7.010468732717351e-05
Epoch 45/100: Training Loss: 8.132649223189548e-05
Epoch 46/100: Training Loss: 4.178613036601252e-05
Epoch 47/100: Training Loss: 3.732008463387036e-05
Epoch 48/100: Training Loss: 0.00014497840364054856
Epoch 49/100: Training Loss: 2.0693492447763546e-05
Epoch 50/100: Training Loss: 6.735784199712503e-05
Epoch 51/100: Training Loss: 1.7324071672504844e-05
Epoch 52/100: Training Loss: 3.747192392656706e-05
Epoch 53/100: Training Loss: 8.215456863873685e-05
Epoch 54/100: Training Loss: 4.139560805760088e-06
Epoch 55/100: Training Loss: 9.513792173085709e-05
Epoch 56/100: Training Loss: 3.3184557276613574e-05
Epoch 57/100: Training Loss: 5.107210333800424e-05
Epoch 58/100: Training Loss: 1.823424669277614e-05
Epoch 59/100: Training Loss: 4.9239062447084024e-05
Epoch 60/100: Training Loss: 0.00013786016613649566
Epoch 61/100: Training Loss: 5.560705429828005e-05
Epoch 62/100: Training Loss: 2.948713922932137e-05
Epoch 63/100: Training Loss: 9.521483801878415e-05
Epoch 64/100: Training Loss: 4.632872997096221e-05
Epoch 65/100: Training Loss: 0.00023403113109493688
Epoch 66/100: Training Loss: 7.980389360389019e-05
Epoch 67/100: Training Loss: 3.380293439551176e-05
Epoch 68/100: Training Loss: 5.115783558442043e-05
Epoch 69/100: Training Loss: 8.46663329800869e-05
Epoch 70/100: Training Loss: 1.8294607773760327e-05
Epoch 71/100: Training Loss: 5.9012997878622686e-05
Epoch 72/100: Training Loss: 1.2443818118237802e-05
Epoch 73/100: Training Loss: 2.673401063249122e-05
Epoch 74/100: Training Loss: 5.8609174724617696e-05
Epoch 75/100: Training Loss: 4.602332780668639e-05
Epoch 76/100: Training Loss: 4.943804099009587e-05
Epoch 77/100: Training Loss: 5.124384596337021e-05
Epoch 78/100: Training Loss: 3.8937227252651664e-05
Epoch 79/100: Training Loss: 3.399679909262182e-05
Epoch 80/100: Training Loss: 0.00015232063054499044
Epoch 81/100: Training Loss: 3.7468084854777584e-05
Epoch 82/100: Training Loss: 7.995561911509588e-05
Epoch 83/100: Training Loss: 0.00010057663486014664
Epoch 84/100: Training Loss: 2.856327987769071e-05
Epoch 85/100: Training Loss: 2.6306754812530803e-05
Epoch 86/100: Training Loss: 0.00013444152959871076
Epoch 87/100: Training Loss: 0.00047812182709102715
Epoch 88/100: Training Loss: 3.117193547983515e-05
Epoch 89/100: Training Loss: 2.8991886326090782e-05
Epoch 90/100: Training Loss: 5.092900836359861e-05
Epoch 91/100: Training Loss: 3.04371030048817e-05
Epoch 92/100: Training Loss: 3.652685064805579e-05
Epoch 93/100: Training Loss: 1.2552505780466541e-05
Epoch 94/100: Training Loss: 6.963058513888406e-05
Epoch 95/100: Training Loss: 1.5699427903575054e-05
Epoch 96/100: Training Loss: 5.4241254993153916e-05
Epoch 97/100: Training Loss: 7.501587364401213e-06
Epoch 98/100: Training Loss: 3.729289928577604e-05
Epoch 99/100: Training Loss: 1.462937303190857e-05
Epoch 0/100: Training Loss: 0.00022101425989107652
Epoch 1/100: Training Loss: 0.00016828788952393965
Epoch 2/100: Training Loss: 6.769106469371103e-05
Epoch 3/100: Training Loss: 0.0003064499321309003
Epoch 4/100: Training Loss: 1.9844680685888638e-05
Epoch 5/100: Training Loss: 0.0002117932520129464
Epoch 6/100: Training Loss: 3.276378424330191e-05
Epoch 7/100: Training Loss: 2.1698008376089008e-05
Epoch 8/100: Training Loss: 1.7570929644121364e-05
Epoch 9/100: Training Loss: 2.6366796175187283e-05
Epoch 10/100: Training Loss: 2.958645663139495e-05
Epoch 11/100: Training Loss: 3.2700066962702705e-05
Epoch 12/100: Training Loss: 6.137181242758577e-05
Epoch 13/100: Training Loss: 3.6080333996902814e-05
Epoch 14/100: Training Loss: 2.8350064530968667e-05
Epoch 15/100: Training Loss: 4.550929350609129e-05
Epoch 16/100: Training Loss: 3.6963181231509554e-05
Epoch 17/100: Training Loss: 4.889879545027559e-05
Epoch 18/100: Training Loss: 3.0642527748237956e-05
Epoch 19/100: Training Loss: 1.3447921215133234e-05
Epoch 20/100: Training Loss: 8.153064515102993e-06
Epoch 21/100: Training Loss: 1.4243948019363663e-05
Epoch 22/100: Training Loss: 1.0917987674474717e-05
Epoch 23/100: Training Loss: 2.071484923362732e-05
Epoch 24/100: Training Loss: 4.030744259415025e-06
Epoch 25/100: Training Loss: 6.944507847286083e-06
Epoch 26/100: Training Loss: 6.565278057347644e-06
Epoch 27/100: Training Loss: 4.7687525776299566e-05
Epoch 28/100: Training Loss: 1.697249879891222e-05
Epoch 29/100: Training Loss: 3.291425739668987e-06
Epoch 30/100: Training Loss: 1.4507332393391566e-05
Epoch 31/100: Training Loss: 1.530190718106248e-05
Epoch 32/100: Training Loss: 7.011499044231393e-06
Epoch 33/100: Training Loss: 2.804379619192332e-07
Epoch 34/100: Training Loss: 2.882546025582335e-06
Epoch 35/100: Training Loss: 9.547008878805421e-06
Epoch 36/100: Training Loss: 5.707644264806401e-06
Epoch 37/100: Training Loss: 0.00026016855104403063
Epoch 38/100: Training Loss: 7.706002162938769e-06
Epoch 39/100: Training Loss: 1.965522427450527e-05
Epoch 40/100: Training Loss: 2.8801309368149802e-05
Epoch 41/100: Training Loss: 0.0002421583810990507
Epoch 42/100: Training Loss: 2.6582964611324397e-05
Epoch 43/100: Training Loss: 0.00021436675028367475
Epoch 44/100: Training Loss: 1.9199898815713825e-06
Epoch 45/100: Training Loss: 4.432346163825555e-06
Epoch 46/100: Training Loss: 3.373216710646044e-05
Epoch 47/100: Training Loss: 0.0001951000229878859
Epoch 48/100: Training Loss: 1.1834998572753234e-05
Epoch 49/100: Training Loss: 0.00022850124673409896
Epoch 50/100: Training Loss: 8.852988883683628e-06
Epoch 51/100: Training Loss: 8.219174130565741e-06
Epoch 52/100: Training Loss: 1.7167227766053243e-06
Epoch 53/100: Training Loss: 1.2368405639955943e-06
Epoch 54/100: Training Loss: 3.010059365558184e-07
Epoch 55/100: Training Loss: 2.1972764939577743e-06
Epoch 56/100: Training Loss: 1.4840246876701712e-06
Epoch 57/100: Training Loss: 1.1874786154790358e-06
Epoch 58/100: Training Loss: 0.00019507482647895813
Epoch 59/100: Training Loss: 5.36305438303812e-06
Epoch 60/100: Training Loss: 2.625696519813077e-06
Epoch 61/100: Training Loss: 7.650286451363089e-07
Epoch 62/100: Training Loss: 1.531635379334065e-07
Epoch 63/100: Training Loss: 2.4288882162760605e-06
Epoch 64/100: Training Loss: 2.5503093969415535e-06
Epoch 65/100: Training Loss: 1.9365592214109546e-06
Epoch 66/100: Training Loss: 3.317851885433563e-07
Epoch 67/100: Training Loss: 3.946752192198553e-06
Epoch 68/100: Training Loss: 2.6098928752947936e-06
Epoch 69/100: Training Loss: 5.268538106148216e-07
Epoch 70/100: Training Loss: 3.86243118555285e-07
Epoch 71/100: Training Loss: 1.397733682427894e-06
Epoch 72/100: Training Loss: 2.848887313368984e-07
Epoch 73/100: Training Loss: 4.7059775996868585e-07
Epoch 74/100: Training Loss: 8.417640690988099e-07
Epoch 75/100: Training Loss: 9.285381317815998e-07
Epoch 76/100: Training Loss: 9.738062296740033e-08
Epoch 77/100: Training Loss: 1.4612203151707285e-07
Epoch 78/100: Training Loss: 2.7301296292783013e-07
Epoch 79/100: Training Loss: 0.00015518866818059574
Epoch 80/100: Training Loss: 1.184231421740895e-06
Epoch 81/100: Training Loss: 3.327009670267051e-06
Epoch 82/100: Training Loss: 4.735728725790977e-06
Epoch 83/100: Training Loss: 8.307476739653132e-07
Epoch 84/100: Training Loss: 1.2102470712058924e-06
Epoch 85/100: Training Loss: 1.2837117537856102e-06
Epoch 86/100: Training Loss: 6.361881704916331e-07
Epoch 87/100: Training Loss: 1.4974688052792441e-07
Epoch 88/100: Training Loss: 1.1643885624256324e-06
Epoch 89/100: Training Loss: 2.5028542784804648e-06
Epoch 90/100: Training Loss: 1.1203498367897488e-06
Epoch 91/100: Training Loss: 1.0792008569379421e-06
Epoch 92/100: Training Loss: 4.0853767793371594e-07
Epoch 93/100: Training Loss: 9.954226499592716e-06
Epoch 94/100: Training Loss: 1.666549757249992e-06
Epoch 95/100: Training Loss: 2.428000020286576e-06
Epoch 96/100: Training Loss: 2.7225323719903827e-06
Epoch 97/100: Training Loss: 6.917978382923386e-06
Epoch 98/100: Training Loss: 2.7693659913810815e-06
Epoch 99/100: Training Loss: 1.594687769697471e-06
Epoch 0/100: Training Loss: 3.613231162871084e-05
Epoch 1/100: Training Loss: 9.050094930257666e-05
Epoch 2/100: Training Loss: 0.00014989912372580323
Epoch 3/100: Training Loss: 1.9017222856733655e-05
Epoch 4/100: Training Loss: 5.2728617246249854e-05
Epoch 5/100: Training Loss: 5.315370377032987e-05
Epoch 6/100: Training Loss: 7.5125972384132e-06
Epoch 7/100: Training Loss: 0.0002172295913992939
Epoch 8/100: Training Loss: 0.00010668955880650727
Epoch 9/100: Training Loss: 7.230331820826376e-05
Epoch 10/100: Training Loss: 1.1829144325673855e-05
Epoch 11/100: Training Loss: 6.838605636172855e-06
Epoch 12/100: Training Loss: 1.823747195818457e-05
Epoch 13/100: Training Loss: 0.00012474252159969047
Epoch 14/100: Training Loss: 0.00016126242650818715
Epoch 15/100: Training Loss: 0.0001788146896845734
Epoch 16/100: Training Loss: 4.377911802947796e-05
Epoch 17/100: Training Loss: 6.333385349556048e-05
Epoch 18/100: Training Loss: 8.06694189386983e-05
Epoch 19/100: Training Loss: 1.0435806355580756e-05
Epoch 20/100: Training Loss: 1.7959450710616353e-05
Epoch 21/100: Training Loss: 4.5691805672810375e-05
Epoch 22/100: Training Loss: 2.973813838261064e-06
Epoch 23/100: Training Loss: 1.51541204221787e-06
Epoch 24/100: Training Loss: 0.00018656006606493128
Epoch 25/100: Training Loss: 4.8503571528992894e-05
Epoch 26/100: Training Loss: 0.00021372405889396844
Epoch 27/100: Training Loss: 0.0002753414076319488
Epoch 28/100: Training Loss: 0.0002448832796465966
Epoch 29/100: Training Loss: 0.00022383473710530365
Epoch 30/100: Training Loss: 1.3984611026153037e-05
Epoch 31/100: Training Loss: 0.0002682275280425076
Epoch 32/100: Training Loss: 0.00010889860445178599
Epoch 33/100: Training Loss: 0.00026391096378801057
Epoch 34/100: Training Loss: 0.00017132713086044732
Epoch 35/100: Training Loss: 0.00012379598027000777
Epoch 36/100: Training Loss: 0.00028202517928066343
Epoch 37/100: Training Loss: 0.00018250102752364727
Epoch 38/100: Training Loss: 0.0001812786516231326
Epoch 39/100: Training Loss: 1.9663331111729968e-05
Epoch 40/100: Training Loss: 2.869088116878738e-05
Epoch 41/100: Training Loss: 2.545522620326363e-05
Epoch 42/100: Training Loss: 0.0002366986451885118
Epoch 43/100: Training Loss: 0.00010757318221478968
Epoch 44/100: Training Loss: 0.00015041412365052008
Epoch 45/100: Training Loss: 4.221481798980642e-05
Epoch 46/100: Training Loss: 0.00018168120614943967
Epoch 47/100: Training Loss: 0.00016651220173330352
Epoch 48/100: Training Loss: 0.00013787980075530742
Epoch 49/100: Training Loss: 0.0001289867002019135
Epoch 50/100: Training Loss: 0.00011116051660155372
Epoch 51/100: Training Loss: 3.0183310573276835e-05
Epoch 52/100: Training Loss: 9.8692974255931e-05
Epoch 53/100: Training Loss: 9.085795915071865e-05
Epoch 54/100: Training Loss: 3.809620484641071e-05
Epoch 55/100: Training Loss: 0.00010959557708232634
Epoch 56/100: Training Loss: 0.0001330735153316902
Epoch 57/100: Training Loss: 8.906447221331882e-05
Epoch 58/100: Training Loss: 0.00013844034535818935
Epoch 59/100: Training Loss: 8.253108662840682e-06
Epoch 60/100: Training Loss: 9.253856292518054e-05
Epoch 61/100: Training Loss: 6.109119010960452e-05
Epoch 62/100: Training Loss: 2.4043006419036794e-05
Epoch 63/100: Training Loss: 1.526682332913447e-05
Epoch 64/100: Training Loss: 2.8013728112669035e-05
Epoch 65/100: Training Loss: 7.1183527274752546e-06
Epoch 66/100: Training Loss: 2.6043306181554936e-07
Epoch 67/100: Training Loss: 1.9234828307606656e-05
Epoch 68/100: Training Loss: 1.6174622498074983e-05
Epoch 69/100: Training Loss: 1.304918333613378e-05
Epoch 70/100: Training Loss: 8.118055087046415e-06
Epoch 71/100: Training Loss: 3.811425190367457e-05
Epoch 72/100: Training Loss: 2.4618855613167936e-05
Epoch 73/100: Training Loss: 1.277212023494705e-05
Epoch 74/100: Training Loss: 3.8854953777130846e-05
Epoch 75/100: Training Loss: 2.0012959048220638e-05
Epoch 76/100: Training Loss: 7.294691790084136e-05
Epoch 77/100: Training Loss: 5.8025371686722825e-06
Epoch 78/100: Training Loss: 6.446979236080899e-05
Epoch 79/100: Training Loss: 4.107771604429192e-05
Epoch 80/100: Training Loss: 6.599361956771892e-06
Epoch 81/100: Training Loss: 5.07312060724343e-06
Epoch 82/100: Training Loss: 1.7874345209123354e-05
Epoch 83/100: Training Loss: 7.47633746005423e-05
Epoch 84/100: Training Loss: 4.219262933676144e-05
Epoch 85/100: Training Loss: 3.189015446857373e-05
Epoch 86/100: Training Loss: 5.6269239582773724e-05
Epoch 87/100: Training Loss: 5.020393479254938e-05
Epoch 88/100: Training Loss: 5.97305321199004e-05
Epoch 89/100: Training Loss: 2.8333729690945093e-06
Epoch 90/100: Training Loss: 5.408057144709996e-05
Epoch 91/100: Training Loss: 4.826795979304248e-05
Epoch 92/100: Training Loss: 2.950643970360679e-06
Epoch 93/100: Training Loss: 7.963290412305138e-05
Epoch 94/100: Training Loss: 8.123092490681855e-05
Epoch 95/100: Training Loss: 8.3808688455463e-05
Epoch 96/100: Training Loss: 0.000125968854548195
Epoch 97/100: Training Loss: 0.00010782329168187858
Epoch 98/100: Training Loss: 8.110525644457286e-06
Epoch 99/100: Training Loss: 9.049149015532111e-05
Epoch 0/100: Training Loss: 0.0004894663208881311
Epoch 1/100: Training Loss: 7.027289541685475e-05
Epoch 2/100: Training Loss: 0.0005113929839788285
Epoch 3/100: Training Loss: 0.00014154964118404727
Epoch 4/100: Training Loss: 0.00023664593432856873
Epoch 5/100: Training Loss: 0.00030061154238945616
Epoch 6/100: Training Loss: 0.0001717473388509413
Epoch 7/100: Training Loss: 0.0005120110195294946
Epoch 8/100: Training Loss: 0.0001463553480869901
Epoch 9/100: Training Loss: 0.0003073289331081694
Epoch 10/100: Training Loss: 0.0006976105206835586
Epoch 11/100: Training Loss: 7.54251367354815e-05
Epoch 12/100: Training Loss: 0.00017971087214166084
Epoch 13/100: Training Loss: 7.085174124325271e-05
Epoch 14/100: Training Loss: 3.087417989046173e-05
Epoch 15/100: Training Loss: 0.0005450897512182725
Epoch 16/100: Training Loss: 2.6464625055679178e-05
Epoch 17/100: Training Loss: 0.0002397316120630872
Epoch 18/100: Training Loss: 0.0002168145821949022
Epoch 19/100: Training Loss: 9.80703996708699e-06
Epoch 20/100: Training Loss: 0.00012706414656301514
Epoch 21/100: Training Loss: 3.1372146944688484e-05
Epoch 22/100: Training Loss: 9.633116918591271e-05
Epoch 23/100: Training Loss: 7.750011622015618e-06
Epoch 24/100: Training Loss: 2.865072966149423e-05
Epoch 25/100: Training Loss: 9.90270183677167e-05
Epoch 26/100: Training Loss: 7.509727822204607e-05
Epoch 27/100: Training Loss: 5.0350929923619315e-06
Epoch 28/100: Training Loss: 0.00010858599788847223
Epoch 29/100: Training Loss: 3.7058404403680984e-06
Epoch 30/100: Training Loss: 9.026005452053737e-06
Epoch 31/100: Training Loss: 3.0320601340020653e-05
Epoch 32/100: Training Loss: 2.125234227726417e-05
Epoch 33/100: Training Loss: 0.00011298132944951015
Epoch 34/100: Training Loss: 9.704737272937741e-05
Epoch 35/100: Training Loss: 5.469817783584637e-05
Epoch 36/100: Training Loss: 0.00032483654475845065
Epoch 37/100: Training Loss: 1.7110686914055748e-05
Epoch 38/100: Training Loss: 1.3549785352254336e-05
Epoch 39/100: Training Loss: 1.2810522303051125e-05
Epoch 40/100: Training Loss: 6.401811652215181e-05
Epoch 41/100: Training Loss: 3.316241061001752e-05
Epoch 42/100: Training Loss: 0.00010090672402782778
Epoch 43/100: Training Loss: 3.072554121964273e-05
Epoch 44/100: Training Loss: 1.1917436495423317e-05
Epoch 45/100: Training Loss: 1.845901123190348e-05
Epoch 46/100: Training Loss: 5.451371003173094e-05
Epoch 47/100: Training Loss: 0.00014830011445864113
Epoch 48/100: Training Loss: 2.1580338840727256e-05
Epoch 49/100: Training Loss: 0.00012775106701703198
Epoch 50/100: Training Loss: 3.177577637927195e-05
Epoch 51/100: Training Loss: 6.982927268321535e-05
Epoch 52/100: Training Loss: 1.4296089983091945e-05
Epoch 53/100: Training Loss: 3.856280818581581e-05
Epoch 54/100: Training Loss: 3.266962352250002e-05
Epoch 55/100: Training Loss: 0.00012095454684664718
Epoch 56/100: Training Loss: 9.062370301875393e-05
Epoch 57/100: Training Loss: 3.9122092117250496e-05
Epoch 58/100: Training Loss: 4.3273235844299857e-05
Epoch 59/100: Training Loss: 3.043840591902886e-06
Epoch 60/100: Training Loss: 2.045191791469017e-05
Epoch 61/100: Training Loss: 9.00311902396183e-06
Epoch 62/100: Training Loss: 1.790287566527856e-05
Epoch 63/100: Training Loss: 2.1203070780081032e-05
Epoch 64/100: Training Loss: 2.702923318519529e-05
Epoch 65/100: Training Loss: 8.972235996506911e-06
Epoch 66/100: Training Loss: 2.0898351099638812e-05
Epoch 67/100: Training Loss: 8.261841680623788e-05
Epoch 68/100: Training Loss: 1.02072119696346e-05
Epoch 69/100: Training Loss: 4.075471295324047e-05
Epoch 70/100: Training Loss: 5.702274661939756e-05
Epoch 71/100: Training Loss: 8.334320005589881e-05
Epoch 72/100: Training Loss: 0.00025415644708987886
Epoch 73/100: Training Loss: 2.3722226228729815e-05
Epoch 74/100: Training Loss: 6.899587084761764e-05
Epoch 75/100: Training Loss: 5.6756219406307274e-05
Epoch 76/100: Training Loss: 2.588995781817795e-05
Epoch 77/100: Training Loss: 3.6472887361207895e-05
Epoch 78/100: Training Loss: 0.00011256577826179235
Epoch 79/100: Training Loss: 5.874307661325531e-05
Epoch 80/100: Training Loss: 0.0001802599403710492
Epoch 81/100: Training Loss: 6.98547563062305e-06
Epoch 82/100: Training Loss: 2.8648230337859254e-05
Epoch 83/100: Training Loss: 2.5796627757691703e-05
Epoch 84/100: Training Loss: 7.108990741745297e-06
Epoch 85/100: Training Loss: 0.0003360548773698047
Epoch 86/100: Training Loss: 1.892676593455593e-05
Epoch 87/100: Training Loss: 1.3698032537920285e-05
Epoch 88/100: Training Loss: 3.9039509354439455e-05
Epoch 89/100: Training Loss: 2.1855027489034476e-05
Epoch 90/100: Training Loss: 0.000130256341226333
Epoch 91/100: Training Loss: 2.243849490068655e-05
Epoch 92/100: Training Loss: 0.00017212591735662612
Epoch 93/100: Training Loss: 6.216004556786697e-05
Epoch 94/100: Training Loss: 6.432376340427229e-05
Epoch 95/100: Training Loss: 4.994181454577277e-05
Epoch 96/100: Training Loss: 6.567021271428176e-06
Epoch 97/100: Training Loss: 1.8930773506254223e-05
Epoch 98/100: Training Loss: 3.648445059635998e-05
Epoch 99/100: Training Loss: 1.2730890102025154e-05
Epoch 0/100: Training Loss: 0.0006756174457447411
Epoch 1/100: Training Loss: 0.00046302986251933693
Epoch 2/100: Training Loss: 8.872765295975946e-05
Epoch 3/100: Training Loss: 8.757667190023601e-05
Epoch 4/100: Training Loss: 2.8594950372729066e-05
Epoch 5/100: Training Loss: 1.8807311830499246e-05
Epoch 6/100: Training Loss: 2.2739650172102077e-05
Epoch 7/100: Training Loss: 0.00011233716468105402
Epoch 8/100: Training Loss: 0.00012839547120402212
Epoch 9/100: Training Loss: 1.8801715542382722e-05
Epoch 10/100: Training Loss: 2.4162143084634046e-05
Epoch 11/100: Training Loss: 4.203169034468219e-05
Epoch 12/100: Training Loss: 2.7219330549507397e-05
Epoch 13/100: Training Loss: 5.999040234449733e-06
Epoch 14/100: Training Loss: 6.761011926482344e-06
Epoch 15/100: Training Loss: 0.00038369975549757746
Epoch 16/100: Training Loss: 4.107662426483204e-06
Epoch 17/100: Training Loss: 0.0002524219119228055
Epoch 18/100: Training Loss: 4.0166982086250066e-05
Epoch 19/100: Training Loss: 1.9765358769038334e-05
Epoch 20/100: Training Loss: 9.29505869144816e-06
Epoch 21/100: Training Loss: 2.7230790411143026e-05
Epoch 22/100: Training Loss: 2.458218627112329e-06
Epoch 23/100: Training Loss: 1.2486686537484952e-05
Epoch 24/100: Training Loss: 9.92431535047266e-06
Epoch 25/100: Training Loss: 0.00012781642243733855
Epoch 26/100: Training Loss: 9.738132274070663e-06
Epoch 27/100: Training Loss: 1.0992425406192985e-05
Epoch 28/100: Training Loss: 2.8926634851993465e-05
Epoch 29/100: Training Loss: 0.00024174735150529666
Epoch 30/100: Training Loss: 1.9673224655502046e-05
Epoch 31/100: Training Loss: 6.156844909682936e-05
Epoch 32/100: Training Loss: 1.1810334399342537e-05
Epoch 33/100: Training Loss: 1.6228279157337052e-05
Epoch 34/100: Training Loss: 1.4102533292610015e-05
Epoch 35/100: Training Loss: 3.351670723291523e-06
Epoch 36/100: Training Loss: 7.76601652510364e-07
Epoch 37/100: Training Loss: 1.7584118033621478e-06
Epoch 38/100: Training Loss: 2.544644474565582e-06
Epoch 39/100: Training Loss: 2.6550657124229345e-06
Epoch 40/100: Training Loss: 4.7997859695032574e-06
Epoch 41/100: Training Loss: 4.6048085769062085e-06
Epoch 42/100: Training Loss: 5.2092520670439095e-06
Epoch 43/100: Training Loss: 2.6284486280896202e-06
Epoch 44/100: Training Loss: 5.769958372975412e-06
Epoch 45/100: Training Loss: 9.661805168662905e-05
Epoch 46/100: Training Loss: 1.8976731100558166e-06
Epoch 47/100: Training Loss: 2.9909346177505806e-06
Epoch 48/100: Training Loss: 5.167359778565676e-06
Epoch 49/100: Training Loss: 5.244830259927987e-06
Epoch 50/100: Training Loss: 1.8980170050928277e-06
Epoch 51/100: Training Loss: 2.5801216523609888e-06
Epoch 52/100: Training Loss: 5.547339519519242e-07
Epoch 53/100: Training Loss: 1.1347462903424229e-05
Epoch 54/100: Training Loss: 1.6819095790319378e-05
Epoch 55/100: Training Loss: 5.55673822961046e-06
Epoch 56/100: Training Loss: 3.1490793510135515e-06
Epoch 57/100: Training Loss: 3.2595992547714655e-06
Epoch 58/100: Training Loss: 1.0128745876018777e-05
Epoch 59/100: Training Loss: 3.3097398037199483e-06
Epoch 60/100: Training Loss: 3.239783331033494e-06
Epoch 61/100: Training Loss: 1.4848462379048892e-06
Epoch 62/100: Training Loss: 1.3916934903026161e-06
Epoch 63/100: Training Loss: 9.090710648506746e-06
Epoch 64/100: Training Loss: 2.530761138272927e-05
Epoch 65/100: Training Loss: 5.23768966775304e-05
Epoch 66/100: Training Loss: 1.2505938134035054e-06
Epoch 67/100: Training Loss: 0.00011346500049524778
Epoch 68/100: Training Loss: 1.0482190101670578e-05
Epoch 69/100: Training Loss: 1.8988751113047247e-06
Epoch 70/100: Training Loss: 7.371110173418383e-07
Epoch 71/100: Training Loss: 3.799401636594581e-07
Epoch 72/100: Training Loss: 1.7755941567199113e-06
Epoch 73/100: Training Loss: 2.157321302094935e-06
Epoch 74/100: Training Loss: 5.882851870296181e-07
Epoch 75/100: Training Loss: 4.300352492925164e-06
Epoch 76/100: Training Loss: 8.253028278026078e-06
Epoch 77/100: Training Loss: 1.0651713780089879e-05
Epoch 78/100: Training Loss: 3.503265593831549e-07
Epoch 79/100: Training Loss: 3.507178362392123e-06
Epoch 80/100: Training Loss: 2.4641015161781033e-06
Epoch 81/100: Training Loss: 7.817831262110862e-06
Epoch 82/100: Training Loss: 4.4434120452591125e-06
Epoch 83/100: Training Loss: 8.606411424787055e-06
Epoch 84/100: Training Loss: 2.7831622938136884e-05
Epoch 85/100: Training Loss: 7.3012620244058255e-06
Epoch 86/100: Training Loss: 7.033827317870251e-06
Epoch 87/100: Training Loss: 3.649543338108624e-06
Epoch 88/100: Training Loss: 0.00017587090487437398
Epoch 89/100: Training Loss: 1.2743593749043118e-05
Epoch 90/100: Training Loss: 4.752178889419466e-06
Epoch 91/100: Training Loss: 4.629424413991883e-06
Epoch 92/100: Training Loss: 1.7873621274270283e-05
Epoch 93/100: Training Loss: 4.297041703817422e-06
Epoch 94/100: Training Loss: 9.299722482396014e-07
Epoch 95/100: Training Loss: 4.770508299126486e-06
Epoch 96/100: Training Loss: 9.213028174946127e-06
Epoch 97/100: Training Loss: 5.108496955190806e-06
Epoch 98/100: Training Loss: 9.489791142031751e-05
Epoch 99/100: Training Loss: 3.2328743688060562e-06
dataset: cities layer_num_from_end:-13 Avg_acc:tensor(0.7991) Avg_AUC:0.8941897651169033 Avg_threshold:0.060935795307159424
dataset: inventions layer_num_from_end:-13 Avg_acc:tensor(0.8642) Avg_AUC:0.9300719785738198 Avg_threshold:0.6506891846656799
dataset: elements layer_num_from_end:-13 Avg_acc:tensor(0.6452) Avg_AUC:0.7531321540062434 Avg_threshold:0.922183096408844
dataset: animals layer_num_from_end:-13 Avg_acc:tensor(0.6786) Avg_AUC:0.7449885046611238 Avg_threshold:0.6057801842689514
dataset: companies layer_num_from_end:-13 Avg_acc:tensor(0.7592) Avg_AUC:0.9055277777777778 Avg_threshold:0.7321529388427734
dataset: facts layer_num_from_end:-13 Avg_acc:tensor(0.7369) Avg_AUC:0.8650572429407493 Avg_threshold:0.8247568011283875
dataset: conj_neg_facts layer_num_from_end:-13 Avg_acc:tensor(0.7380) Avg_AUC:0.576177330835716 Avg_threshold:0.8539838194847107


================layer -17================
Epoch 0/100: Training Loss: 0.003843647547257252
Epoch 1/100: Training Loss: 0.0031141589085261026
Epoch 2/100: Training Loss: 0.0031333890480872914
Epoch 3/100: Training Loss: 0.0031308443882526495
Epoch 4/100: Training Loss: 0.0027661629212208283
Epoch 5/100: Training Loss: 0.0029767679098324897
Epoch 6/100: Training Loss: 0.0020688615548305023
Epoch 7/100: Training Loss: 0.0019904161110902443
Epoch 8/100: Training Loss: 0.001793484275157635
Epoch 9/100: Training Loss: 0.0015536869565645854
Epoch 10/100: Training Loss: 0.0013190936774779588
Epoch 11/100: Training Loss: 0.001245360057323407
Epoch 12/100: Training Loss: 0.0010958342598034786
Epoch 13/100: Training Loss: 0.0008826135442807124
Epoch 14/100: Training Loss: 0.0009608622162770003
Epoch 15/100: Training Loss: 0.0008665030010235616
Epoch 16/100: Training Loss: 0.000711857985991698
Epoch 17/100: Training Loss: 0.001190846929183373
Epoch 18/100: Training Loss: 0.004440776812724578
Epoch 19/100: Training Loss: 0.001766737454976791
Epoch 20/100: Training Loss: 0.0006066196335431857
Epoch 21/100: Training Loss: 0.0006157712867626777
Epoch 22/100: Training Loss: 0.0007256063131185678
Epoch 23/100: Training Loss: 0.0006762734399392054
Epoch 24/100: Training Loss: 0.0008220798694170439
Epoch 25/100: Training Loss: 0.0035856694747240115
Epoch 26/100: Training Loss: 0.0006235361290283692
Epoch 27/100: Training Loss: 0.0005081755419572195
Epoch 28/100: Training Loss: 0.00046614079903333617
Epoch 29/100: Training Loss: 0.000522773903913987
Epoch 30/100: Training Loss: 0.0006733858623565772
Epoch 31/100: Training Loss: 0.0003583344320456187
Epoch 32/100: Training Loss: 0.0005488736698260674
Epoch 33/100: Training Loss: 0.00041226861186516593
Epoch 34/100: Training Loss: 0.0004049840454871838
Epoch 35/100: Training Loss: 0.00048162339207453607
Epoch 36/100: Training Loss: 0.0005115375686914492
Epoch 37/100: Training Loss: 0.0005420376666081258
Epoch 38/100: Training Loss: 0.0005279243565522707
Epoch 39/100: Training Loss: 0.000572362007238926
Epoch 40/100: Training Loss: 0.00043868961242529063
Epoch 41/100: Training Loss: 0.0003553793932764958
Epoch 42/100: Training Loss: 0.00043661386156693485
Epoch 43/100: Training Loss: 0.0003923133779794742
Epoch 44/100: Training Loss: 0.00047284307388158946
Epoch 45/100: Training Loss: 0.0005254652828742296
Epoch 46/100: Training Loss: 0.00044310837984085083
Epoch 47/100: Training Loss: 0.0005614846371687376
Epoch 48/100: Training Loss: 0.0005236822061049632
Epoch 49/100: Training Loss: 0.0005368935183072701
Epoch 50/100: Training Loss: 0.0004393652272530091
Epoch 51/100: Training Loss: 0.0003089098594127557
Epoch 52/100: Training Loss: 0.000549473728124912
Epoch 53/100: Training Loss: 0.000441857255422152
Epoch 54/100: Training Loss: 0.0023642835708764885
Epoch 55/100: Training Loss: 0.0003068408666130824
Epoch 56/100: Training Loss: 0.0004676019725127098
Epoch 57/100: Training Loss: 0.00054440465875161
Epoch 58/100: Training Loss: 0.0003430217695541871
Epoch 59/100: Training Loss: 0.00030757864125264
Epoch 60/100: Training Loss: 0.0006180875576459444
Epoch 61/100: Training Loss: 0.00019784777974471069
Epoch 62/100: Training Loss: 0.0004013986446154423
Epoch 63/100: Training Loss: 0.0003010034083555906
Epoch 64/100: Training Loss: 0.00039881131110283046
Epoch 65/100: Training Loss: 0.00033745991113858344
Epoch 66/100: Training Loss: 0.0004500599625783089
Epoch 67/100: Training Loss: 0.00031850377145486
Epoch 68/100: Training Loss: 0.00038842584651250107
Epoch 69/100: Training Loss: 0.00031478896450537903
Epoch 70/100: Training Loss: 0.0005273582079471686
Epoch 71/100: Training Loss: 0.0004424048253358939
Epoch 72/100: Training Loss: 0.0004777603615552951
Epoch 73/100: Training Loss: 0.000267511520248193
Epoch 74/100: Training Loss: 0.0004080457087510671
Epoch 75/100: Training Loss: 0.00022572255096374414
Epoch 76/100: Training Loss: 0.0003942418365906446
Epoch 77/100: Training Loss: 0.00046058624791793333
Epoch 78/100: Training Loss: 0.0003207208684239632
Epoch 79/100: Training Loss: 0.00032056008393947896
Epoch 80/100: Training Loss: 0.00027484050354896445
Epoch 81/100: Training Loss: 0.00026016968947190506
Epoch 82/100: Training Loss: 0.0003534947545864643
Epoch 83/100: Training Loss: 0.0003765889992698645
Epoch 84/100: Training Loss: 0.0006092294381979185
Epoch 85/100: Training Loss: 0.00032631833201799635
Epoch 86/100: Training Loss: 0.0002701159757681382
Epoch 87/100: Training Loss: 0.00027520515215702547
Epoch 88/100: Training Loss: 0.00033815794934829074
Epoch 89/100: Training Loss: 0.0002999234323700269
Epoch 90/100: Training Loss: 0.00023524336612377412
Epoch 91/100: Training Loss: 0.00022869461621993626
Epoch 92/100: Training Loss: 0.0003525438742377819
Epoch 93/100: Training Loss: 0.00021223294047208934
Epoch 94/100: Training Loss: 0.0002907559227867004
Epoch 95/100: Training Loss: 0.0001962816653152307
Epoch 96/100: Training Loss: 0.0002905985770317224
Epoch 97/100: Training Loss: 0.00019852683330193543
Epoch 98/100: Training Loss: 0.0002937972163542723
Epoch 99/100: Training Loss: 0.00021784981855979332
Epoch 0/100: Training Loss: 0.000191017300425089
Epoch 1/100: Training Loss: 0.0002920525357327654
Epoch 2/100: Training Loss: 3.4354121795951515e-05
Epoch 3/100: Training Loss: 0.00015878789293926392
Epoch 4/100: Training Loss: 1.1355296662703758e-05
Epoch 5/100: Training Loss: 2.9651366692086507e-05
Epoch 6/100: Training Loss: 2.427003659608653e-06
Epoch 7/100: Training Loss: 1.1094254880074428e-05
Epoch 8/100: Training Loss: 7.608568195604423e-06
Epoch 9/100: Training Loss: 7.816231726029796e-06
Epoch 10/100: Training Loss: 5.347498654985107e-06
Epoch 11/100: Training Loss: 2.9991246705599164e-06
Epoch 12/100: Training Loss: 2.5160817404365326e-06
Epoch 13/100: Training Loss: 3.3954334875460162e-06
Epoch 14/100: Training Loss: 9.707797469045015e-06
Epoch 15/100: Training Loss: 6.8464482437000685e-06
Epoch 16/100: Training Loss: 7.343030066935203e-06
Epoch 17/100: Training Loss: 7.864311775986121e-06
Epoch 18/100: Training Loss: 5.617202677233604e-06
Epoch 19/100: Training Loss: 1.0756660887612356e-05
Epoch 20/100: Training Loss: 2.6065612424816517e-06
Epoch 21/100: Training Loss: 2.3324404855680573e-06
Epoch 22/100: Training Loss: 1.3788349481692094e-06
Epoch 23/100: Training Loss: 3.2352246009982756e-06
Epoch 24/100: Training Loss: 2.5652805259743613e-06
Epoch 25/100: Training Loss: 6.664326652516966e-06
Epoch 26/100: Training Loss: 1.8758799529930936e-06
Epoch 27/100: Training Loss: 2.9657168664672986e-06
Epoch 28/100: Training Loss: 1.1363263020600973e-05
Epoch 29/100: Training Loss: 3.2463020670140003e-06
Epoch 30/100: Training Loss: 6.485622346969196e-06
Epoch 31/100: Training Loss: 2.0926341888465557e-06
Epoch 32/100: Training Loss: 3.752480519438505e-07
Epoch 33/100: Training Loss: 7.830127826244507e-07
Epoch 34/100: Training Loss: 2.3046417596161096e-06
Epoch 35/100: Training Loss: 5.0929550398892885e-05
Epoch 36/100: Training Loss: 1.249065614334671e-05
Epoch 37/100: Training Loss: 1.8177017874414344e-06
Epoch 38/100: Training Loss: 2.3708270023255335e-06
Epoch 39/100: Training Loss: 5.83630048924631e-06
Epoch 40/100: Training Loss: 3.997204123657915e-06
Epoch 41/100: Training Loss: 2.1388027827451716e-06
Epoch 42/100: Training Loss: 2.488032173398765e-06
Epoch 43/100: Training Loss: 6.996341584241978e-06
Epoch 44/100: Training Loss: 2.277969077830892e-06
Epoch 45/100: Training Loss: 6.9582622890849284e-06
Epoch 46/100: Training Loss: 0.00012875466937441461
Epoch 47/100: Training Loss: 1.097789828795021e-06
Epoch 48/100: Training Loss: 0.0001261216074629215
Epoch 49/100: Training Loss: 9.561604752326426e-07
Epoch 50/100: Training Loss: 1.926884481123745e-06
Epoch 51/100: Training Loss: 2.796445856280124e-06
Epoch 52/100: Training Loss: 3.5064302764899795e-06
Epoch 53/100: Training Loss: 2.015960056150975e-05
Epoch 54/100: Training Loss: 2.6735248495177304e-06
Epoch 55/100: Training Loss: 1.2633996293149187e-06
Epoch 56/100: Training Loss: 2.5185483879505785e-06
Epoch 57/100: Training Loss: 4.906048100324757e-06
Epoch 58/100: Training Loss: 2.26380838072888e-05
Epoch 59/100: Training Loss: 1.8285746130708086e-06
Epoch 60/100: Training Loss: 1.9760833971720237e-06
Epoch 61/100: Training Loss: 3.824475630616661e-06
Epoch 62/100: Training Loss: 4.5174844912775363e-07
Epoch 63/100: Training Loss: 6.831763990874916e-07
Epoch 64/100: Training Loss: 4.71211146331505e-07
Epoch 65/100: Training Loss: 2.2626557646344328e-06
Epoch 66/100: Training Loss: 1.1850654297913405e-06
Epoch 67/100: Training Loss: 3.2882731838043227e-06
Epoch 68/100: Training Loss: 5.641771008515305e-07
Epoch 69/100: Training Loss: 4.173337634352157e-06
Epoch 70/100: Training Loss: 4.778758543355583e-05
Epoch 71/100: Training Loss: 3.1136160073258954e-06
Epoch 72/100: Training Loss: 1.6983049282712253e-05
Epoch 73/100: Training Loss: 6.015261247829271e-07
Epoch 74/100: Training Loss: 1.6439302548799547e-06
Epoch 75/100: Training Loss: 1.0220740604393952e-06
Epoch 76/100: Training Loss: 4.233921107506138e-06
Epoch 77/100: Training Loss: 6.692889125625114e-07
Epoch 78/100: Training Loss: 3.4352825044115564e-06
Epoch 79/100: Training Loss: 3.413529870843707e-07
Epoch 80/100: Training Loss: 2.65571617650812e-06
Epoch 81/100: Training Loss: 1.5424514625789472e-06
Epoch 82/100: Training Loss: 5.5080411262442715e-06
Epoch 83/100: Training Loss: 1.947847174087982e-05
Epoch 84/100: Training Loss: 2.151618674063362e-05
Epoch 85/100: Training Loss: 8.657993853192425e-06
Epoch 86/100: Training Loss: 2.9864002670675114e-05
Epoch 87/100: Training Loss: 7.744774644183738e-06
Epoch 88/100: Training Loss: 4.190587135336692e-07
Epoch 89/100: Training Loss: 3.083660465669338e-06
Epoch 90/100: Training Loss: 3.838115016775281e-06
Epoch 91/100: Training Loss: 1.5087606335234215e-05
Epoch 92/100: Training Loss: 7.760313218417723e-07
Epoch 93/100: Training Loss: 8.921680691121605e-07
Epoch 94/100: Training Loss: 1.0127973776193744e-06
Epoch 95/100: Training Loss: 2.9437232453526404e-06
Epoch 96/100: Training Loss: 1.5681630393047504e-05
Epoch 97/100: Training Loss: 3.759784993586118e-06
Epoch 98/100: Training Loss: 9.247351379338401e-06
Epoch 99/100: Training Loss: 1.918954274621312e-06
Epoch 0/100: Training Loss: 0.0010320689883167387
Epoch 1/100: Training Loss: 0.0003639977442193355
Epoch 2/100: Training Loss: 0.00034439041199187885
Epoch 3/100: Training Loss: 0.0009000593990222361
Epoch 4/100: Training Loss: 0.0005532275227939381
Epoch 5/100: Training Loss: 0.0006421463252192709
Epoch 6/100: Training Loss: 0.0007030421117851637
Epoch 7/100: Training Loss: 0.00025347618563142834
Epoch 8/100: Training Loss: 0.0006805880846480978
Epoch 9/100: Training Loss: 0.0005486624745222238
Epoch 10/100: Training Loss: 0.00039208134240154767
Epoch 11/100: Training Loss: 0.00029799908534434044
Epoch 12/100: Training Loss: 0.00018977643542699685
Epoch 13/100: Training Loss: 0.00023932124559695905
Epoch 14/100: Training Loss: 0.0002938303675047413
Epoch 15/100: Training Loss: 0.00020386131977603446
Epoch 16/100: Training Loss: 0.0002523544012691101
Epoch 17/100: Training Loss: 0.00017207901402296525
Epoch 18/100: Training Loss: 0.0006133957820780137
Epoch 19/100: Training Loss: 0.0002783609731164993
Epoch 20/100: Training Loss: 0.00011671921111879305
Epoch 21/100: Training Loss: 0.00019869805056584906
Epoch 22/100: Training Loss: 0.0002348018447737888
Epoch 23/100: Training Loss: 6.230505882884583e-05
Epoch 24/100: Training Loss: 0.00015407554569287537
Epoch 25/100: Training Loss: 0.00025609214853377363
Epoch 26/100: Training Loss: 9.55506956954887e-05
Epoch 27/100: Training Loss: 0.00014213437206065494
Epoch 28/100: Training Loss: 0.00012894428221348723
Epoch 29/100: Training Loss: 0.00018083064804249758
Epoch 30/100: Training Loss: 0.0004992920618790847
Epoch 31/100: Training Loss: 0.00025631527825178605
Epoch 32/100: Training Loss: 0.00020456250032148748
Epoch 33/100: Training Loss: 5.3277888179364784e-05
Epoch 34/100: Training Loss: 0.00015005133877512555
Epoch 35/100: Training Loss: 0.00020242799326305476
Epoch 36/100: Training Loss: 0.0001305977969822301
Epoch 37/100: Training Loss: 0.0002429513471428625
Epoch 38/100: Training Loss: 0.00012700595130208392
Epoch 39/100: Training Loss: 2.4690943858607322e-05
Epoch 40/100: Training Loss: 0.00011629104715396915
Epoch 41/100: Training Loss: 2.4104844863058753e-05
Epoch 42/100: Training Loss: 0.00011969516180219694
Epoch 43/100: Training Loss: 0.00020854813952791206
Epoch 44/100: Training Loss: 5.971289732876946e-05
Epoch 45/100: Training Loss: 0.00017286065072495474
Epoch 46/100: Training Loss: 0.00019378302728428561
Epoch 47/100: Training Loss: 0.0001272176691579603
Epoch 48/100: Training Loss: 6.838870230573335e-05
Epoch 49/100: Training Loss: 5.958405032551666e-05
Epoch 50/100: Training Loss: 7.515999897303085e-05
Epoch 51/100: Training Loss: 0.0004912053163235004
Epoch 52/100: Training Loss: 0.00010796290784399973
Epoch 53/100: Training Loss: 0.000122860259464963
Epoch 54/100: Training Loss: 0.00011593566963036135
Epoch 55/100: Training Loss: 3.012463585283961e-05
Epoch 56/100: Training Loss: 0.00011170884737601646
Epoch 57/100: Training Loss: 0.00011689018677262699
Epoch 58/100: Training Loss: 2.6323562867231497e-05
Epoch 59/100: Training Loss: 2.5773800201545473e-05
Epoch 60/100: Training Loss: 0.00021698676487978767
Epoch 61/100: Training Loss: 9.420046338398532e-05
Epoch 62/100: Training Loss: 3.9293207396748915e-05
Epoch 63/100: Training Loss: 3.89597602020022e-05
Epoch 64/100: Training Loss: 8.636138220718004e-05
Epoch 65/100: Training Loss: 1.579172892403279e-05
Epoch 66/100: Training Loss: 4.25805780210646e-05
Epoch 67/100: Training Loss: 1.777591799044501e-05
Epoch 68/100: Training Loss: 0.00012130811136232782
Epoch 69/100: Training Loss: 7.399770313224911e-06
Epoch 70/100: Training Loss: 2.859473203169814e-05
Epoch 71/100: Training Loss: 1.4463791463574673e-05
Epoch 72/100: Training Loss: 3.3876679551142915e-05
Epoch 73/100: Training Loss: 1.0942539823877866e-05
Epoch 74/100: Training Loss: 3.707567356290861e-05
Epoch 75/100: Training Loss: 5.003745031302871e-05
Epoch 76/100: Training Loss: 7.408064246447377e-05
Epoch 77/100: Training Loss: 2.1318462571946745e-05
Epoch 78/100: Training Loss: 1.719201244919548e-05
Epoch 79/100: Training Loss: 6.749328411272748e-05
Epoch 80/100: Training Loss: 0.00036974662569313566
Epoch 81/100: Training Loss: 4.2789164778873394e-05
Epoch 82/100: Training Loss: 3.2461682393540085e-05
Epoch 83/100: Training Loss: 0.0003826027542217824
Epoch 84/100: Training Loss: 1.6563841693811288e-05
Epoch 85/100: Training Loss: 2.291772042599199e-05
Epoch 86/100: Training Loss: 5.752501832279145e-05
Epoch 87/100: Training Loss: 3.039823187624707e-05
Epoch 88/100: Training Loss: 1.972502653752517e-05
Epoch 89/100: Training Loss: 1.4641395942792634e-05
Epoch 90/100: Training Loss: 2.122052410002208e-05
Epoch 91/100: Training Loss: 5.733739264410545e-05
Epoch 92/100: Training Loss: 1.6217810055451696e-05
Epoch 93/100: Training Loss: 1.521040137638064e-05
Epoch 94/100: Training Loss: 1.77020569462582e-05
Epoch 95/100: Training Loss: 1.4062668512916674e-05
Epoch 96/100: Training Loss: 4.527134593255919e-05
Epoch 97/100: Training Loss: 0.0004480837157409116
Epoch 98/100: Training Loss: 5.4423377018978154e-05
Epoch 99/100: Training Loss: 5.550985962017629e-05
Epoch 0/100: Training Loss: 0.0002933445640585639
Epoch 1/100: Training Loss: 0.00019698888063430787
Epoch 2/100: Training Loss: 3.5682189362970266e-05
Epoch 3/100: Training Loss: 7.440504533323375e-05
Epoch 4/100: Training Loss: 9.756369346922094e-05
Epoch 5/100: Training Loss: 0.00018624158745462243
Epoch 6/100: Training Loss: 7.610741833394224e-05
Epoch 7/100: Training Loss: 6.062111986631697e-05
Epoch 8/100: Training Loss: 0.0001544427973302928
Epoch 9/100: Training Loss: 0.000163645094091242
Epoch 10/100: Training Loss: 7.5889683582566e-05
Epoch 11/100: Training Loss: 0.00010047944431955164
Epoch 12/100: Training Loss: 5.885686048052528e-05
Epoch 13/100: Training Loss: 8.813476359302347e-05
Epoch 14/100: Training Loss: 0.00014438043263825502
Epoch 15/100: Training Loss: 1.655610024251721e-05
Epoch 16/100: Training Loss: 2.72695737128908e-05
Epoch 17/100: Training Loss: 4.3182735416022216e-05
Epoch 18/100: Training Loss: 5.108600228347561e-05
Epoch 19/100: Training Loss: 2.4516647681593894e-05
Epoch 20/100: Training Loss: 4.594798453829505e-05
Epoch 21/100: Training Loss: 2.4002157575027508e-05
Epoch 22/100: Training Loss: 2.483496315438639e-05
Epoch 23/100: Training Loss: 1.873768950727853e-05
Epoch 24/100: Training Loss: 3.194673071530732e-05
Epoch 25/100: Training Loss: 5.113233134827831e-05
Epoch 26/100: Training Loss: 2.362362151457505e-05
Epoch 27/100: Training Loss: 3.297893669117581e-05
Epoch 28/100: Training Loss: 5.241291682151231e-05
Epoch 29/100: Training Loss: 2.057735003869642e-05
Epoch 30/100: Training Loss: 1.5756731260229242e-05
Epoch 31/100: Training Loss: 2.7711850337006828e-05
Epoch 32/100: Training Loss: 7.190018236129121e-06
Epoch 33/100: Training Loss: 3.057724838568406e-05
Epoch 34/100: Training Loss: 1.736460676924749e-05
Epoch 35/100: Training Loss: 3.260474609719081e-05
Epoch 36/100: Training Loss: 9.338896382938732e-06
Epoch 37/100: Training Loss: 9.787721898068082e-06
Epoch 38/100: Training Loss: 1.1438031850213354e-05
Epoch 39/100: Training Loss: 3.2846803184260024e-05
Epoch 40/100: Training Loss: 1.6975231119431554e-06
Epoch 41/100: Training Loss: 1.1164192321964286e-05
Epoch 42/100: Training Loss: 1.323011555624279e-05
Epoch 43/100: Training Loss: 6.121388551863757e-06
Epoch 44/100: Training Loss: 3.49952754649249e-05
Epoch 45/100: Training Loss: 2.7278762056746267e-05
Epoch 46/100: Training Loss: 2.1058113568208434e-05
Epoch 47/100: Training Loss: 2.2886527321216736e-06
Epoch 48/100: Training Loss: 5.068203265016729e-05
Epoch 49/100: Training Loss: 3.607054399749772e-06
Epoch 50/100: Training Loss: 3.7756219336932356e-06
Epoch 51/100: Training Loss: 9.004588619890538e-06
Epoch 52/100: Training Loss: 6.710408805784854e-06
Epoch 53/100: Training Loss: 1.55553584177555e-06
Epoch 54/100: Training Loss: 1.448434010275047e-06
Epoch 55/100: Training Loss: 3.5776582080870865e-06
Epoch 56/100: Training Loss: 6.7158173558048224e-06
Epoch 57/100: Training Loss: 9.781713809140703e-06
Epoch 58/100: Training Loss: 6.226784105158665e-06
Epoch 59/100: Training Loss: 3.0684163158928804e-06
Epoch 60/100: Training Loss: 7.229360794140534e-06
Epoch 61/100: Training Loss: 7.73653726686131e-05
Epoch 62/100: Training Loss: 2.7428730390965937e-06
Epoch 63/100: Training Loss: 3.89830966014415e-06
Epoch 64/100: Training Loss: 3.650500333275307e-06
Epoch 65/100: Training Loss: 7.790430258451537e-06
Epoch 66/100: Training Loss: 1.213795357299122e-06
Epoch 67/100: Training Loss: 3.1543403482911263e-06
Epoch 68/100: Training Loss: 8.594708792357282e-06
Epoch 69/100: Training Loss: 4.6280498447066e-06
Epoch 70/100: Training Loss: 1.9361012329516765e-06
Epoch 71/100: Training Loss: 3.3890669742091135e-06
Epoch 72/100: Training Loss: 6.009977369103581e-07
Epoch 73/100: Training Loss: 8.280284766277128e-06
Epoch 74/100: Training Loss: 1.509669792457399e-06
Epoch 75/100: Training Loss: 4.653596657920968e-06
Epoch 76/100: Training Loss: 1.0427710896527224e-05
Epoch 77/100: Training Loss: 8.701385972513394e-06
Epoch 78/100: Training Loss: 5.066501630194993e-07
Epoch 79/100: Training Loss: 2.2731634420448574e-07
Epoch 80/100: Training Loss: 7.678028619424864e-06
Epoch 81/100: Training Loss: 4.058796383271163e-06
Epoch 82/100: Training Loss: 7.647264944601127e-07
Epoch 83/100: Training Loss: 2.9952464286576616e-05
Epoch 84/100: Training Loss: 1.525764607570388e-06
Epoch 85/100: Training Loss: 3.258985816501081e-06
Epoch 86/100: Training Loss: 0.0001010913486507806
Epoch 87/100: Training Loss: 1.4538894571490925e-07
Epoch 88/100: Training Loss: 2.4153231854804537e-06
Epoch 89/100: Training Loss: 1.1498547709462318e-06
Epoch 90/100: Training Loss: 1.6163366539827126e-06
Epoch 91/100: Training Loss: 1.239328015468676e-06
Epoch 92/100: Training Loss: 3.3237394580448214e-06
Epoch 93/100: Training Loss: 4.041746301068501e-06
Epoch 94/100: Training Loss: 2.2188834422691302e-06
Epoch 95/100: Training Loss: 7.277375764467499e-06
Epoch 96/100: Training Loss: 1.007227414943786e-06
Epoch 97/100: Training Loss: 8.328297619961879e-06
Epoch 98/100: Training Loss: 3.3184288027272984e-06
Epoch 99/100: Training Loss: 6.46038838154213e-06
Epoch 0/100: Training Loss: 0.00016701271251050008
Epoch 1/100: Training Loss: 9.110382831041713e-05
Epoch 2/100: Training Loss: 1.8117584562796053e-05
Epoch 3/100: Training Loss: 4.776395548323882e-05
Epoch 4/100: Training Loss: 3.7317554796895675e-05
Epoch 5/100: Training Loss: 1.893474501536189e-05
Epoch 6/100: Training Loss: 0.00016467448341132309
Epoch 7/100: Training Loss: 4.177632385409922e-05
Epoch 8/100: Training Loss: 5.6656592521249975e-05
Epoch 9/100: Training Loss: 0.00023365997567704197
Epoch 10/100: Training Loss: 4.4128887595668915e-05
Epoch 11/100: Training Loss: 3.671254705174178e-05
Epoch 12/100: Training Loss: 8.15620379788535e-05
Epoch 13/100: Training Loss: 5.609861442020961e-05
Epoch 14/100: Training Loss: 3.68974081450893e-05
Epoch 15/100: Training Loss: 7.593183679514766e-05
Epoch 16/100: Training Loss: 0.000127092836433292
Epoch 17/100: Training Loss: 0.00012892470931127873
Epoch 18/100: Training Loss: 4.8807633328273e-05
Epoch 19/100: Training Loss: 5.798471430616994e-05
Epoch 20/100: Training Loss: 4.622054153255054e-06
Epoch 21/100: Training Loss: 1.4801979631460207e-05
Epoch 22/100: Training Loss: 4.058947554931113e-05
Epoch 23/100: Training Loss: 3.339598290107217e-05
Epoch 24/100: Training Loss: 2.447422593832016e-05
Epoch 25/100: Training Loss: 5.8139927582257356e-05
Epoch 26/100: Training Loss: 5.001221968006978e-05
Epoch 27/100: Training Loss: 4.0914957767807394e-05
Epoch 28/100: Training Loss: 2.629608173386842e-05
Epoch 29/100: Training Loss: 8.020409241250033e-05
Epoch 30/100: Training Loss: 6.767671366441085e-05
Epoch 31/100: Training Loss: 1.698571111371715e-05
Epoch 32/100: Training Loss: 2.1197498359713137e-05
Epoch 33/100: Training Loss: 2.7328017189206066e-05
Epoch 34/100: Training Loss: 1.6451278950349526e-05
Epoch 35/100: Training Loss: 9.904098418039111e-06
Epoch 36/100: Training Loss: 2.363701415364094e-05
Epoch 37/100: Training Loss: 1.628446598721814e-05
Epoch 38/100: Training Loss: 1.5534872558259743e-05
Epoch 39/100: Training Loss: 1.652837057130128e-05
Epoch 40/100: Training Loss: 1.59819063211229e-05
Epoch 41/100: Training Loss: 3.6900798674277995e-05
Epoch 42/100: Training Loss: 6.828742093204903e-05
Epoch 43/100: Training Loss: 4.128521213883079e-05
Epoch 44/100: Training Loss: 2.068785246684804e-05
Epoch 45/100: Training Loss: 4.4043502053632166e-05
Epoch 46/100: Training Loss: 8.701526534138462e-06
Epoch 47/100: Training Loss: 3.664633087504844e-06
Epoch 48/100: Training Loss: 1.34619105229592e-05
Epoch 49/100: Training Loss: 4.041855854372824e-05
Epoch 50/100: Training Loss: 1.2534647999751953e-05
Epoch 51/100: Training Loss: 4.406819283138222e-05
Epoch 52/100: Training Loss: 1.3997872716270834e-05
Epoch 53/100: Training Loss: 1.7446276950671376e-05
Epoch 54/100: Training Loss: 9.013501892922111e-06
Epoch 55/100: Training Loss: 8.056309366006456e-06
Epoch 56/100: Training Loss: 5.7910876949467964e-06
Epoch 57/100: Training Loss: 1.5065322891907758e-05
Epoch 58/100: Training Loss: 6.6117164657412585e-06
Epoch 59/100: Training Loss: 4.0838451978797734e-06
Epoch 60/100: Training Loss: 1.8595327292719196e-05
Epoch 61/100: Training Loss: 9.0279524141605e-06
Epoch 62/100: Training Loss: 1.6085845353897268e-05
Epoch 63/100: Training Loss: 1.4724721702905843e-05
Epoch 64/100: Training Loss: 2.0164090813854323e-05
Epoch 65/100: Training Loss: 3.3411154446239295e-05
Epoch 66/100: Training Loss: 2.011696889584515e-05
Epoch 67/100: Training Loss: 3.136034780246321e-05
Epoch 68/100: Training Loss: 2.4761184878338318e-05
Epoch 69/100: Training Loss: 1.0949088483705499e-05
Epoch 70/100: Training Loss: 7.72624564129636e-06
Epoch 71/100: Training Loss: 2.207021252574047e-06
Epoch 72/100: Training Loss: 1.3305646826594656e-05
Epoch 73/100: Training Loss: 5.622813773174096e-07
Epoch 74/100: Training Loss: 9.622831489839313e-06
Epoch 75/100: Training Loss: 6.408311001750456e-06
Epoch 76/100: Training Loss: 3.4123136069677118e-06
Epoch 77/100: Training Loss: 8.036587430241471e-06
Epoch 78/100: Training Loss: 3.459482252048457e-05
Epoch 79/100: Training Loss: 6.969188219837604e-06
Epoch 80/100: Training Loss: 2.248788870493388e-06
Epoch 81/100: Training Loss: 3.445394335031372e-06
Epoch 82/100: Training Loss: 1.401741761491046e-05
Epoch 83/100: Training Loss: 4.000567792472752e-05
Epoch 84/100: Training Loss: 2.8705808080835824e-05
Epoch 85/100: Training Loss: 2.2112642959927634e-05
Epoch 86/100: Training Loss: 9.335416545104322e-06
Epoch 87/100: Training Loss: 4.104254627814903e-06
Epoch 88/100: Training Loss: 2.930111419152005e-06
Epoch 89/100: Training Loss: 1.6744390198711976e-05
Epoch 90/100: Training Loss: 2.260531045599467e-05
Epoch 91/100: Training Loss: 9.279682039184504e-06
Epoch 92/100: Training Loss: 9.359724064301785e-05
Epoch 93/100: Training Loss: 1.0011730552543693e-05
Epoch 94/100: Training Loss: 2.6263757973634703e-05
Epoch 95/100: Training Loss: 5.868031248484614e-06
Epoch 96/100: Training Loss: 1.0950590616890363e-05
Epoch 97/100: Training Loss: 1.52678952084571e-05
Epoch 98/100: Training Loss: 1.3362717158096726e-05
Epoch 99/100: Training Loss: 4.194342114958346e-05
Epoch 0/100: Training Loss: 0.0006294456203426935
Epoch 1/100: Training Loss: 0.0001911362385855312
Epoch 2/100: Training Loss: 0.000421609894364281
Epoch 3/100: Training Loss: 0.00023050581525384852
Epoch 4/100: Training Loss: 0.0002834219980028878
Epoch 5/100: Training Loss: 2.745734280453319e-05
Epoch 6/100: Training Loss: 2.1883175271538508e-05
Epoch 7/100: Training Loss: 0.00020993721827996517
Epoch 8/100: Training Loss: 0.0005022567589726068
Epoch 9/100: Training Loss: 0.00028831768879848245
Epoch 10/100: Training Loss: 6.616827248867634e-05
Epoch 11/100: Training Loss: 2.430250406660865e-05
Epoch 12/100: Training Loss: 0.00019064209366266707
Epoch 13/100: Training Loss: 7.748674700217965e-05
Epoch 14/100: Training Loss: 0.0002634803541993673
Epoch 15/100: Training Loss: 0.0001809703499342488
Epoch 16/100: Training Loss: 0.0005441525604872577
Epoch 17/100: Training Loss: 4.173112225479784e-05
Epoch 18/100: Training Loss: 0.00010418907566144403
Epoch 19/100: Training Loss: 6.150447277236829e-05
Epoch 20/100: Training Loss: 0.00019503788676409595
Epoch 21/100: Training Loss: 0.00011602869933157895
Epoch 22/100: Training Loss: 0.00012163097549856237
Epoch 23/100: Training Loss: 9.86302831927232e-05
Epoch 24/100: Training Loss: 0.00019730212150421817
Epoch 25/100: Training Loss: 5.5353099529722094e-05
Epoch 26/100: Training Loss: 9.763722134902414e-05
Epoch 27/100: Training Loss: 3.586478729163651e-05
Epoch 28/100: Training Loss: 0.0001223376762550489
Epoch 29/100: Training Loss: 4.373867872410116e-05
Epoch 30/100: Training Loss: 9.050249921537079e-05
Epoch 31/100: Training Loss: 2.7728042425175683e-05
Epoch 32/100: Training Loss: 0.00017834406972458931
Epoch 33/100: Training Loss: 0.00019162972416497965
Epoch 34/100: Training Loss: 8.586751280632694e-05
Epoch 35/100: Training Loss: 2.3394763321343775e-05
Epoch 36/100: Training Loss: 3.2856856448064864e-05
Epoch 37/100: Training Loss: 0.0001694659282148412
Epoch 38/100: Training Loss: 4.546683430012348e-05
Epoch 39/100: Training Loss: 5.743075656679879e-05
Epoch 40/100: Training Loss: 4.8095529355042804e-05
Epoch 41/100: Training Loss: 3.866695972011153e-05
Epoch 42/100: Training Loss: 3.5598045791936134e-05
Epoch 43/100: Training Loss: 9.261937070209368e-05
Epoch 44/100: Training Loss: 9.549091017879217e-05
Epoch 45/100: Training Loss: 0.000617433965733621
Epoch 46/100: Training Loss: 0.00017829575633580705
Epoch 47/100: Training Loss: 0.00015137807787519642
Epoch 48/100: Training Loss: 0.00010557699355140197
Epoch 49/100: Training Loss: 4.370970470187938e-05
Epoch 50/100: Training Loss: 0.00011309120902972939
Epoch 51/100: Training Loss: 0.00011497832406147391
Epoch 52/100: Training Loss: 4.168680201457665e-05
Epoch 53/100: Training Loss: 1.496271296214741e-05
Epoch 54/100: Training Loss: 1.5216220203226646e-05
Epoch 55/100: Training Loss: 5.529890974274779e-05
Epoch 56/100: Training Loss: 2.878819246141784e-05
Epoch 57/100: Training Loss: 3.288999463604615e-05
Epoch 58/100: Training Loss: 1.5250452489188288e-05
Epoch 59/100: Training Loss: 1.87401882315104e-05
Epoch 60/100: Training Loss: 1.6698235670615613e-05
Epoch 61/100: Training Loss: 7.977982183539234e-06
Epoch 62/100: Training Loss: 2.2382374473773273e-05
Epoch 63/100: Training Loss: 9.72149943092228e-06
Epoch 64/100: Training Loss: 7.275728492346485e-05
Epoch 65/100: Training Loss: 2.186215870016444e-05
Epoch 66/100: Training Loss: 6.0241203755140305e-05
Epoch 67/100: Training Loss: 2.5357509163760505e-05
Epoch 68/100: Training Loss: 9.274422355919284e-06
Epoch 69/100: Training Loss: 0.00020112239022170548
Epoch 70/100: Training Loss: 2.6644176630451616e-05
Epoch 71/100: Training Loss: 1.8284448002160122e-05
Epoch 72/100: Training Loss: 0.000246273040507747
Epoch 73/100: Training Loss: 7.329525145809207e-05
Epoch 74/100: Training Loss: 4.860411389870981e-05
Epoch 75/100: Training Loss: 0.00012093493962182408
Epoch 76/100: Training Loss: 2.8070744407256093e-05
Epoch 77/100: Training Loss: 0.0003041113042725926
Epoch 78/100: Training Loss: 2.4638606433187965e-05
Epoch 79/100: Training Loss: 2.2877432175178444e-05
Epoch 80/100: Training Loss: 3.383715381532644e-05
Epoch 81/100: Training Loss: 3.2213524775167485e-05
Epoch 82/100: Training Loss: 0.00011282790023668677
Epoch 83/100: Training Loss: 2.999429022316384e-05
Epoch 84/100: Training Loss: 6.651851571107332e-05
Epoch 85/100: Training Loss: 8.072492262694688e-05
Epoch 86/100: Training Loss: 3.934543233661525e-05
Epoch 87/100: Training Loss: 7.369273663622088e-05
Epoch 88/100: Training Loss: 1.318525292177116e-05
Epoch 89/100: Training Loss: 0.00011341865545353004
Epoch 90/100: Training Loss: 4.298628958980594e-05
Epoch 91/100: Training Loss: 2.5866627363504563e-05
Epoch 92/100: Training Loss: 4.239729399570322e-05
Epoch 93/100: Training Loss: 1.975658877167554e-05
Epoch 94/100: Training Loss: 2.27155757293237e-05
Epoch 95/100: Training Loss: 9.195257670583978e-05
Epoch 96/100: Training Loss: 3.079283339655505e-05
Epoch 97/100: Training Loss: 6.357138663266613e-05
Epoch 98/100: Training Loss: 5.8981285026643126e-05
Epoch 99/100: Training Loss: 0.00014728343987886885
Epoch 0/100: Training Loss: 0.0016666396850962275
Epoch 1/100: Training Loss: 0.001025174832130227
Epoch 2/100: Training Loss: 0.0011893390540050285
Epoch 3/100: Training Loss: 0.0008924884111891948
Epoch 4/100: Training Loss: 0.0008514915747492838
Epoch 5/100: Training Loss: 0.0008751320999299464
Epoch 6/100: Training Loss: 0.0010557579620002096
Epoch 7/100: Training Loss: 0.0005062090174499648
Epoch 8/100: Training Loss: 0.0006282288130088772
Epoch 9/100: Training Loss: 0.0005749182048934457
Epoch 10/100: Training Loss: 0.0003851134279918243
Epoch 11/100: Training Loss: 0.0007867817119632601
Epoch 12/100: Training Loss: 0.00034044943582851254
Epoch 13/100: Training Loss: 0.0003194762853228992
Epoch 14/100: Training Loss: 0.0004374147664271128
Epoch 15/100: Training Loss: 0.0002761686997563315
Epoch 16/100: Training Loss: 0.0006558456763023753
Epoch 17/100: Training Loss: 0.00029216412738834263
Epoch 18/100: Training Loss: 0.0002340430669453112
Epoch 19/100: Training Loss: 0.00016922000879129488
Epoch 20/100: Training Loss: 0.00019779096403464073
Epoch 21/100: Training Loss: 6.819076547942086e-06
Epoch 22/100: Training Loss: 0.00019884259845109263
Epoch 23/100: Training Loss: 0.00016226188484328745
Epoch 24/100: Training Loss: 5.405892789230218e-05
Epoch 25/100: Training Loss: 2.9022886831263255e-05
Epoch 26/100: Training Loss: 3.986489946532143e-05
Epoch 27/100: Training Loss: 0.00018682613166992975
Epoch 28/100: Training Loss: 0.00015015439067720833
Epoch 29/100: Training Loss: 2.1688769584012138e-05
Epoch 30/100: Training Loss: 3.0379019878103058e-05
Epoch 31/100: Training Loss: 4.552341914096755e-05
Epoch 32/100: Training Loss: 1.999214834496045e-05
Epoch 33/100: Training Loss: 1.806358602148535e-05
Epoch 34/100: Training Loss: 0.00032805073421632227
Epoch 35/100: Training Loss: 1.0505713305264846e-05
Epoch 36/100: Training Loss: 3.937108550772004e-05
Epoch 37/100: Training Loss: 7.698404803170484e-06
Epoch 38/100: Training Loss: 1.1634037750585197e-05
Epoch 39/100: Training Loss: 2.3138229087863802e-05
Epoch 40/100: Training Loss: 3.698320052849605e-06
Epoch 41/100: Training Loss: 1.5983371598637694e-05
Epoch 42/100: Training Loss: 2.3347659729788657e-05
Epoch 43/100: Training Loss: 2.2644649004989675e-05
Epoch 44/100: Training Loss: 0.0013415765334672458
Epoch 45/100: Training Loss: 7.836448003625656e-05
Epoch 46/100: Training Loss: 7.339861795239385e-05
Epoch 47/100: Training Loss: 5.325780254308418e-05
Epoch 48/100: Training Loss: 3.600647180203365e-05
Epoch 49/100: Training Loss: 4.484534948529684e-05
Epoch 50/100: Training Loss: 2.7129027758612226e-05
Epoch 51/100: Training Loss: 3.44354016163424e-05
Epoch 52/100: Training Loss: 1.601474317868194e-05
Epoch 53/100: Training Loss: 1.0951120414741906e-05
Epoch 54/100: Training Loss: 1.7493760689358007e-05
Epoch 55/100: Training Loss: 1.2538961715361462e-05
Epoch 56/100: Training Loss: 3.1803083092375184e-05
Epoch 57/100: Training Loss: 1.6520800060621826e-05
Epoch 58/100: Training Loss: 0.00011547084379891109
Epoch 59/100: Training Loss: 1.244867771797116e-05
Epoch 60/100: Training Loss: 6.663855248769837e-06
Epoch 61/100: Training Loss: 1.5792126804924333e-05
Epoch 62/100: Training Loss: 1.907531556259891e-05
Epoch 63/100: Training Loss: 9.770728564182205e-06
Epoch 64/100: Training Loss: 1.2418303237768566e-05
Epoch 65/100: Training Loss: 3.7399565408449e-06
Epoch 66/100: Training Loss: 3.540033585237414e-06
Epoch 67/100: Training Loss: 2.1467584648875377e-05
Epoch 68/100: Training Loss: 6.6014566362706e-06
Epoch 69/100: Training Loss: 1.1886915843516187e-05
Epoch 70/100: Training Loss: 1.6434409712011103e-06
Epoch 71/100: Training Loss: 3.231287883770038e-06
Epoch 72/100: Training Loss: 1.1601798276045263e-06
Epoch 73/100: Training Loss: 5.068059075172706e-06
Epoch 74/100: Training Loss: 3.3190117881637517e-06
Epoch 75/100: Training Loss: 9.962114309182082e-06
Epoch 76/100: Training Loss: 5.191549107499187e-06
Epoch 77/100: Training Loss: 1.2447971917454972e-05
Epoch 78/100: Training Loss: 4.34877056494823e-06
Epoch 79/100: Training Loss: 6.138863388632712e-06
Epoch 80/100: Training Loss: 1.449547859426638e-06
Epoch 81/100: Training Loss: 2.447523817737407e-06
Epoch 82/100: Training Loss: 3.105709162194098e-06
Epoch 83/100: Training Loss: 1.5612512319494924e-06
Epoch 84/100: Training Loss: 1.8049783236242731e-06
Epoch 85/100: Training Loss: 1.020270732289907e-06
Epoch 86/100: Training Loss: 2.5960183472710874e-06
Epoch 87/100: Training Loss: 1.9247215575300524e-05
Epoch 88/100: Training Loss: 1.953061875749396e-06
Epoch 89/100: Training Loss: 7.4077376703243085e-06
Epoch 90/100: Training Loss: 6.777584535015471e-07
Epoch 91/100: Training Loss: 4.80842513487478e-05
Epoch 92/100: Training Loss: 3.0110520206291577e-06
Epoch 93/100: Training Loss: 2.7776750602295847e-06
Epoch 94/100: Training Loss: 2.366685122669144e-06
Epoch 95/100: Training Loss: 3.0526201211484025e-06
Epoch 96/100: Training Loss: 5.747365447872155e-06
Epoch 97/100: Training Loss: 2.851460448512181e-06
Epoch 98/100: Training Loss: 1.3020486025883077e-06
Epoch 99/100: Training Loss: 0.0004233660117927688
dataset: cities layer_num_from_end:-17 Avg_acc:tensor(0.8487) Avg_AUC:0.9324404166979002 Avg_threshold:0.265811949968338
dataset: inventions layer_num_from_end:-17 Avg_acc:tensor(0.8390) Avg_AUC:0.9371285989286909 Avg_threshold:0.2892400622367859
dataset: elements layer_num_from_end:-17 Avg_acc:tensor(0.7548) Avg_AUC:0.8762909006821598 Avg_threshold:0.2865006923675537
dataset: animals layer_num_from_end:-17 Avg_acc:tensor(0.6468) Avg_AUC:0.7881275195263291 Avg_threshold:0.8957800269126892
dataset: companies layer_num_from_end:-17 Avg_acc:tensor(0.8275) Avg_AUC:0.9284277777777777 Avg_threshold:0.474723219871521
dataset: facts layer_num_from_end:-17 Avg_acc:tensor(0.7173) Avg_AUC:0.8550504079627493 Avg_threshold:0.8050251603126526
dataset: conj_neg_facts layer_num_from_end:-17 Avg_acc:tensor(0.7344) Avg_AUC:0.6062144664629137 Avg_threshold:0.30294838547706604


