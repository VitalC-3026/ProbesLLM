2024-04-25 05:07:28.791059: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-04-25 05:07:29.803015: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
================layer -1================
Epoch 0/100: Training Loss: 0.0034233250401236796
Epoch 1/100: Training Loss: 0.003854947073476298
Epoch 2/100: Training Loss: 0.004124196676107554
Epoch 3/100: Training Loss: 0.003823997257472752
Epoch 4/100: Training Loss: 0.0031170201051485286
Epoch 5/100: Training Loss: 0.0032768601720983333
Epoch 6/100: Training Loss: 0.003136551671928459
Epoch 7/100: Training Loss: 0.0033948590288629065
Epoch 8/100: Training Loss: 0.0031557097718432235
Epoch 9/100: Training Loss: 0.0031397046742739376
Epoch 10/100: Training Loss: 0.002380096203797347
Epoch 11/100: Training Loss: 0.0027866236396602816
Epoch 12/100: Training Loss: 0.0025528929867110885
Epoch 13/100: Training Loss: 0.0025064549246034424
Epoch 14/100: Training Loss: 0.0021801409604666117
Epoch 15/100: Training Loss: 0.002300807020880959
Epoch 16/100: Training Loss: 0.0013846072492065963
Epoch 17/100: Training Loss: 0.0019651805604254447
Epoch 18/100: Training Loss: 0.0016742997861408687
Epoch 19/100: Training Loss: 0.0013400405853778333
Epoch 20/100: Training Loss: 0.0015791844237934458
Epoch 21/100: Training Loss: 0.0011835878753995563
Epoch 22/100: Training Loss: 0.0011580280073872813
Epoch 23/100: Training Loss: 0.0010608606613599337
Epoch 24/100: Training Loss: 0.0012877011424178011
Epoch 25/100: Training Loss: 0.001107053219021617
Epoch 26/100: Training Loss: 0.0008355384835830102
Epoch 27/100: Training Loss: 0.0006053756167004992
Epoch 28/100: Training Loss: 0.0008355242597473251
Epoch 29/100: Training Loss: 0.000581327539223891
Epoch 30/100: Training Loss: 0.0006110010134590256
Epoch 31/100: Training Loss: 0.0008869298271365932
Epoch 32/100: Training Loss: 0.0007280792285512377
Epoch 33/100: Training Loss: 0.0007138666468900401
Epoch 34/100: Training Loss: 0.0013158591268779515
Epoch 35/100: Training Loss: 0.001386631723050471
Epoch 36/100: Training Loss: 0.00036044681926707285
Epoch 37/100: Training Loss: 0.0004167911056038383
Epoch 38/100: Training Loss: 0.0005810218049096061
Epoch 39/100: Training Loss: 0.0004239123757485743
Epoch 40/100: Training Loss: 0.001613692595408513
Epoch 41/100: Training Loss: 0.00012813154217246529
Epoch 42/100: Training Loss: 0.00015243004773046588
Epoch 43/100: Training Loss: 0.0004364224736923938
Epoch 44/100: Training Loss: 0.0003888423105219861
Epoch 45/100: Training Loss: 0.0003785607698080423
Epoch 46/100: Training Loss: 0.00036644425008680435
Epoch 47/100: Training Loss: 0.0003491596846313743
Epoch 48/100: Training Loss: 0.00035311573452049205
Epoch 49/100: Training Loss: 0.00018692077821368103
Epoch 50/100: Training Loss: 0.0005515059391101757
Epoch 51/100: Training Loss: 8.226710964332928e-05
Epoch 52/100: Training Loss: 0.00044539482235074875
Epoch 53/100: Training Loss: 0.0009159028946936547
Epoch 54/100: Training Loss: 0.00031503493135625666
Epoch 55/100: Training Loss: 0.0001570652727480535
Epoch 56/100: Training Loss: 0.00042274536994787364
Epoch 57/100: Training Loss: 0.00013589736569177854
Epoch 58/100: Training Loss: 0.0004823582468332944
Epoch 59/100: Training Loss: 0.0002553503100688641
Epoch 60/100: Training Loss: 0.00048281356588110224
Epoch 61/100: Training Loss: 0.0006329836128474949
Epoch 62/100: Training Loss: 0.000195709897296412
Epoch 63/100: Training Loss: 9.661959158582287e-05
Epoch 64/100: Training Loss: 0.00017782015452434967
Epoch 65/100: Training Loss: 0.000269642216967536
Epoch 66/100: Training Loss: 0.0008182446439783057
Epoch 67/100: Training Loss: 0.0014297174407051994
Epoch 68/100: Training Loss: 0.0018249014457622607
Epoch 69/100: Training Loss: 0.0011344544745825387
Epoch 70/100: Training Loss: 0.0006269046685078761
Epoch 71/100: Training Loss: 0.00021239289349609322
Epoch 72/100: Training Loss: 0.00016762632746379692
Epoch 73/100: Training Loss: 0.00029715275639420624
Epoch 74/100: Training Loss: 0.000228053987859846
Epoch 75/100: Training Loss: 0.00013033472574674166
Epoch 76/100: Training Loss: 2.0246972310376333e-05
Epoch 77/100: Training Loss: 2.284125804288925e-06
Epoch 78/100: Training Loss: 2.889334504212533e-05
Epoch 79/100: Training Loss: 1.5581645384118274e-05
Epoch 80/100: Training Loss: 3.918194964241523e-06
Epoch 81/100: Training Loss: 8.11097101419122e-05
Epoch 82/100: Training Loss: 2.840405877132516e-05
Epoch 83/100: Training Loss: 5.782538718850046e-06
Epoch 84/100: Training Loss: 0.0003927282572209418
Epoch 85/100: Training Loss: 0.00011268854662254973
Epoch 86/100: Training Loss: 0.000580994086665707
Epoch 87/100: Training Loss: 0.00028677274297167374
Epoch 88/100: Training Loss: 0.00044341911594350856
Epoch 89/100: Training Loss: 0.00018362962751538602
Epoch 90/100: Training Loss: 0.00038320086635909714
Epoch 91/100: Training Loss: 0.00015691999945190404
Epoch 92/100: Training Loss: 3.697749526871668e-05
Epoch 93/100: Training Loss: 0.00016031470280010384
Epoch 94/100: Training Loss: 0.0001237560714875068
Epoch 95/100: Training Loss: 0.00011786974810220145
Epoch 96/100: Training Loss: 7.941737328167562e-05
Epoch 97/100: Training Loss: 0.0001584283773090456
Epoch 98/100: Training Loss: 1.9404591041636632e-05
Epoch 99/100: Training Loss: 0.00015656173333421453
Epoch 0/100: Training Loss: 0.0011302146173658824
Epoch 1/100: Training Loss: 0.0014634279977707636
Epoch 2/100: Training Loss: 0.0017318251587095715
Epoch 3/100: Training Loss: 0.0012361845799854824
Epoch 4/100: Training Loss: 0.0020181261357806977
Epoch 5/100: Training Loss: 0.0014575904323941185
Epoch 6/100: Training Loss: 0.0013453567311877295
Epoch 7/100: Training Loss: 0.0011916938282194591
Epoch 8/100: Training Loss: 0.0012440982319059826
Epoch 9/100: Training Loss: 0.0007512820618493216
Epoch 10/100: Training Loss: 0.0010863237437747774
Epoch 11/100: Training Loss: 0.0008778066862197149
Epoch 12/100: Training Loss: 0.000919190403961
Epoch 13/100: Training Loss: 0.0009463985761006673
Epoch 14/100: Training Loss: 0.0008496505873543875
Epoch 15/100: Training Loss: 0.0005617016837710426
Epoch 16/100: Training Loss: 0.0007018542005902245
Epoch 17/100: Training Loss: 0.0003730973672299158
Epoch 18/100: Training Loss: 0.0005326266090075175
Epoch 19/100: Training Loss: 0.000651689867178599
Epoch 20/100: Training Loss: 0.00048498411973317465
Epoch 21/100: Training Loss: 0.000755670808610462
Epoch 22/100: Training Loss: 0.00048634608586629234
Epoch 23/100: Training Loss: 0.0004380430139246441
Epoch 24/100: Training Loss: 0.00025572688097045534
Epoch 25/100: Training Loss: 0.00035950647933142524
Epoch 26/100: Training Loss: 0.00026567436399913967
Epoch 27/100: Training Loss: 0.00043638192472003753
Epoch 28/100: Training Loss: 0.00033275946265175227
Epoch 29/100: Training Loss: 9.484141178074337e-05
Epoch 30/100: Training Loss: 0.00023335796736535572
Epoch 31/100: Training Loss: 0.0001256055065563747
Epoch 32/100: Training Loss: 0.00011072083420696713
Epoch 33/100: Training Loss: 0.0003956427531582969
Epoch 34/100: Training Loss: 0.0003370101962770734
Epoch 35/100: Training Loss: 0.00035472334850402104
Epoch 36/100: Training Loss: 0.00025230630167893
Epoch 37/100: Training Loss: 0.00035122718129839217
Epoch 38/100: Training Loss: 0.0002576712341535659
Epoch 39/100: Training Loss: 0.00020719997230030242
Epoch 40/100: Training Loss: 0.0007433078118732997
Epoch 41/100: Training Loss: 0.00033732163054602486
Epoch 42/100: Training Loss: 0.0002031210632551284
Epoch 43/100: Training Loss: 0.00022221273254780543
Epoch 44/100: Training Loss: 4.414288947979609e-05
Epoch 45/100: Training Loss: 0.0002264504631360372
Epoch 46/100: Training Loss: 0.0001568017793553216
Epoch 47/100: Training Loss: 0.00013983809344825292
Epoch 48/100: Training Loss: 0.0001929017227320444
Epoch 49/100: Training Loss: 0.00010427383794671013
Epoch 50/100: Training Loss: 0.00022078539643968854
Epoch 51/100: Training Loss: 0.00015780631275404068
Epoch 52/100: Training Loss: 0.0001251116217601867
Epoch 53/100: Training Loss: 0.00013377654055754344
Epoch 54/100: Training Loss: 0.0001302035614138558
Epoch 55/100: Training Loss: 6.89307227730751e-05
Epoch 56/100: Training Loss: 0.0003144603400003342
Epoch 57/100: Training Loss: 0.00013258827051946094
Epoch 58/100: Training Loss: 0.0002234436926387605
Epoch 59/100: Training Loss: 0.00013307641659464155
Epoch 60/100: Training Loss: 0.00013203055908282598
Epoch 61/100: Training Loss: 0.00014991978449480875
Epoch 62/100: Training Loss: 0.00024123578554108028
Epoch 63/100: Training Loss: 0.00035222126614479793
Epoch 64/100: Training Loss: 0.00013881520855994452
Epoch 65/100: Training Loss: 0.00013533364981412887
Epoch 66/100: Training Loss: 0.00016091400313945044
Epoch 67/100: Training Loss: 0.0006012999585696629
Epoch 68/100: Training Loss: 6.61384580390794e-05
Epoch 69/100: Training Loss: 0.00013482622092678433
Epoch 70/100: Training Loss: 0.00012380414243255342
Epoch 71/100: Training Loss: 0.00013423439647470203
Epoch 72/100: Training Loss: 0.00010749694137346177
Epoch 73/100: Training Loss: 6.495752327498936e-05
Epoch 74/100: Training Loss: 0.000423595415694373
Epoch 75/100: Training Loss: 0.00013185801605383556
Epoch 76/100: Training Loss: 9.56102850891295e-05
Epoch 77/100: Training Loss: 0.00012216268195992425
Epoch 78/100: Training Loss: 0.00013604981913453056
Epoch 79/100: Training Loss: 0.0001388136829648699
Epoch 80/100: Training Loss: 0.00012656130961009434
Epoch 81/100: Training Loss: 8.170039703448613e-05
Epoch 82/100: Training Loss: 8.908284916764214e-05
Epoch 83/100: Training Loss: 0.00014498652446837652
Epoch 84/100: Training Loss: 9.406318976765588e-05
Epoch 85/100: Training Loss: 5.928404806625276e-05
Epoch 86/100: Training Loss: 0.00017363028157325018
Epoch 87/100: Training Loss: 0.00011170157009647007
Epoch 88/100: Training Loss: 5.11143711351213e-05
Epoch 89/100: Training Loss: 4.940547403835115e-05
Epoch 90/100: Training Loss: 0.0001339323641288848
Epoch 91/100: Training Loss: 0.00012840061847652708
Epoch 92/100: Training Loss: 0.00012194920508634476
Epoch 93/100: Training Loss: 8.560552128723689e-05
Epoch 94/100: Training Loss: 0.00040002202703839256
Epoch 95/100: Training Loss: 9.551395972569783e-05
Epoch 96/100: Training Loss: 0.00010881514421531132
Epoch 97/100: Training Loss: 0.00013657644213665098
Epoch 98/100: Training Loss: 0.00011867375246116093
Epoch 99/100: Training Loss: 0.0004459036248070853
Epoch 0/100: Training Loss: 0.0006481136063591754
Epoch 1/100: Training Loss: 0.0008518930962331576
Epoch 2/100: Training Loss: 0.0005345670439356522
Epoch 3/100: Training Loss: 0.0008670094487764281
Epoch 4/100: Training Loss: 0.00042303103051311394
Epoch 5/100: Training Loss: 0.0009833644334074976
Epoch 6/100: Training Loss: 0.000821219859935111
Epoch 7/100: Training Loss: 0.0003688899661711366
Epoch 8/100: Training Loss: 0.00036058637449781387
Epoch 9/100: Training Loss: 0.00028809637045688766
Epoch 10/100: Training Loss: 0.00041332533605378883
Epoch 11/100: Training Loss: 0.00045067979563340296
Epoch 12/100: Training Loss: 0.00018588692950401948
Epoch 13/100: Training Loss: 0.00012049952642523127
Epoch 14/100: Training Loss: 0.00017606204362224332
Epoch 15/100: Training Loss: 7.655132088443929e-05
Epoch 16/100: Training Loss: 7.33787135826312e-05
Epoch 17/100: Training Loss: 5.8221351578653e-05
Epoch 18/100: Training Loss: 4.2413758913533005e-06
Epoch 19/100: Training Loss: 9.90857627037332e-05
Epoch 20/100: Training Loss: 0.0003219289030674264
Epoch 21/100: Training Loss: 0.0002714792887369792
Epoch 22/100: Training Loss: 4.331426297446235e-05
Epoch 23/100: Training Loss: 5.208508377309612e-05
Epoch 24/100: Training Loss: 3.739711058011158e-05
Epoch 25/100: Training Loss: 0.00012773399551709494
Epoch 26/100: Training Loss: 2.2352159434228205e-05
Epoch 27/100: Training Loss: 3.840753750549518e-05
Epoch 28/100: Training Loss: 3.14585556336444e-05
Epoch 29/100: Training Loss: 5.6752756380920504e-05
Epoch 30/100: Training Loss: 2.0965596558235817e-05
Epoch 31/100: Training Loss: 6.188575080115732e-05
Epoch 32/100: Training Loss: 9.500781159035022e-05
Epoch 33/100: Training Loss: 1.3008560735187371e-05
Epoch 34/100: Training Loss: 1.4933612313773706e-05
Epoch 35/100: Training Loss: 4.135673012521913e-05
Epoch 36/100: Training Loss: 2.0112114237080946e-05
Epoch 37/100: Training Loss: 3.872084435817006e-06
Epoch 38/100: Training Loss: 7.602844021017318e-05
Epoch 39/100: Training Loss: 5.458455255849184e-05
Epoch 40/100: Training Loss: 1.4258754089010133e-05
Epoch 41/100: Training Loss: 3.112705895672742e-06
Epoch 42/100: Training Loss: 3.3840937405931866e-06
Epoch 43/100: Training Loss: 1.2204129186322649e-05
Epoch 44/100: Training Loss: 1.8520506570111218e-06
Epoch 45/100: Training Loss: 6.589657885002361e-07
Epoch 46/100: Training Loss: 3.0917430417143186e-05
Epoch 47/100: Training Loss: 8.841394725963652e-07
Epoch 48/100: Training Loss: 2.1881123430182716e-07
Epoch 49/100: Training Loss: 8.545879037867633e-06
Epoch 50/100: Training Loss: 2.14360319904322e-06
Epoch 51/100: Training Loss: 1.1619619301540389e-05
Epoch 52/100: Training Loss: 1.6965499377811697e-06
Epoch 53/100: Training Loss: 8.760794563938102e-06
Epoch 54/100: Training Loss: 3.179493393722198e-06
Epoch 55/100: Training Loss: 2.231697492721341e-06
Epoch 56/100: Training Loss: 1.0311237270597645e-05
Epoch 57/100: Training Loss: 4.135183007311192e-05
Epoch 58/100: Training Loss: 8.670658535353071e-07
Epoch 59/100: Training Loss: 3.1770475552700977e-06
Epoch 60/100: Training Loss: 3.1066454591886174e-07
Epoch 61/100: Training Loss: 8.219408878623296e-07
Epoch 62/100: Training Loss: 2.178346891206303e-06
Epoch 63/100: Training Loss: 1.3917776123939945e-05
Epoch 64/100: Training Loss: 1.3852095827728296e-06
Epoch 65/100: Training Loss: 6.494177081030336e-07
Epoch 66/100: Training Loss: 5.896948612958408e-06
Epoch 67/100: Training Loss: 1.6629673279285859e-06
Epoch 68/100: Training Loss: 1.1802347421889557e-07
Epoch 69/100: Training Loss: 3.4556980970666276e-06
Epoch 70/100: Training Loss: 1.2998565209569404e-05
Epoch 71/100: Training Loss: 2.3909662157851015e-07
Epoch 72/100: Training Loss: 4.015679245622609e-06
Epoch 73/100: Training Loss: 1.4669204363041072e-06
Epoch 74/100: Training Loss: 8.030737888702243e-07
Epoch 75/100: Training Loss: 1.8105079750452253e-06
Epoch 76/100: Training Loss: 4.116218532625434e-06
Epoch 77/100: Training Loss: 8.00922827357404e-07
Epoch 78/100: Training Loss: 1.6781617608144701e-06
Epoch 79/100: Training Loss: 2.0679610699573778e-08
Epoch 80/100: Training Loss: 5.889296887902547e-07
Epoch 81/100: Training Loss: 1.5948342139289962e-06
Epoch 82/100: Training Loss: 5.628919503618058e-07
Epoch 83/100: Training Loss: 4.006627675177132e-07
Epoch 84/100: Training Loss: 1.3390872176631624e-07
Epoch 85/100: Training Loss: 4.135159647474132e-08
Epoch 86/100: Training Loss: 4.123445268473599e-08
Epoch 87/100: Training Loss: 8.77009801861408e-07
Epoch 88/100: Training Loss: 3.894606892162447e-07
Epoch 89/100: Training Loss: 2.367014007595964e-07
Epoch 90/100: Training Loss: 5.403601137889947e-06
Epoch 91/100: Training Loss: 2.4298304729319983e-07
Epoch 92/100: Training Loss: 1.3971214944260012e-06
Epoch 93/100: Training Loss: 1.2365787677599198e-08
Epoch 94/100: Training Loss: 5.926294957887274e-08
Epoch 95/100: Training Loss: 2.2517703417733275e-07
Epoch 96/100: Training Loss: 1.9049141008844621e-07
Epoch 97/100: Training Loss: 1.074222949229078e-08
Epoch 98/100: Training Loss: 1.2865267118619982e-07
Epoch 99/100: Training Loss: 5.132467829347622e-07
Epoch 0/100: Training Loss: 0.0011423994665560515
Epoch 1/100: Training Loss: 0.00127669348232988
Epoch 2/100: Training Loss: 0.0012064573989398239
Epoch 3/100: Training Loss: 0.0015495562035104504
Epoch 4/100: Training Loss: 0.0008802165587743124
Epoch 5/100: Training Loss: 0.001012857913395057
Epoch 6/100: Training Loss: 0.0011379235032675922
Epoch 7/100: Training Loss: 0.0011586708147168735
Epoch 8/100: Training Loss: 0.0009272102140574064
Epoch 9/100: Training Loss: 0.0011387925217117088
Epoch 10/100: Training Loss: 0.0009743757725913743
Epoch 11/100: Training Loss: 0.00048545715601547903
Epoch 12/100: Training Loss: 0.0008664286654928457
Epoch 13/100: Training Loss: 0.0006625591844752215
Epoch 14/100: Training Loss: 0.0006817646216655123
Epoch 15/100: Training Loss: 0.0007888142037506841
Epoch 16/100: Training Loss: 0.000800073866682928
Epoch 17/100: Training Loss: 0.0006153848436143664
Epoch 18/100: Training Loss: 0.0006942481954316587
Epoch 19/100: Training Loss: 0.000832907315613567
Epoch 20/100: Training Loss: 0.0006897787109089358
Epoch 21/100: Training Loss: 0.00046411405007044476
Epoch 22/100: Training Loss: 0.00044408221031732605
Epoch 23/100: Training Loss: 0.0006515323971780602
Epoch 24/100: Training Loss: 0.00039278024780577505
Epoch 25/100: Training Loss: 0.0004833247062664677
Epoch 26/100: Training Loss: 0.0005112347971414022
Epoch 27/100: Training Loss: 0.00017243227808947723
Epoch 28/100: Training Loss: 0.0002824074548223744
Epoch 29/100: Training Loss: 0.0004080269791653767
Epoch 30/100: Training Loss: 0.00016707615647914905
Epoch 31/100: Training Loss: 0.000342648350386228
Epoch 32/100: Training Loss: 0.00012412067988644475
Epoch 33/100: Training Loss: 0.000235537004067702
Epoch 34/100: Training Loss: 0.00020149035226319723
Epoch 35/100: Training Loss: 0.000340750971854021
Epoch 36/100: Training Loss: 0.00028959775100583613
Epoch 37/100: Training Loss: 0.0002662225082012766
Epoch 38/100: Training Loss: 0.0002822871657385342
Epoch 39/100: Training Loss: 0.00034530000122273024
Epoch 40/100: Training Loss: 0.0002539882589365549
Epoch 41/100: Training Loss: 0.0004211146258501615
Epoch 42/100: Training Loss: 0.00027055098958637404
Epoch 43/100: Training Loss: 0.000490630162510895
Epoch 44/100: Training Loss: 0.0006780350841761787
Epoch 45/100: Training Loss: 0.00021636122522722696
Epoch 46/100: Training Loss: 0.0009790604241228333
Epoch 47/100: Training Loss: 0.0005581610372676942
Epoch 48/100: Training Loss: 0.0007636727054337949
Epoch 49/100: Training Loss: 0.00011779241947736142
Epoch 50/100: Training Loss: 0.00010505282648519617
Epoch 51/100: Training Loss: 0.0002824632082008509
Epoch 52/100: Training Loss: 0.00021383947364374059
Epoch 53/100: Training Loss: 0.00024632999790463473
Epoch 54/100: Training Loss: 0.0003242076235116968
Epoch 55/100: Training Loss: 0.00022938716166837203
Epoch 56/100: Training Loss: 0.00020942727220807098
Epoch 57/100: Training Loss: 0.00010853040297538186
Epoch 58/100: Training Loss: 0.00046515569162829487
Epoch 59/100: Training Loss: 0.0002759210585396071
Epoch 60/100: Training Loss: 0.000342364976371544
Epoch 61/100: Training Loss: 0.0003105011827127945
Epoch 62/100: Training Loss: 0.00024021981994886904
Epoch 63/100: Training Loss: 0.00043680155334841224
Epoch 64/100: Training Loss: 0.00038515304885624685
Epoch 65/100: Training Loss: 0.0003985030495602152
Epoch 66/100: Training Loss: 0.00039579665315323984
Epoch 67/100: Training Loss: 0.00019613619227916147
Epoch 68/100: Training Loss: 0.00026888659466867864
Epoch 69/100: Training Loss: 0.00022127457718918288
Epoch 70/100: Training Loss: 0.0002697151208269423
Epoch 71/100: Training Loss: 0.00031404312394091473
Epoch 72/100: Training Loss: 0.00040184674055679983
Epoch 73/100: Training Loss: 0.002102804788644763
Epoch 74/100: Training Loss: 0.0007728331762811412
Epoch 75/100: Training Loss: 0.00024346273014510888
Epoch 76/100: Training Loss: 0.000264268782403734
Epoch 77/100: Training Loss: 0.0002626899791800457
Epoch 78/100: Training Loss: 6.637793340256824e-05
Epoch 79/100: Training Loss: 0.00010343246917793716
Epoch 80/100: Training Loss: 0.0005557676732252185
Epoch 81/100: Training Loss: 9.110962279176942e-05
Epoch 82/100: Training Loss: 0.00016597795169710536
Epoch 83/100: Training Loss: 0.00015850829473440197
Epoch 84/100: Training Loss: 0.0001372505465279455
Epoch 85/100: Training Loss: 0.00020820516103131758
Epoch 86/100: Training Loss: 0.0001270167781534978
Epoch 87/100: Training Loss: 0.0001127656081304458
Epoch 88/100: Training Loss: 0.00010048493670956524
Epoch 89/100: Training Loss: 0.00018378235579688768
Epoch 90/100: Training Loss: 9.360486542545079e-05
Epoch 91/100: Training Loss: 6.676864796790524e-05
Epoch 92/100: Training Loss: 0.0005450851963338069
Epoch 93/100: Training Loss: 8.814918678163906e-05
Epoch 94/100: Training Loss: 0.00026006003220876056
Epoch 95/100: Training Loss: 5.762021232774292e-05
Epoch 96/100: Training Loss: 0.00011444552509105148
Epoch 97/100: Training Loss: 0.00010703627362055479
Epoch 98/100: Training Loss: 0.00021205536553249268
Epoch 99/100: Training Loss: 0.00024135094046016822
Epoch 0/100: Training Loss: 0.0012724475533354516
Epoch 1/100: Training Loss: 0.001004206272316914
Epoch 2/100: Training Loss: 0.0010770187512332318
Epoch 3/100: Training Loss: 0.0011831521257465961
Epoch 4/100: Training Loss: 0.0011383913749573277
Epoch 5/100: Training Loss: 0.0008802719268144346
Epoch 6/100: Training Loss: 0.0008920620323396197
Epoch 7/100: Training Loss: 0.0010438129744108986
Epoch 8/100: Training Loss: 0.0009202979797241735
Epoch 9/100: Training Loss: 0.0009468843539555868
Epoch 10/100: Training Loss: 0.0009538206253565994
Epoch 11/100: Training Loss: 0.0010077844501710405
Epoch 12/100: Training Loss: 0.0008875647304104824
Epoch 13/100: Training Loss: 0.0009158231025817347
Epoch 14/100: Training Loss: 0.0008534057175411898
Epoch 15/100: Training Loss: 0.0007050138916455063
Epoch 16/100: Training Loss: 0.0009800771287843293
Epoch 17/100: Training Loss: 0.0006651454696468278
Epoch 18/100: Training Loss: 0.0008646413391711665
Epoch 19/100: Training Loss: 0.0005500891848522075
Epoch 20/100: Training Loss: 0.00048454861868830286
Epoch 21/100: Training Loss: 0.0004208254828756931
Epoch 22/100: Training Loss: 0.00066109206162247
Epoch 23/100: Training Loss: 0.0005463674007093205
Epoch 24/100: Training Loss: 0.00048042768064667196
Epoch 25/100: Training Loss: 0.0003331347788665809
Epoch 26/100: Training Loss: 0.0005682434375379599
Epoch 27/100: Training Loss: 0.00042951260419452894
Epoch 28/100: Training Loss: 0.00023396090403491376
Epoch 29/100: Training Loss: 0.00031240824975219426
Epoch 30/100: Training Loss: 0.0002716194253926184
Epoch 31/100: Training Loss: 0.0002942840550460067
Epoch 32/100: Training Loss: 0.00014015053417168412
Epoch 33/100: Training Loss: 0.00042163916662627573
Epoch 34/100: Training Loss: 0.00011045724957012663
Epoch 35/100: Training Loss: 0.000137741147887473
Epoch 36/100: Training Loss: 3.40643831912209e-05
Epoch 37/100: Training Loss: 0.00010227721513194196
Epoch 38/100: Training Loss: 7.634935900568962e-05
Epoch 39/100: Training Loss: 7.368957934280236e-05
Epoch 40/100: Training Loss: 6.218418004173858e-05
Epoch 41/100: Training Loss: 0.00015367001441179536
Epoch 42/100: Training Loss: 0.0001868137761073954
Epoch 43/100: Training Loss: 0.00024819363127736487
Epoch 44/100: Training Loss: 0.00023287856111339495
Epoch 45/100: Training Loss: 0.0001250217285226373
Epoch 46/100: Training Loss: 9.644906237429263e-05
Epoch 47/100: Training Loss: 0.00023740844107141682
Epoch 48/100: Training Loss: 6.582667393719449e-05
Epoch 49/100: Training Loss: 9.906239004111758e-05
Epoch 50/100: Training Loss: 7.855251212330426e-05
Epoch 51/100: Training Loss: 9.431712804179566e-05
Epoch 52/100: Training Loss: 6.16335720919511e-05
Epoch 53/100: Training Loss: 6.688688425164597e-05
Epoch 54/100: Training Loss: 3.7805026616243756e-05
Epoch 55/100: Training Loss: 5.371958090394151e-05
Epoch 56/100: Training Loss: 5.656534203273408e-05
Epoch 57/100: Training Loss: 0.0003657341003417969
Epoch 58/100: Training Loss: 5.957711597575861e-05
Epoch 59/100: Training Loss: 6.616440619908127e-05
Epoch 60/100: Training Loss: 4.2524056363047335e-05
Epoch 61/100: Training Loss: 0.00013661945202186995
Epoch 62/100: Training Loss: 6.170006121928785e-05
Epoch 63/100: Training Loss: 7.667396144539703e-05
Epoch 64/100: Training Loss: 5.495964147734876e-05
Epoch 65/100: Training Loss: 9.00832169196185e-05
Epoch 66/100: Training Loss: 0.00012452379964730318
Epoch 67/100: Training Loss: 0.0002400227183220433
Epoch 68/100: Training Loss: 8.01362535532783e-05
Epoch 69/100: Training Loss: 0.00018462257496282167
Epoch 70/100: Training Loss: 3.424906811001254e-05
Epoch 71/100: Training Loss: 6.364489558572863e-05
Epoch 72/100: Training Loss: 7.536443972996637e-05
Epoch 73/100: Training Loss: 3.516584790001313e-05
Epoch 74/100: Training Loss: 2.569925752194489e-05
Epoch 75/100: Training Loss: 0.00026534914093859056
Epoch 76/100: Training Loss: 1.747698030051063e-05
Epoch 77/100: Training Loss: 1.3552619801724658e-05
Epoch 78/100: Training Loss: 3.323394933021536e-05
Epoch 79/100: Training Loss: 4.43173727641503e-05
Epoch 80/100: Training Loss: 4.423606921644772e-05
Epoch 81/100: Training Loss: 1.0768547380233512e-05
Epoch 82/100: Training Loss: 0.0002623528753425561
Epoch 83/100: Training Loss: 5.131249096901978e-05
Epoch 84/100: Training Loss: 1.2173648357537446e-05
Epoch 85/100: Training Loss: 2.6134970397049304e-05
Epoch 86/100: Training Loss: 0.00024361229118178873
Epoch 87/100: Training Loss: 0.00015995797573351394
Epoch 88/100: Training Loss: 3.6447292979003165e-05
Epoch 89/100: Training Loss: 3.805038446158755e-05
Epoch 90/100: Training Loss: 2.9806428881106423e-05
Epoch 91/100: Training Loss: 1.1317671605331056e-05
Epoch 92/100: Training Loss: 0.0008091855721146453
Epoch 93/100: Training Loss: 0.0006078687660834368
Epoch 94/100: Training Loss: 0.00020947172215171888
Epoch 95/100: Training Loss: 2.2681503027093177e-05
Epoch 96/100: Training Loss: 0.00015479930694781098
Epoch 97/100: Training Loss: 3.8579298073754594e-05
Epoch 98/100: Training Loss: 5.2422874004525295e-05
Epoch 99/100: Training Loss: 2.5176694251450837e-05
Epoch 0/100: Training Loss: 7.781766756357018e-05
Epoch 1/100: Training Loss: 0.0001163163180931949
Epoch 2/100: Training Loss: 0.0010826656075774647
Epoch 3/100: Training Loss: 0.00011982967688830731
Epoch 4/100: Training Loss: 0.001200927643921113
Epoch 5/100: Training Loss: 0.0002489820890460137
Epoch 6/100: Training Loss: 1.6836312095649908e-05
Epoch 7/100: Training Loss: 0.00013268443230164413
Epoch 8/100: Training Loss: 0.00016100498775687653
Epoch 9/100: Training Loss: 0.0008013629522479949
Epoch 10/100: Training Loss: 7.970378989358138e-05
Epoch 11/100: Training Loss: 2.6815588339569977e-05
Epoch 12/100: Training Loss: 0.00011552513448360095
Epoch 13/100: Training Loss: 1.7587647313116865e-05
Epoch 14/100: Training Loss: 0.0005848801736250973
Epoch 15/100: Training Loss: 0.00012916200892428324
Epoch 16/100: Training Loss: 0.00014374689558909146
Epoch 17/100: Training Loss: 8.882913712316151e-05
Epoch 18/100: Training Loss: 3.442308261327498e-05
Epoch 19/100: Training Loss: 6.150054708297693e-05
Epoch 20/100: Training Loss: 7.612054918912312e-05
Epoch 21/100: Training Loss: 3.8630322529225496e-05
Epoch 22/100: Training Loss: 7.325725791325893e-05
Epoch 23/100: Training Loss: 2.4592045877800613e-05
Epoch 24/100: Training Loss: 6.3385089218127925e-06
Epoch 25/100: Training Loss: 2.4306118941977097e-05
Epoch 26/100: Training Loss: 4.599052339582868e-05
Epoch 27/100: Training Loss: 3.6779477040717415e-05
Epoch 28/100: Training Loss: 3.6493275288396473e-05
Epoch 29/100: Training Loss: 0.00011113550365669107
Epoch 30/100: Training Loss: 0.00013064313898041878
Epoch 31/100: Training Loss: 2.191106045064658e-05
Epoch 32/100: Training Loss: 5.129793663214744e-05
Epoch 33/100: Training Loss: 3.159412595092273e-05
Epoch 34/100: Training Loss: 6.955297580926704e-05
Epoch 35/100: Training Loss: 1.5627915602378042e-05
Epoch 36/100: Training Loss: 2.9656495157393693e-05
Epoch 37/100: Training Loss: 2.0250545016356875e-05
Epoch 38/100: Training Loss: 3.454230062715901e-05
Epoch 39/100: Training Loss: 2.9429649406350468e-05
Epoch 40/100: Training Loss: 4.2879382536617875e-05
Epoch 41/100: Training Loss: 3.249619583614537e-05
Epoch 42/100: Training Loss: 5.4608450316991964e-05
Epoch 43/100: Training Loss: 5.259393197451598e-05
Epoch 44/100: Training Loss: 1.553814657538501e-05
Epoch 45/100: Training Loss: 3.9751617460396024e-05
Epoch 46/100: Training Loss: 1.3335224075255963e-05
Epoch 47/100: Training Loss: 3.079190605976543e-05
Epoch 48/100: Training Loss: 4.9892477080470225e-05
Epoch 49/100: Training Loss: 3.6732723338821735e-05
Epoch 50/100: Training Loss: 7.557121282159864e-05
Epoch 51/100: Training Loss: 6.29576445999937e-07
Epoch 52/100: Training Loss: 3.382494014445736e-05
Epoch 53/100: Training Loss: 1.6873266190220656e-05
Epoch 54/100: Training Loss: 6.534935103890768e-06
Epoch 55/100: Training Loss: 5.5679436380466755e-05
Epoch 56/100: Training Loss: 2.4675964148806744e-06
Epoch 57/100: Training Loss: 1.406215951370132e-05
Epoch 58/100: Training Loss: 2.6951552711130583e-05
Epoch 59/100: Training Loss: 4.022942465540257e-06
Epoch 60/100: Training Loss: 9.602065246107288e-07
Epoch 61/100: Training Loss: 1.9455557597465204e-06
Epoch 62/100: Training Loss: 3.916380732850252e-06
Epoch 63/100: Training Loss: 5.861875149628597e-06
Epoch 64/100: Training Loss: 6.353975528855513e-06
Epoch 65/100: Training Loss: 4.31573572426825e-06
Epoch 66/100: Training Loss: 4.729818753392133e-06
Epoch 67/100: Training Loss: 3.2251886151015617e-06
Epoch 68/100: Training Loss: 1.982827856264293e-05
Epoch 69/100: Training Loss: 9.943866356334865e-06
Epoch 70/100: Training Loss: 6.213189327360875e-06
Epoch 71/100: Training Loss: 1.2193864268148252e-06
Epoch 72/100: Training Loss: 8.15311134891013e-06
Epoch 73/100: Training Loss: 4.652458454637115e-06
Epoch 74/100: Training Loss: 4.23829908759728e-06
Epoch 75/100: Training Loss: 1.2100783811415387e-06
Epoch 76/100: Training Loss: 1.1258239506698045e-05
Epoch 77/100: Training Loss: 2.423956972793338e-05
Epoch 78/100: Training Loss: 6.896884789127656e-06
Epoch 79/100: Training Loss: 2.938039991467541e-08
Epoch 80/100: Training Loss: 6.512728949411133e-06
Epoch 81/100: Training Loss: 7.878740201431568e-07
Epoch 82/100: Training Loss: 3.7767069520040545e-07
Epoch 83/100: Training Loss: 1.2212428558766144e-05
Epoch 84/100: Training Loss: 1.4818305928321158e-06
Epoch 85/100: Training Loss: 2.5526169508660706e-06
Epoch 86/100: Training Loss: 2.7555454875706788e-06
Epoch 87/100: Training Loss: 3.165126488101287e-06
Epoch 88/100: Training Loss: 2.077493169922949e-06
Epoch 89/100: Training Loss: 1.2989917418873701e-06
Epoch 90/100: Training Loss: 2.4951579467997617e-06
Epoch 91/100: Training Loss: 6.878560826627306e-07
Epoch 92/100: Training Loss: 2.298384972475033e-06
Epoch 93/100: Training Loss: 2.394747959593839e-06
Epoch 94/100: Training Loss: 5.182691258694584e-06
Epoch 95/100: Training Loss: 4.149951581653443e-06
Epoch 96/100: Training Loss: 3.6522804430099226e-06
Epoch 97/100: Training Loss: 2.144395719463359e-06
Epoch 98/100: Training Loss: 1.523255124367529e-07
Epoch 99/100: Training Loss: 8.449970010615693e-07
dataset: cities layer_num_from_end:-1 Avg_acc:tensor(0.6065) Avg_AUC:0.721865171426027 Avg_threshold:0.20312882959842682
dataset: inventions layer_num_from_end:-1 Avg_acc:tensor(0.5799) Avg_AUC:0.7989517073987278 Avg_threshold:0.6556137204170227
dataset: elements layer_num_from_end:-1 Avg_acc:tensor(0.6860) Avg_AUC:0.6808185917447104 Avg_threshold:0.622338593006134
dataset: animals layer_num_from_end:-1 Avg_acc:tensor(0.6399) Avg_AUC:0.7196672650541698 Avg_threshold:0.02028362825512886
dataset: companies layer_num_from_end:-1 Avg_acc:tensor(0.6425) Avg_AUC:0.6890291666666666 Avg_threshold:0.41365957260131836
dataset: facts layer_num_from_end:-1 Avg_acc:tensor(0.7222) Avg_AUC:0.7613631509248578 Avg_threshold:0.9857327938079834


================layer -5================
Epoch 0/100: Training Loss: 0.0025702521100744503
Epoch 1/100: Training Loss: 0.0017414459815392126
Epoch 2/100: Training Loss: 0.0013790174380882638
Epoch 3/100: Training Loss: 0.0012671660918455857
Epoch 4/100: Training Loss: 0.0025853122864569816
Epoch 5/100: Training Loss: 0.0024851352184802502
Epoch 6/100: Training Loss: 0.0003810645035513631
Epoch 7/100: Training Loss: 0.0017832542632843231
Epoch 8/100: Training Loss: 0.0015251845955015061
Epoch 9/100: Training Loss: 0.0020360308927255912
Epoch 10/100: Training Loss: 0.0022964654685734037
Epoch 11/100: Training Loss: 0.0015712629456620117
Epoch 12/100: Training Loss: 0.0005441769123911024
Epoch 13/100: Training Loss: 0.0014929571351804934
Epoch 14/100: Training Loss: 0.0011062324047088623
Epoch 15/100: Training Loss: 0.0016113121609587768
Epoch 16/100: Training Loss: 0.0009779452860772192
Epoch 17/100: Training Loss: 0.0012198189547011902
Epoch 18/100: Training Loss: 0.0006130054280474469
Epoch 19/100: Training Loss: 0.0007769852043031813
Epoch 20/100: Training Loss: 0.0004992510055328583
Epoch 21/100: Training Loss: 0.0008094871377611494
Epoch 22/100: Training Loss: 0.0004610757415111248
Epoch 23/100: Training Loss: 0.00043648257330581024
Epoch 24/100: Training Loss: 0.0003665891456437278
Epoch 25/100: Training Loss: 0.0014289490409664341
Epoch 26/100: Training Loss: 0.001260114508075314
Epoch 27/100: Training Loss: 0.00019949359389451833
Epoch 28/100: Training Loss: 0.0004895197657438425
Epoch 29/100: Training Loss: 0.00010171395211995065
Epoch 30/100: Training Loss: 0.0004162591445696104
Epoch 31/100: Training Loss: 0.0002776151457866589
Epoch 32/100: Training Loss: 0.00019907727316542938
Epoch 33/100: Training Loss: 0.0005760352303098131
Epoch 34/100: Training Loss: 0.000291593164413959
Epoch 35/100: Training Loss: 0.0016246334864543034
Epoch 36/100: Training Loss: 0.0004443446030983558
Epoch 37/100: Training Loss: 0.00021595776185289129
Epoch 38/100: Training Loss: 0.00027442374429502685
Epoch 39/100: Training Loss: 4.7969950975238024e-05
Epoch 40/100: Training Loss: 0.0004548392929397263
Epoch 41/100: Training Loss: 0.0009745550113958079
Epoch 42/100: Training Loss: 0.00013805060953527064
Epoch 43/100: Training Loss: 0.0007010261704038073
Epoch 44/100: Training Loss: 0.0006951226012690084
Epoch 45/100: Training Loss: 7.21582152939343e-05
Epoch 46/100: Training Loss: 0.000502195391621623
Epoch 47/100: Training Loss: 8.212644737083595e-05
Epoch 48/100: Training Loss: 0.00016150441203084025
Epoch 49/100: Training Loss: 0.00014192506775155767
Epoch 50/100: Training Loss: 0.000257556515556949
Epoch 51/100: Training Loss: 0.0003284855739220039
Epoch 52/100: Training Loss: 0.00027100101009115473
Epoch 53/100: Training Loss: 0.00025230101667917694
Epoch 54/100: Training Loss: 0.00025334029451950444
Epoch 55/100: Training Loss: 0.0002122051571007375
Epoch 56/100: Training Loss: 0.0005957823950093943
Epoch 57/100: Training Loss: 0.00025594937843042653
Epoch 58/100: Training Loss: 0.0002588541149259447
Epoch 59/100: Training Loss: 0.0014538277279246938
Epoch 60/100: Training Loss: 0.0003077142930531002
Epoch 61/100: Training Loss: 0.00023883469142280258
Epoch 62/100: Training Loss: 0.0009728284565718858
Epoch 63/100: Training Loss: 0.0006030455648482262
Epoch 64/100: Training Loss: 0.0005413055732533649
Epoch 65/100: Training Loss: 0.0010875173798807851
Epoch 66/100: Training Loss: 0.00012205610168980551
Epoch 67/100: Training Loss: 0.0002079247594713331
Epoch 68/100: Training Loss: 0.00011311250028910337
Epoch 69/100: Training Loss: 9.864032711390849e-05
Epoch 70/100: Training Loss: 6.440761857307874e-05
Epoch 71/100: Training Loss: 0.0001873715513652855
Epoch 72/100: Training Loss: 0.00010940864994809344
Epoch 73/100: Training Loss: 6.0214621985292104e-05
Epoch 74/100: Training Loss: 0.0021613986342103333
Epoch 75/100: Training Loss: 2.719232506424814e-05
Epoch 76/100: Training Loss: 0.00023506727668788885
Epoch 77/100: Training Loss: 0.00029269561588347377
Epoch 78/100: Training Loss: 4.060345317621331e-05
Epoch 79/100: Training Loss: 0.00032485130575153376
Epoch 80/100: Training Loss: 3.899152266916695e-05
Epoch 81/100: Training Loss: 0.0006874773677412446
Epoch 82/100: Training Loss: 8.368938900790848e-05
Epoch 83/100: Training Loss: 1.3858620468136314e-05
Epoch 84/100: Training Loss: 0.00018210504594799522
Epoch 85/100: Training Loss: 0.00041409805938080475
Epoch 86/100: Training Loss: 0.000165615152645778
Epoch 87/100: Training Loss: 0.0002091063407959638
Epoch 88/100: Training Loss: 0.0005712964005403586
Epoch 89/100: Training Loss: 3.741088264680409e-05
Epoch 90/100: Training Loss: 0.00011171146549961783
Epoch 91/100: Training Loss: 4.822517569352697e-05
Epoch 92/100: Training Loss: 0.0008044131881707198
Epoch 93/100: Training Loss: 0.000582418971128397
Epoch 94/100: Training Loss: 7.203032451492924e-05
Epoch 95/100: Training Loss: 2.6561324712592405e-05
Epoch 96/100: Training Loss: 2.8739601920743087e-05
Epoch 97/100: Training Loss: 5.497581690185137e-06
Epoch 98/100: Training Loss: 5.625719139417568e-05
Epoch 99/100: Training Loss: 0.00023740423205015544
Epoch 0/100: Training Loss: 0.00010785941211950211
Epoch 1/100: Training Loss: 8.109263366176969e-05
Epoch 2/100: Training Loss: 2.2865064619552522e-05
Epoch 3/100: Training Loss: 0.00011271024566321146
Epoch 4/100: Training Loss: 0.0001223567341055189
Epoch 5/100: Training Loss: 5.446170855845724e-05
Epoch 6/100: Training Loss: 3.2499560066277074e-06
Epoch 7/100: Training Loss: 8.1877534588178e-05
Epoch 8/100: Training Loss: 2.5502821829702173e-06
Epoch 9/100: Training Loss: 7.397888804830256e-06
Epoch 10/100: Training Loss: 2.7713055411974588e-06
Epoch 11/100: Training Loss: 6.310698864538045e-06
Epoch 12/100: Training Loss: 4.69156230489413e-05
Epoch 13/100: Training Loss: 7.958342653832266e-07
Epoch 14/100: Training Loss: 6.725156258436895e-06
Epoch 15/100: Training Loss: 3.864710812368208e-07
Epoch 16/100: Training Loss: 1.5383055489066812e-06
Epoch 17/100: Training Loss: 2.537484812949385e-06
Epoch 18/100: Training Loss: 9.310235674049528e-07
Epoch 19/100: Training Loss: 7.995646834994355e-07
Epoch 20/100: Training Loss: 2.279801022571822e-06
Epoch 21/100: Training Loss: 1.8892867956310511e-06
Epoch 22/100: Training Loss: 1.807264717561858e-06
Epoch 23/100: Training Loss: 2.263392144370647e-06
Epoch 24/100: Training Loss: 3.1446253636940606e-06
Epoch 25/100: Training Loss: 1.4615627670926707e-06
Epoch 26/100: Training Loss: 5.903757049790806e-07
Epoch 27/100: Training Loss: 2.328449183897603e-06
Epoch 28/100: Training Loss: 2.3047029528589476e-06
Epoch 29/100: Training Loss: 5.333146102549065e-06
Epoch 30/100: Training Loss: 1.9316924625032003e-06
Epoch 31/100: Training Loss: 0.00011206576157183875
Epoch 32/100: Training Loss: 4.684585811836379e-07
Epoch 33/100: Training Loss: 1.862407329359225e-06
Epoch 34/100: Training Loss: 9.289014823956503e-07
Epoch 35/100: Training Loss: 3.1160027165675447e-06
Epoch 36/100: Training Loss: 2.408996113531646e-06
Epoch 37/100: Training Loss: 3.7724376722638096e-06
Epoch 38/100: Training Loss: 1.4109072452854542e-05
Epoch 39/100: Training Loss: 1.0681370761068094e-05
Epoch 40/100: Training Loss: 2.4852448231762362e-06
Epoch 41/100: Training Loss: 1.4355485161234225e-06
Epoch 42/100: Training Loss: 4.89364666420789e-06
Epoch 43/100: Training Loss: 6.461427879652806e-06
Epoch 44/100: Training Loss: 5.560967561212324e-06
Epoch 45/100: Training Loss: 1.2647895519399927e-06
Epoch 46/100: Training Loss: 6.8575193706367695e-06
Epoch 47/100: Training Loss: 1.1329991615466065e-06
Epoch 48/100: Training Loss: 3.8699751409391565e-06
Epoch 49/100: Training Loss: 3.274238302505442e-06
Epoch 50/100: Training Loss: 1.469515042290801e-06
Epoch 51/100: Training Loss: 3.410066987964369e-06
Epoch 52/100: Training Loss: 2.5511652763400757e-06
Epoch 53/100: Training Loss: 3.0340055846387433e-06
Epoch 54/100: Training Loss: 2.3178549738423454e-06
Epoch 55/100: Training Loss: 1.1069782166963532e-05
Epoch 56/100: Training Loss: 2.6711842067362296e-06
Epoch 57/100: Training Loss: 1.7734449578537827e-06
Epoch 58/100: Training Loss: 2.872308028773183e-06
Epoch 59/100: Training Loss: 2.964756761988004e-05
Epoch 60/100: Training Loss: 3.114397016664346e-06
Epoch 61/100: Training Loss: 1.2559337275368827e-05
Epoch 62/100: Training Loss: 3.4429803712382203e-06
Epoch 63/100: Training Loss: 3.792679247756799e-06
Epoch 64/100: Training Loss: 2.947873491350384e-06
Epoch 65/100: Training Loss: 4.667774330647219e-06
Epoch 66/100: Training Loss: 3.5404510396931853e-06
Epoch 67/100: Training Loss: 2.8214300982654096e-06
Epoch 68/100: Training Loss: 8.247998387863238e-06
Epoch 69/100: Training Loss: 2.93819411169915e-06
Epoch 70/100: Training Loss: 7.142148180199521e-06
Epoch 71/100: Training Loss: 1.1530368854956967e-05
Epoch 72/100: Training Loss: 2.058812173172122e-06
Epoch 73/100: Training Loss: 8.761961085145317e-07
Epoch 74/100: Training Loss: 2.8837935089887606e-06
Epoch 75/100: Training Loss: 0.0003423943760849181
Epoch 76/100: Training Loss: 5.447481054857019e-07
Epoch 77/100: Training Loss: 1.1611738175685916e-06
Epoch 78/100: Training Loss: 2.143030897492454e-06
Epoch 79/100: Training Loss: 2.1627747974846335e-06
Epoch 80/100: Training Loss: 8.43828089446539e-06
Epoch 81/100: Training Loss: 2.301664928728271e-06
Epoch 82/100: Training Loss: 4.867662210017443e-06
Epoch 83/100: Training Loss: 8.714435915906159e-07
Epoch 84/100: Training Loss: 1.9851840555756575e-06
Epoch 85/100: Training Loss: 5.256510727728407e-07
Epoch 86/100: Training Loss: 9.983982419639471e-07
Epoch 87/100: Training Loss: 1.466869504102284e-06
Epoch 88/100: Training Loss: 3.320564948288458e-06
Epoch 89/100: Training Loss: 4.9693260474928785e-06
Epoch 90/100: Training Loss: 2.7172037378130922e-06
Epoch 91/100: Training Loss: 7.172470629614379e-07
Epoch 92/100: Training Loss: 1.7596361604297444e-06
Epoch 93/100: Training Loss: 3.4863425840047145e-06
Epoch 94/100: Training Loss: 7.2456563689879005e-06
Epoch 95/100: Training Loss: 1.5053134467009277e-06
Epoch 96/100: Training Loss: 1.0102080220046142e-06
Epoch 97/100: Training Loss: 3.147415728086517e-06
Epoch 98/100: Training Loss: 1.1483717354990187e-05
Epoch 99/100: Training Loss: 3.0828698072582485e-06
Epoch 0/100: Training Loss: 0.002298815930775887
Epoch 1/100: Training Loss: 0.0010998304084622317
Epoch 2/100: Training Loss: 0.0008973979549728137
Epoch 3/100: Training Loss: 0.0008289014978660383
Epoch 4/100: Training Loss: 0.0008101131704499682
Epoch 5/100: Training Loss: 0.001044134441897166
Epoch 6/100: Training Loss: 0.0007932946550474465
Epoch 7/100: Training Loss: 0.000408496573674593
Epoch 8/100: Training Loss: 0.0003107283755743818
Epoch 9/100: Training Loss: 0.00022359505045614083
Epoch 10/100: Training Loss: 7.958050897653154e-05
Epoch 11/100: Training Loss: 0.0002501744517891241
Epoch 12/100: Training Loss: 1.5031293981509814e-05
Epoch 13/100: Training Loss: 7.85450390774569e-05
Epoch 14/100: Training Loss: 0.00032831310368270326
Epoch 15/100: Training Loss: 2.6301943140921834e-05
Epoch 16/100: Training Loss: 0.0001454219418606884
Epoch 17/100: Training Loss: 0.00012751795548043377
Epoch 18/100: Training Loss: 2.169865183532238e-06
Epoch 19/100: Training Loss: 0.0003953906271955092
Epoch 20/100: Training Loss: 0.00022956176841859338
Epoch 21/100: Training Loss: 0.0006556752488481627
Epoch 22/100: Training Loss: 0.00021524177037840554
Epoch 23/100: Training Loss: 0.0002678840351905182
Epoch 24/100: Training Loss: 7.681141225554103e-05
Epoch 25/100: Training Loss: 0.00025453347024871864
Epoch 26/100: Training Loss: 0.00022583587063873986
Epoch 27/100: Training Loss: 1.7564540518988238e-05
Epoch 28/100: Training Loss: 1.6866592417875353e-05
Epoch 29/100: Training Loss: 5.589329639820458e-06
Epoch 30/100: Training Loss: 4.347447279117091e-06
Epoch 31/100: Training Loss: 3.16180686334626e-05
Epoch 32/100: Training Loss: 1.0294710203326292e-05
Epoch 33/100: Training Loss: 4.993024677467003e-06
Epoch 34/100: Training Loss: 7.658080718250011e-06
Epoch 35/100: Training Loss: 2.1932886721347445e-06
Epoch 36/100: Training Loss: 2.8010481204703557e-06
Epoch 37/100: Training Loss: 5.116806269513903e-06
Epoch 38/100: Training Loss: 3.5736899112733148e-06
Epoch 39/100: Training Loss: 1.5301479147278147e-05
Epoch 40/100: Training Loss: 1.4966647464522926e-05
Epoch 41/100: Training Loss: 3.453885423962018e-06
Epoch 42/100: Training Loss: 1.0009408354473342e-05
Epoch 43/100: Training Loss: 5.4454001010679226e-06
Epoch 44/100: Training Loss: 1.690148463030513e-05
Epoch 45/100: Training Loss: 2.0144589924269155e-05
Epoch 46/100: Training Loss: 2.993083820282984e-05
Epoch 47/100: Training Loss: 1.3776917492957424e-05
Epoch 48/100: Training Loss: 1.783011262293914e-05
Epoch 49/100: Training Loss: 1.4418070550731046e-05
Epoch 50/100: Training Loss: 2.080650706451645e-06
Epoch 51/100: Training Loss: 6.410770109302992e-06
Epoch 52/100: Training Loss: 3.127880367658121e-06
Epoch 53/100: Training Loss: 1.0560102519585932e-05
Epoch 54/100: Training Loss: 8.959246339271967e-05
Epoch 55/100: Training Loss: 1.7336236951734236e-05
Epoch 56/100: Training Loss: 5.229376703143406e-05
Epoch 57/100: Training Loss: 3.2629707171548185e-06
Epoch 58/100: Training Loss: 8.40082387183067e-06
Epoch 59/100: Training Loss: 1.0912408225899405e-05
Epoch 60/100: Training Loss: 4.966533352002251e-06
Epoch 61/100: Training Loss: 8.099455033525003e-06
Epoch 62/100: Training Loss: 1.2923479839551935e-05
Epoch 63/100: Training Loss: 4.77879870977762e-06
Epoch 64/100: Training Loss: 1.1438261120126759e-06
Epoch 65/100: Training Loss: 4.568737116363123e-05
Epoch 66/100: Training Loss: 1.0013914213692256e-05
Epoch 67/100: Training Loss: 4.905119663376888e-06
Epoch 68/100: Training Loss: 9.348148667662265e-06
Epoch 69/100: Training Loss: 8.592791341656118e-06
Epoch 70/100: Training Loss: 1.5786615573316456e-05
Epoch 71/100: Training Loss: 1.1010235063821006e-05
Epoch 72/100: Training Loss: 1.5157195337438325e-06
Epoch 73/100: Training Loss: 3.627401819046167e-05
Epoch 74/100: Training Loss: 4.2565627028044466e-05
Epoch 75/100: Training Loss: 6.160291796215147e-06
Epoch 76/100: Training Loss: 1.926334659687335e-05
Epoch 77/100: Training Loss: 5.577207929331907e-06
Epoch 78/100: Training Loss: 1.9668483083768428e-05
Epoch 79/100: Training Loss: 1.0124041433099934e-05
Epoch 80/100: Training Loss: 2.563205643666448e-05
Epoch 81/100: Training Loss: 5.703872266254551e-06
Epoch 82/100: Training Loss: 1.1685325560881365e-05
Epoch 83/100: Training Loss: 1.1154160135798604e-05
Epoch 84/100: Training Loss: 2.0567457274281437e-05
Epoch 85/100: Training Loss: 3.9032466119999506e-05
Epoch 86/100: Training Loss: 8.909366756892033e-06
Epoch 87/100: Training Loss: 3.789770428082354e-06
Epoch 88/100: Training Loss: 7.515294673881657e-06
Epoch 89/100: Training Loss: 1.1334635710401787e-05
Epoch 90/100: Training Loss: 2.347109102302318e-05
Epoch 91/100: Training Loss: 9.467788442647714e-06
Epoch 92/100: Training Loss: 8.34651458874452e-06
Epoch 93/100: Training Loss: 4.5240082838338056e-06
Epoch 94/100: Training Loss: 1.9986148950436134e-05
Epoch 95/100: Training Loss: 6.229795930947331e-06
Epoch 96/100: Training Loss: 6.115656319282038e-05
Epoch 97/100: Training Loss: 9.432056476529554e-06
Epoch 98/100: Training Loss: 4.0306679974812613e-07
Epoch 99/100: Training Loss: 7.538325923809902e-06
Epoch 0/100: Training Loss: 0.0007576822946612962
Epoch 1/100: Training Loss: 0.00042894129879808655
Epoch 2/100: Training Loss: 0.0007404723173178336
Epoch 3/100: Training Loss: 0.00045677051308074435
Epoch 4/100: Training Loss: 0.0004365658702481772
Epoch 5/100: Training Loss: 0.00011382126901748676
Epoch 6/100: Training Loss: 0.00020176535787213827
Epoch 7/100: Training Loss: 0.00028850813058839327
Epoch 8/100: Training Loss: 0.00048463795208124723
Epoch 9/100: Training Loss: 0.0006341556995963129
Epoch 10/100: Training Loss: 0.0003610995512653664
Epoch 11/100: Training Loss: 0.00024885050772468825
Epoch 12/100: Training Loss: 8.217927410406766e-05
Epoch 13/100: Training Loss: 0.00023417279196246235
Epoch 14/100: Training Loss: 0.00013542349859712205
Epoch 15/100: Training Loss: 0.00010769708972910176
Epoch 16/100: Training Loss: 5.0279257846051365e-05
Epoch 17/100: Training Loss: 0.0002259026474998769
Epoch 18/100: Training Loss: 6.136110115454393e-05
Epoch 19/100: Training Loss: 8.686666111439323e-05
Epoch 20/100: Training Loss: 3.3801090836093046e-05
Epoch 21/100: Training Loss: 0.00017462404453812014
Epoch 22/100: Training Loss: 0.0001702563126306027
Epoch 23/100: Training Loss: 5.7138156163807655e-05
Epoch 24/100: Training Loss: 6.224117399269832e-05
Epoch 25/100: Training Loss: 4.397154052332403e-05
Epoch 26/100: Training Loss: 8.100027371431894e-05
Epoch 27/100: Training Loss: 6.613117791172387e-05
Epoch 28/100: Training Loss: 8.675355266257761e-05
Epoch 29/100: Training Loss: 6.517796250789061e-05
Epoch 30/100: Training Loss: 3.672565293052922e-05
Epoch 31/100: Training Loss: 3.71627671563107e-05
Epoch 32/100: Training Loss: 1.8724641911145568e-05
Epoch 33/100: Training Loss: 3.461658972616933e-05
Epoch 34/100: Training Loss: 1.3219898989523091e-05
Epoch 35/100: Training Loss: 0.00011008917601500156
Epoch 36/100: Training Loss: 3.334981330854881e-05
Epoch 37/100: Training Loss: 4.505996866790569e-05
Epoch 38/100: Training Loss: 2.8474578085009026e-05
Epoch 39/100: Training Loss: 3.0447796419046927e-05
Epoch 40/100: Training Loss: 4.467871130088677e-05
Epoch 41/100: Training Loss: 8.236801314757065e-06
Epoch 42/100: Training Loss: 9.361058383604179e-06
Epoch 43/100: Training Loss: 3.375095013835004e-05
Epoch 44/100: Training Loss: 4.2825001449400676e-05
Epoch 45/100: Training Loss: 1.2442583200220325e-05
Epoch 46/100: Training Loss: 3.512322920676015e-05
Epoch 47/100: Training Loss: 1.0835349892720509e-05
Epoch 48/100: Training Loss: 3.963774115135128e-05
Epoch 49/100: Training Loss: 5.248058047415554e-05
Epoch 50/100: Training Loss: 1.0756843223951865e-05
Epoch 51/100: Training Loss: 4.0198104896983085e-05
Epoch 52/100: Training Loss: 7.114187332887004e-05
Epoch 53/100: Training Loss: 1.825524858482506e-05
Epoch 54/100: Training Loss: 0.00027240229689556617
Epoch 55/100: Training Loss: 9.626568551512731e-06
Epoch 56/100: Training Loss: 2.5536880742956476e-05
Epoch 57/100: Training Loss: 1.2306308661754005e-05
Epoch 58/100: Training Loss: 5.983000234273321e-05
Epoch 59/100: Training Loss: 2.8387209229567202e-05
Epoch 60/100: Training Loss: 1.1733142605078394e-05
Epoch 61/100: Training Loss: 3.365145609286672e-05
Epoch 62/100: Training Loss: 6.885300975310918e-06
Epoch 63/100: Training Loss: 3.0219892336838488e-05
Epoch 64/100: Training Loss: 6.327052839136354e-05
Epoch 65/100: Training Loss: 3.018959510873481e-05
Epoch 66/100: Training Loss: 2.6452303346660403e-05
Epoch 67/100: Training Loss: 3.27378511428833e-05
Epoch 68/100: Training Loss: 2.5671081176558553e-05
Epoch 69/100: Training Loss: 2.9393797965297376e-05
Epoch 70/100: Training Loss: 3.2547834342804507e-05
Epoch 71/100: Training Loss: 6.951129059464747e-06
Epoch 72/100: Training Loss: 5.234072911710555e-05
Epoch 73/100: Training Loss: 1.1331480500346797e-05
Epoch 74/100: Training Loss: 3.41743554757989e-05
Epoch 75/100: Training Loss: 2.6444810024206188e-05
Epoch 76/100: Training Loss: 2.44969353619693e-05
Epoch 77/100: Training Loss: 3.073557678178169e-06
Epoch 78/100: Training Loss: 4.142011709259328e-05
Epoch 79/100: Training Loss: 3.138425251567997e-05
Epoch 80/100: Training Loss: 3.712523080732512e-05
Epoch 81/100: Training Loss: 5.387688046636213e-05
Epoch 82/100: Training Loss: 5.0428678879991244e-05
Epoch 83/100: Training Loss: 3.249373214976223e-05
Epoch 84/100: Training Loss: 3.2155569822747926e-05
Epoch 85/100: Training Loss: 3.0946868341326135e-05
Epoch 86/100: Training Loss: 3.770129599918922e-06
Epoch 87/100: Training Loss: 4.90597966644499e-05
Epoch 88/100: Training Loss: 3.785456886182085e-05
Epoch 89/100: Training Loss: 6.048021847067248e-05
Epoch 90/100: Training Loss: 3.197993903200408e-05
Epoch 91/100: Training Loss: 6.163863415699362e-06
Epoch 92/100: Training Loss: 3.3568617874297544e-05
Epoch 93/100: Training Loss: 3.1997373210635165e-05
Epoch 94/100: Training Loss: 1.768387881094131e-05
Epoch 95/100: Training Loss: 1.8986895824400124e-05
Epoch 96/100: Training Loss: 4.495603846755005e-05
Epoch 97/100: Training Loss: 9.63751136680732e-05
Epoch 98/100: Training Loss: 3.079774874995872e-05
Epoch 99/100: Training Loss: 2.666166871066255e-05
Epoch 0/100: Training Loss: 0.0012516712441163905
Epoch 1/100: Training Loss: 0.0010251594963026983
Epoch 2/100: Training Loss: 0.0008940329300422294
Epoch 3/100: Training Loss: 0.0013252596060434978
Epoch 4/100: Training Loss: 0.0012656567143458947
Epoch 5/100: Training Loss: 0.001091853222426246
Epoch 6/100: Training Loss: 0.0009762101313647102
Epoch 7/100: Training Loss: 0.0007170954463528652
Epoch 8/100: Training Loss: 0.0006256558585400675
Epoch 9/100: Training Loss: 0.0007996215831999685
Epoch 10/100: Training Loss: 0.00054153705052301
Epoch 11/100: Training Loss: 0.000680370219782287
Epoch 12/100: Training Loss: 0.0006984132615958943
Epoch 13/100: Training Loss: 0.0005528576233807732
Epoch 14/100: Training Loss: 0.00041739346788210026
Epoch 15/100: Training Loss: 0.0003436190240523394
Epoch 16/100: Training Loss: 0.00031969126533059515
Epoch 17/100: Training Loss: 0.0002455476890592014
Epoch 18/100: Training Loss: 0.000282845350311083
Epoch 19/100: Training Loss: 5.179687001395459e-05
Epoch 20/100: Training Loss: 0.00022143351973271836
Epoch 21/100: Training Loss: 0.00014231268170417525
Epoch 22/100: Training Loss: 0.0002696002450059442
Epoch 23/100: Training Loss: 0.00017226600617754694
Epoch 24/100: Training Loss: 0.00017169386367587482
Epoch 25/100: Training Loss: 2.4602296488250003e-05
Epoch 26/100: Training Loss: 0.00011659152440580667
Epoch 27/100: Training Loss: 0.0004979216438882492
Epoch 28/100: Training Loss: 0.0002175663579620567
Epoch 29/100: Training Loss: 0.0002572795412704056
Epoch 30/100: Training Loss: 0.0002611166268002753
Epoch 31/100: Training Loss: 0.00019291751817160962
Epoch 32/100: Training Loss: 0.00011082351499912785
Epoch 33/100: Training Loss: 0.0003896039344516455
Epoch 34/100: Training Loss: 0.0002212437308009933
Epoch 35/100: Training Loss: 0.00018082873201837727
Epoch 36/100: Training Loss: 0.00023253906664310716
Epoch 37/100: Training Loss: 4.1454491735089056e-05
Epoch 38/100: Training Loss: 0.0001431893301652927
Epoch 39/100: Training Loss: 0.00028975419334921184
Epoch 40/100: Training Loss: 0.00012384670987433078
Epoch 41/100: Training Loss: 1.1045638672715309e-05
Epoch 42/100: Training Loss: 3.716176348354886e-05
Epoch 43/100: Training Loss: 0.00011422888686259587
Epoch 44/100: Training Loss: 2.3490667124004924e-05
Epoch 45/100: Training Loss: 7.396871589270293e-05
Epoch 46/100: Training Loss: 5.951418778767773e-05
Epoch 47/100: Training Loss: 7.378659211099148e-05
Epoch 48/100: Training Loss: 0.0001540722909803484
Epoch 49/100: Training Loss: 0.00023034790202098733
Epoch 50/100: Training Loss: 3.7109022777454524e-05
Epoch 51/100: Training Loss: 0.00010241367215035008
Epoch 52/100: Training Loss: 0.00010212354690713041
Epoch 53/100: Training Loss: 6.849374439494283e-05
Epoch 54/100: Training Loss: 1.8189849523717866e-05
Epoch 55/100: Training Loss: 8.94068210732703e-05
Epoch 56/100: Training Loss: 3.2605968561826967e-06
Epoch 57/100: Training Loss: 0.00034606938852983364
Epoch 58/100: Training Loss: 3.649092361550121e-05
Epoch 59/100: Training Loss: 0.00021638827142762203
Epoch 60/100: Training Loss: 0.0001242911333546919
Epoch 61/100: Training Loss: 6.451065122496849e-05
Epoch 62/100: Training Loss: 4.041122783925019e-06
Epoch 63/100: Training Loss: 1.706914547100371e-05
Epoch 64/100: Training Loss: 0.0010972838921874178
Epoch 65/100: Training Loss: 8.86118185578608e-05
Epoch 66/100: Training Loss: 9.054145501816973e-05
Epoch 67/100: Training Loss: 4.404297500264411e-05
Epoch 68/100: Training Loss: 8.034316695057878e-06
Epoch 69/100: Training Loss: 1.6401193159468032e-05
Epoch 70/100: Training Loss: 4.8690445848978035e-06
Epoch 71/100: Training Loss: 0.00018284177663279515
Epoch 72/100: Training Loss: 5.8463674184738424e-05
Epoch 73/100: Training Loss: 8.498511149310598e-05
Epoch 74/100: Training Loss: 3.726794338766851e-05
Epoch 75/100: Training Loss: 1.0295869216468988e-05
Epoch 76/100: Training Loss: 1.7295979869588479e-06
Epoch 77/100: Training Loss: 5.4714032540134356e-05
Epoch 78/100: Training Loss: 8.064912011226018e-05
Epoch 79/100: Training Loss: 4.3178383516622525e-05
Epoch 80/100: Training Loss: 8.488773578303117e-06
Epoch 81/100: Training Loss: 5.7106230444475715e-05
Epoch 82/100: Training Loss: 4.764895100950026e-06
Epoch 83/100: Training Loss: 2.340375110233093e-06
Epoch 84/100: Training Loss: 7.83481507324705e-05
Epoch 85/100: Training Loss: 2.3966171212640462e-06
Epoch 86/100: Training Loss: 5.919900357577146e-05
Epoch 87/100: Training Loss: 5.49218313216104e-07
Epoch 88/100: Training Loss: 0.00014496368228220473
Epoch 89/100: Training Loss: 7.974371021869137e-05
Epoch 90/100: Training Loss: 2.9304461991962264e-05
Epoch 91/100: Training Loss: 7.2884551413795525e-06
Epoch 92/100: Training Loss: 6.856097053398616e-07
Epoch 93/100: Training Loss: 0.00022184437907793943
Epoch 94/100: Training Loss: 0.0002082141627575837
Epoch 95/100: Training Loss: 7.087854421971476e-06
Epoch 96/100: Training Loss: 3.536980526120055e-05
Epoch 97/100: Training Loss: 3.332523263844789e-05
Epoch 98/100: Training Loss: 5.283658666645779e-05
Epoch 99/100: Training Loss: 6.548586009325935e-05
Epoch 0/100: Training Loss: 0.0003623971475650332
Epoch 1/100: Training Loss: 7.786885322675771e-05
Epoch 2/100: Training Loss: 7.314784604994977e-05
Epoch 3/100: Training Loss: 0.0003053312837658777
Epoch 4/100: Training Loss: 7.963986670384642e-05
Epoch 5/100: Training Loss: 0.00015760344001671748
Epoch 6/100: Training Loss: 3.0705181825077227e-06
Epoch 7/100: Training Loss: 4.414311093245513e-05
Epoch 8/100: Training Loss: 1.3956382425733137e-05
Epoch 9/100: Training Loss: 0.00027656900659378016
Epoch 10/100: Training Loss: 8.489046708203032e-05
Epoch 11/100: Training Loss: 0.0001374958846412721
Epoch 12/100: Training Loss: 4.139538217484253e-05
Epoch 13/100: Training Loss: 7.554715983482379e-05
Epoch 14/100: Training Loss: 8.32849803797255e-05
Epoch 15/100: Training Loss: 2.8878672864174677e-05
Epoch 16/100: Training Loss: 7.030328155541029e-05
Epoch 17/100: Training Loss: 0.00013673617842605018
Epoch 18/100: Training Loss: 0.0001577468680553749
Epoch 19/100: Training Loss: 2.426916757376049e-06
Epoch 20/100: Training Loss: 3.175741929639419e-05
Epoch 21/100: Training Loss: 1.9569294548983875e-05
Epoch 22/100: Training Loss: 1.0416838634907502e-05
Epoch 23/100: Training Loss: 8.052041038731223e-06
Epoch 24/100: Training Loss: 3.6790077105618192e-06
Epoch 25/100: Training Loss: 2.2774662569479305e-05
Epoch 26/100: Training Loss: 1.69136892978006e-05
Epoch 27/100: Training Loss: 1.0781949527771049e-06
Epoch 28/100: Training Loss: 6.468316640451865e-06
Epoch 29/100: Training Loss: 4.472844826114262e-05
Epoch 30/100: Training Loss: 8.11886397873071e-06
Epoch 31/100: Training Loss: 1.2766944999158801e-05
Epoch 32/100: Training Loss: 1.399342485417806e-05
Epoch 33/100: Training Loss: 2.812277918258372e-06
Epoch 34/100: Training Loss: 5.804801106767018e-06
Epoch 35/100: Training Loss: 1.6190225053447751e-06
Epoch 36/100: Training Loss: 4.3077826766031127e-07
Epoch 37/100: Training Loss: 3.6642288263512023e-07
Epoch 38/100: Training Loss: 6.123531808252217e-07
Epoch 39/100: Training Loss: 8.20860110738029e-07
Epoch 40/100: Training Loss: 2.627103774594601e-07
Epoch 41/100: Training Loss: 1.4126916661088637e-07
Epoch 42/100: Training Loss: 3.519415458995823e-07
Epoch 43/100: Training Loss: 6.344187917488799e-07
Epoch 44/100: Training Loss: 1.8440026122492147e-06
Epoch 45/100: Training Loss: 1.0014542861595156e-07
Epoch 46/100: Training Loss: 3.419680448916394e-06
Epoch 47/100: Training Loss: 2.662268418104953e-08
Epoch 48/100: Training Loss: 7.975953832342054e-09
Epoch 49/100: Training Loss: 1.8272962638196607e-09
Epoch 50/100: Training Loss: 8.094870797089907e-09
Epoch 51/100: Training Loss: 5.237963590421986e-07
Epoch 52/100: Training Loss: 6.129838683286931e-06
Epoch 53/100: Training Loss: 1.3251163147769397e-07
Epoch 54/100: Training Loss: 3.842472407391046e-07
Epoch 55/100: Training Loss: 6.081797860016372e-10
Epoch 56/100: Training Loss: 1.2382811898939786e-08
Epoch 57/100: Training Loss: 2.7670816120863903e-08
Epoch 58/100: Training Loss: 1.2316974440113935e-09
Epoch 59/100: Training Loss: 2.514541983949042e-09
Epoch 60/100: Training Loss: 2.9813007352968016e-09
Epoch 61/100: Training Loss: 3.2141697822047063e-07
Epoch 62/100: Training Loss: 3.380884463567313e-08
Epoch 63/100: Training Loss: 2.7578445964583372e-08
Epoch 64/100: Training Loss: 1.0100929096528707e-09
Epoch 65/100: Training Loss: 5.792979111239822e-09
Epoch 66/100: Training Loss: 3.52212136548091e-07
Epoch 67/100: Training Loss: 1.1472296944328102e-08
Epoch 68/100: Training Loss: 1.6359766112069056e-06
Epoch 69/100: Training Loss: 5.427414872541814e-07
Epoch 70/100: Training Loss: 8.745666972821714e-07
Epoch 71/100: Training Loss: 1.2496577332339377e-07
Epoch 72/100: Training Loss: 5.402751250598204e-07
Epoch 73/100: Training Loss: 6.979514525527684e-10
Epoch 74/100: Training Loss: 3.819509689637244e-07
Epoch 75/100: Training Loss: 1.7856235999779763e-06
Epoch 76/100: Training Loss: 1.0653090784849656e-07
Epoch 77/100: Training Loss: 1.67526860663781e-09
Epoch 78/100: Training Loss: 2.094101167256825e-09
Epoch 79/100: Training Loss: 1.5354933021142991e-09
Epoch 80/100: Training Loss: 3.252657212423863e-08
Epoch 81/100: Training Loss: 1.158968352981782e-08
Epoch 82/100: Training Loss: 3.9174529397751114e-07
Epoch 83/100: Training Loss: 1.3432697438257606e-07
Epoch 84/100: Training Loss: 6.57558109092175e-08
Epoch 85/100: Training Loss: 3.280605111324034e-08
Epoch 86/100: Training Loss: 5.091110333206665e-13
Epoch 87/100: Training Loss: 8.021890370287909e-10
Epoch 88/100: Training Loss: 4.1877177676643446e-09
Epoch 89/100: Training Loss: 4.6634311439901313e-07
Epoch 90/100: Training Loss: 3.7689503040753605e-09
Epoch 91/100: Training Loss: 3.071372340740287e-08
Epoch 92/100: Training Loss: 1.912452018450601e-08
Epoch 93/100: Training Loss: 1.9543345689618702e-08
Epoch 94/100: Training Loss: 4.466909743993777e-09
Epoch 95/100: Training Loss: 6.002419553330415e-09
Epoch 96/100: Training Loss: 1.7081157373050686e-07
Epoch 97/100: Training Loss: 5.702177524475045e-07
Epoch 98/100: Training Loss: 3.613960308696752e-07
Epoch 99/100: Training Loss: 3.462071242958713e-08
dataset: cities layer_num_from_end:-5 Avg_acc:tensor(0.7395) Avg_AUC:0.8473069266840038 Avg_threshold:0.6414967179298401
dataset: inventions layer_num_from_end:-5 Avg_acc:tensor(0.6438) Avg_AUC:0.8742310428523602 Avg_threshold:0.7029803991317749
dataset: elements layer_num_from_end:-5 Avg_acc:tensor(0.6183) Avg_AUC:0.674672216441207 Avg_threshold:0.8404088616371155
dataset: animals layer_num_from_end:-5 Avg_acc:tensor(0.6558) Avg_AUC:0.761170556185437 Avg_threshold:0.7987504005432129
dataset: companies layer_num_from_end:-5 Avg_acc:tensor(0.6942) Avg_AUC:0.78725 Avg_threshold:0.9349569082260132
dataset: facts layer_num_from_end:-5 Avg_acc:tensor(0.7353) Avg_AUC:0.8212493058225468 Avg_threshold:0.9560137987136841


================layer -9================
Epoch 0/100: Training Loss: 0.0015067641551677997
Epoch 1/100: Training Loss: 0.0007623438234929438
Epoch 2/100: Training Loss: 0.0011874375851837905
Epoch 3/100: Training Loss: 0.00033762516987907303
Epoch 4/100: Training Loss: 0.000796196448219406
Epoch 5/100: Training Loss: 0.0003847128653026127
Epoch 6/100: Training Loss: 0.0004058923910964619
Epoch 7/100: Training Loss: 0.00026126656603146266
Epoch 8/100: Training Loss: 0.0002915415834713649
Epoch 9/100: Training Loss: 0.0011203243182255672
Epoch 10/100: Training Loss: 0.0007178389525913692
Epoch 11/100: Training Loss: 0.0007900221051869693
Epoch 12/100: Training Loss: 0.0007122671166500011
Epoch 13/100: Training Loss: 0.001419098956601603
Epoch 14/100: Training Loss: 0.0006042575606933007
Epoch 15/100: Training Loss: 0.0010111978004028748
Epoch 16/100: Training Loss: 0.000580516311672184
Epoch 17/100: Training Loss: 0.0006296934260355009
Epoch 18/100: Training Loss: 0.0012734585500263668
Epoch 19/100: Training Loss: 0.0016536510490870976
Epoch 20/100: Training Loss: 0.0006666603413495151
Epoch 21/100: Training Loss: 0.0006383706431288819
Epoch 22/100: Training Loss: 0.000991437506008815
Epoch 23/100: Training Loss: 0.00024875675464843536
Epoch 24/100: Training Loss: 0.0007252096087782533
Epoch 25/100: Training Loss: 0.000942555966077151
Epoch 26/100: Training Loss: 0.0011720461445254879
Epoch 27/100: Training Loss: 0.00047785981551750554
Epoch 28/100: Training Loss: 0.000409955276059104
Epoch 29/100: Training Loss: 0.00045046892824706497
Epoch 30/100: Training Loss: 0.0007491686961033961
Epoch 31/100: Training Loss: 0.0003055214517183237
Epoch 32/100: Training Loss: 0.00041308045595675913
Epoch 33/100: Training Loss: 0.0009932281462462632
Epoch 34/100: Training Loss: 0.0010492096205691357
Epoch 35/100: Training Loss: 0.0008021840578192597
Epoch 36/100: Training Loss: 0.0002107254353228149
Epoch 37/100: Training Loss: 0.0007223168557340449
Epoch 38/100: Training Loss: 0.0005318901234573417
Epoch 39/100: Training Loss: 0.00022590829880087527
Epoch 40/100: Training Loss: 0.00048579879990824453
Epoch 41/100: Training Loss: 0.000947869532591813
Epoch 42/100: Training Loss: 0.0004939999613728556
Epoch 43/100: Training Loss: 0.0006885176355188543
Epoch 44/100: Training Loss: 0.0007020553925654271
Epoch 45/100: Training Loss: 0.0003653568821353512
Epoch 46/100: Training Loss: 0.0004798435977288893
Epoch 47/100: Training Loss: 0.000613271669074372
Epoch 48/100: Training Loss: 0.0004913086866165375
Epoch 49/100: Training Loss: 0.0005295701793857387
Epoch 50/100: Training Loss: 0.00039101264380908513
Epoch 51/100: Training Loss: 0.0008862439569059785
Epoch 52/100: Training Loss: 0.000657731836492365
Epoch 53/100: Training Loss: 0.000548288330331549
Epoch 54/100: Training Loss: 0.0008804342755070933
Epoch 55/100: Training Loss: 0.00036370999121165777
Epoch 56/100: Training Loss: 0.0006826583530519392
Epoch 57/100: Training Loss: 0.0003838831155033378
Epoch 58/100: Training Loss: 0.0007823093475161733
Epoch 59/100: Training Loss: 0.0005472156551334408
Epoch 60/100: Training Loss: 0.0003092419225852806
Epoch 61/100: Training Loss: 0.00040385762711504954
Epoch 62/100: Training Loss: 0.0002975722292920093
Epoch 63/100: Training Loss: 0.0003207382987012396
Epoch 64/100: Training Loss: 0.0008121386066183343
Epoch 65/100: Training Loss: 0.0005578611801554274
Epoch 66/100: Training Loss: 0.0004780369100870786
Epoch 67/100: Training Loss: 0.000425051194387716
Epoch 68/100: Training Loss: 0.0004852127570372361
Epoch 69/100: Training Loss: 0.00027400124948341527
Epoch 70/100: Training Loss: 0.00026999875069498184
Epoch 71/100: Training Loss: 0.00046463988044045186
Epoch 72/100: Training Loss: 0.0002092687035357202
Epoch 73/100: Training Loss: 0.0002484364838866921
Epoch 74/100: Training Loss: 0.00020139224149964073
Epoch 75/100: Training Loss: 0.0003452651150576718
Epoch 76/100: Training Loss: 0.0006818966223643377
Epoch 77/100: Training Loss: 0.000577125151257415
Epoch 78/100: Training Loss: 9.529195543560949e-05
Epoch 79/100: Training Loss: 0.0007283349970837573
Epoch 80/100: Training Loss: 0.0002497931408298599
Epoch 81/100: Training Loss: 0.0004529445008798079
Epoch 82/100: Training Loss: 0.0007419195416924003
Epoch 83/100: Training Loss: 0.000198060685729647
Epoch 84/100: Training Loss: 0.00026101934222074656
Epoch 85/100: Training Loss: 0.0002465150937750623
Epoch 86/100: Training Loss: 0.0003527729036091091
Epoch 87/100: Training Loss: 0.0005258628642642414
Epoch 88/100: Training Loss: 0.0002532117850296981
Epoch 89/100: Training Loss: 0.0005278270040358696
Epoch 90/100: Training Loss: 0.0019250439180360807
Epoch 91/100: Training Loss: 0.0004410826034479208
Epoch 92/100: Training Loss: 0.00023399447123487513
Epoch 93/100: Training Loss: 0.0004710702004132571
Epoch 94/100: Training Loss: 0.00035293342975469737
Epoch 95/100: Training Loss: 0.00035135085870335983
Epoch 96/100: Training Loss: 0.0002810988132353429
Epoch 97/100: Training Loss: 0.00019469564142343882
Epoch 98/100: Training Loss: 0.001239125336800422
Epoch 99/100: Training Loss: 0.0001725680813505933
Epoch 0/100: Training Loss: 0.001548698686418079
Epoch 1/100: Training Loss: 0.0010883748531341554
Epoch 2/100: Training Loss: 0.0012435744206110637
Epoch 3/100: Training Loss: 0.0012113845064526513
Epoch 4/100: Training Loss: 0.0010304662443342662
Epoch 5/100: Training Loss: 0.0009828832887467883
Epoch 6/100: Training Loss: 0.001187502628281003
Epoch 7/100: Training Loss: 0.001056466641880217
Epoch 8/100: Training Loss: 0.001398309071858724
Epoch 9/100: Training Loss: 0.0006926750852948143
Epoch 10/100: Training Loss: 0.0005922919227963402
Epoch 11/100: Training Loss: 0.0006838368518011911
Epoch 12/100: Training Loss: 0.000658329895564488
Epoch 13/100: Training Loss: 0.0004520150167601449
Epoch 14/100: Training Loss: 0.0003185849814187913
Epoch 15/100: Training Loss: 0.00028161250409625827
Epoch 16/100: Training Loss: 0.00033298727302324206
Epoch 17/100: Training Loss: 0.0002965590073948815
Epoch 18/100: Training Loss: 0.0004485117182845161
Epoch 19/100: Training Loss: 0.00046899602526710146
Epoch 20/100: Training Loss: 0.0001781385037161055
Epoch 21/100: Training Loss: 0.0002138557710817882
Epoch 22/100: Training Loss: 0.00035774966790562586
Epoch 23/100: Training Loss: 0.00026113706685247873
Epoch 24/100: Training Loss: 0.00018783268474397205
Epoch 25/100: Training Loss: 0.00018219458205359323
Epoch 26/100: Training Loss: 0.0001788990128607977
Epoch 27/100: Training Loss: 0.00033495937074933733
Epoch 28/100: Training Loss: 0.0001829535833426884
Epoch 29/100: Training Loss: 0.00014335505132164275
Epoch 30/100: Training Loss: 0.00014339740432444073
Epoch 31/100: Training Loss: 0.00034499785729816984
Epoch 32/100: Training Loss: 0.00016742660885765438
Epoch 33/100: Training Loss: 0.00014825355084169479
Epoch 34/100: Training Loss: 0.00017203112088498616
Epoch 35/100: Training Loss: 0.00015044398605823517
Epoch 36/100: Training Loss: 0.00018481887167408353
Epoch 37/100: Training Loss: 0.00011212600483780815
Epoch 38/100: Training Loss: 0.0006025412252971105
Epoch 39/100: Training Loss: 0.00030676211629595076
Epoch 40/100: Training Loss: 8.024856270778747e-05
Epoch 41/100: Training Loss: 0.00032885819673538207
Epoch 42/100: Training Loss: 0.00014180422184013186
Epoch 43/100: Training Loss: 0.00015011051935809
Epoch 44/100: Training Loss: 0.00014680829786119007
Epoch 45/100: Training Loss: 0.00016689563081378027
Epoch 46/100: Training Loss: 0.0003941069756235395
Epoch 47/100: Training Loss: 0.00018726711471875508
Epoch 48/100: Training Loss: 0.00012531756822552
Epoch 49/100: Training Loss: 0.00017056011018298922
Epoch 50/100: Training Loss: 0.00014151808406625475
Epoch 51/100: Training Loss: 0.0004966031227793012
Epoch 52/100: Training Loss: 0.0002768516008343015
Epoch 53/100: Training Loss: 0.0001190872419448126
Epoch 54/100: Training Loss: 0.00018160417675971985
Epoch 55/100: Training Loss: 0.00013111141465959095
Epoch 56/100: Training Loss: 0.0002956563872950418
Epoch 57/100: Training Loss: 0.00011601307917208899
Epoch 58/100: Training Loss: 0.00010973597388891947
Epoch 59/100: Training Loss: 0.00012470237201168423
Epoch 60/100: Training Loss: 0.00015687729631151472
Epoch 61/100: Training Loss: 0.0001087999769619533
Epoch 62/100: Training Loss: 8.52110307841074e-05
Epoch 63/100: Training Loss: 7.71836865515936e-05
Epoch 64/100: Training Loss: 0.0001334826417622112
Epoch 65/100: Training Loss: 0.0001778434075060345
Epoch 66/100: Training Loss: 0.0003113459263529096
Epoch 67/100: Training Loss: 0.0001494397364911579
Epoch 68/100: Training Loss: 0.0002595470419951848
Epoch 69/100: Training Loss: 8.273262175775709e-05
Epoch 70/100: Training Loss: 8.452579024292174e-05
Epoch 71/100: Training Loss: 0.00018143430352211
Epoch 72/100: Training Loss: 0.0003024439016977946
Epoch 73/100: Training Loss: 1.6388024336525372e-05
Epoch 74/100: Training Loss: 4.665474629118329e-05
Epoch 75/100: Training Loss: 9.66616329692659e-05
Epoch 76/100: Training Loss: 0.00010965967639571144
Epoch 77/100: Training Loss: 0.00026569440960884095
Epoch 78/100: Training Loss: 8.805568019549052e-05
Epoch 79/100: Training Loss: 0.0008166477084159852
Epoch 80/100: Training Loss: 2.8231032636194e-05
Epoch 81/100: Training Loss: 0.00018574827838511694
Epoch 82/100: Training Loss: 4.002196004702932e-05
Epoch 83/100: Training Loss: 0.0010798072531109765
Epoch 84/100: Training Loss: 7.642572302193868e-05
Epoch 85/100: Training Loss: 0.0005108119831198738
Epoch 86/100: Training Loss: 5.869528367405846e-05
Epoch 87/100: Training Loss: 0.00011146603418248041
Epoch 88/100: Training Loss: 7.403593599086716e-05
Epoch 89/100: Training Loss: 0.00012768526517209554
Epoch 90/100: Training Loss: 4.178128604378019e-05
Epoch 91/100: Training Loss: 0.0001827342879204523
Epoch 92/100: Training Loss: 4.931391172465824e-05
Epoch 93/100: Training Loss: 4.403670983655112e-05
Epoch 94/100: Training Loss: 7.707961790618443e-05
Epoch 95/100: Training Loss: 4.373656231023017e-05
Epoch 96/100: Training Loss: 1.1267407708579585e-05
Epoch 97/100: Training Loss: 0.0007197290658950805
Epoch 98/100: Training Loss: 7.168613374233245e-05
Epoch 99/100: Training Loss: 2.3064372085389638e-05
Epoch 0/100: Training Loss: 0.0003415931471817785
Epoch 1/100: Training Loss: 0.00011387744324384547
Epoch 2/100: Training Loss: 0.00020592899130974458
Epoch 3/100: Training Loss: 8.744756273514362e-05
Epoch 4/100: Training Loss: 5.4650500023679485e-05
Epoch 5/100: Training Loss: 2.771454508141648e-05
Epoch 6/100: Training Loss: 6.641114769365005e-05
Epoch 7/100: Training Loss: 1.3119287619130502e-05
Epoch 8/100: Training Loss: 2.948582234928648e-05
Epoch 9/100: Training Loss: 1.508345251246322e-05
Epoch 10/100: Training Loss: 8.253554302118093e-06
Epoch 11/100: Training Loss: 8.324560948270116e-06
Epoch 12/100: Training Loss: 6.172997537526867e-06
Epoch 13/100: Training Loss: 3.79345998418131e-05
Epoch 14/100: Training Loss: 1.1271457645675832e-05
Epoch 15/100: Training Loss: 4.00573787965077e-06
Epoch 16/100: Training Loss: 9.359523868389267e-06
Epoch 17/100: Training Loss: 1.1528328751220548e-06
Epoch 18/100: Training Loss: 9.400853820198732e-06
Epoch 19/100: Training Loss: 1.3176418787283864e-05
Epoch 20/100: Training Loss: 1.5102033179512413e-05
Epoch 21/100: Training Loss: 1.4797239441522877e-05
Epoch 22/100: Training Loss: 2.1942858796396988e-06
Epoch 23/100: Training Loss: 2.2158806426651615e-06
Epoch 24/100: Training Loss: 2.162126105108993e-05
Epoch 25/100: Training Loss: 3.867455740287292e-06
Epoch 26/100: Training Loss: 6.795061274255208e-06
Epoch 27/100: Training Loss: 2.4302210435449934e-05
Epoch 28/100: Training Loss: 3.748271297172105e-06
Epoch 29/100: Training Loss: 3.236142261851606e-05
Epoch 30/100: Training Loss: 9.219507224482598e-07
Epoch 31/100: Training Loss: 7.195026910812449e-06
Epoch 32/100: Training Loss: 1.0183968558087856e-07
Epoch 33/100: Training Loss: 7.295319687673371e-07
Epoch 34/100: Training Loss: 8.37583617447949e-06
Epoch 35/100: Training Loss: 1.4275550279471514e-05
Epoch 36/100: Training Loss: 1.352240400705978e-05
Epoch 37/100: Training Loss: 1.0373795849474356e-06
Epoch 38/100: Training Loss: 4.021349816406659e-06
Epoch 39/100: Training Loss: 4.0356774157810985e-07
Epoch 40/100: Training Loss: 1.855883792006998e-05
Epoch 41/100: Training Loss: 1.59580079610828e-05
Epoch 42/100: Training Loss: 3.107837948510401e-06
Epoch 43/100: Training Loss: 2.283335086047578e-06
Epoch 44/100: Training Loss: 1.0298655621877176e-06
Epoch 45/100: Training Loss: 9.305450766027974e-06
Epoch 46/100: Training Loss: 1.8096207618731246e-06
Epoch 47/100: Training Loss: 3.588399559491687e-06
Epoch 48/100: Training Loss: 6.777370053765585e-06
Epoch 49/100: Training Loss: 6.35103257797319e-06
Epoch 50/100: Training Loss: 4.171484933911468e-05
Epoch 51/100: Training Loss: 2.9372462301517275e-06
Epoch 52/100: Training Loss: 1.2779569284592886e-05
Epoch 53/100: Training Loss: 6.047340458352789e-06
Epoch 54/100: Training Loss: 1.2952274904191064e-05
Epoch 55/100: Training Loss: 1.920392173978922e-06
Epoch 56/100: Training Loss: 2.501923278224268e-06
Epoch 57/100: Training Loss: 6.862681546657205e-06
Epoch 58/100: Training Loss: 5.762968815380721e-06
Epoch 59/100: Training Loss: 2.8807302904527464e-07
Epoch 60/100: Training Loss: 4.773917641463897e-06
Epoch 61/100: Training Loss: 5.997133349700512e-06
Epoch 62/100: Training Loss: 7.075102937271555e-06
Epoch 63/100: Training Loss: 1.698959399934867e-05
Epoch 64/100: Training Loss: 2.0183782803444125e-06
Epoch 65/100: Training Loss: 7.843013017547788e-06
Epoch 66/100: Training Loss: 4.4375072329414545e-06
Epoch 67/100: Training Loss: 9.937066700961664e-06
Epoch 68/100: Training Loss: 5.719611506096179e-06
Epoch 69/100: Training Loss: 3.8295772794312497e-07
Epoch 70/100: Training Loss: 3.387161019108564e-07
Epoch 71/100: Training Loss: 3.481881494234077e-06
Epoch 72/100: Training Loss: 0.00039342710440107384
Epoch 73/100: Training Loss: 2.3217412544865544e-06
Epoch 74/100: Training Loss: 0.00037072641815212993
Epoch 75/100: Training Loss: 2.964082893153889e-06
Epoch 76/100: Training Loss: 0.0002994518057047892
Epoch 77/100: Training Loss: 1.4210981931046045e-05
Epoch 78/100: Training Loss: 4.766860695408402e-06
Epoch 79/100: Training Loss: 6.519388675475292e-06
Epoch 80/100: Training Loss: 2.135358956291807e-05
Epoch 81/100: Training Loss: 1.8104003536865579e-06
Epoch 82/100: Training Loss: 9.717417510150434e-06
Epoch 83/100: Training Loss: 1.139296509093232e-05
Epoch 84/100: Training Loss: 1.1494225762087665e-05
Epoch 85/100: Training Loss: 4.213651736005605e-06
Epoch 86/100: Training Loss: 4.745034023011617e-05
Epoch 87/100: Training Loss: 0.0003360747147521241
Epoch 88/100: Training Loss: 9.317967227751689e-06
Epoch 89/100: Training Loss: 9.123674243224038e-06
Epoch 90/100: Training Loss: 4.190171854369503e-06
Epoch 91/100: Training Loss: 5.174237493273737e-05
Epoch 92/100: Training Loss: 0.00015070675302752487
Epoch 93/100: Training Loss: 0.00030690888992602306
Epoch 94/100: Training Loss: 7.588448498746475e-05
Epoch 95/100: Training Loss: 4.140332752137447e-05
Epoch 96/100: Training Loss: 0.00029216914011134234
Epoch 97/100: Training Loss: 0.0002945146436313931
Epoch 98/100: Training Loss: 1.0526796012259216e-05
Epoch 99/100: Training Loss: 3.841988367142437e-05
Epoch 0/100: Training Loss: 0.0007217373824925814
Epoch 1/100: Training Loss: 0.00048608883567478347
Epoch 2/100: Training Loss: 0.0007141578024712162
Epoch 3/100: Training Loss: 0.0005014751844360057
Epoch 4/100: Training Loss: 0.0005816322017982962
Epoch 5/100: Training Loss: 0.0005956947227607027
Epoch 6/100: Training Loss: 8.996600366157034e-05
Epoch 7/100: Training Loss: 0.0001632478178123345
Epoch 8/100: Training Loss: 0.0004103362272327073
Epoch 9/100: Training Loss: 0.00013614011307557425
Epoch 10/100: Training Loss: 0.00013241883601255463
Epoch 11/100: Training Loss: 0.0005067109989659222
Epoch 12/100: Training Loss: 0.00012104184443248067
Epoch 13/100: Training Loss: 7.564282035770048e-05
Epoch 14/100: Training Loss: 5.1266248315428766e-05
Epoch 15/100: Training Loss: 6.039864001210761e-05
Epoch 16/100: Training Loss: 5.4615032341745165e-05
Epoch 17/100: Training Loss: 4.14326472054933e-05
Epoch 18/100: Training Loss: 2.232795707197581e-05
Epoch 19/100: Training Loss: 1.5177864104437367e-05
Epoch 20/100: Training Loss: 1.393343492946475e-05
Epoch 21/100: Training Loss: 0.00010334676950450105
Epoch 22/100: Training Loss: 2.6144442746892645e-05
Epoch 23/100: Training Loss: 4.722645443274779e-05
Epoch 24/100: Training Loss: 0.00014028121415831616
Epoch 25/100: Training Loss: 0.00012576085141891442
Epoch 26/100: Training Loss: 8.88038480627364e-05
Epoch 27/100: Training Loss: 3.0920030954091445e-05
Epoch 28/100: Training Loss: 4.9601241514302686e-05
Epoch 29/100: Training Loss: 6.686050246879099e-05
Epoch 30/100: Training Loss: 0.00014444530586113676
Epoch 31/100: Training Loss: 3.768633466195945e-05
Epoch 32/100: Training Loss: 1.815811321499267e-05
Epoch 33/100: Training Loss: 1.7039385592735907e-05
Epoch 34/100: Training Loss: 8.250771153376298e-05
Epoch 35/100: Training Loss: 2.4735671130643375e-05
Epoch 36/100: Training Loss: 5.049345753907005e-05
Epoch 37/100: Training Loss: 3.03444376098361e-05
Epoch 38/100: Training Loss: 8.721869204931213e-05
Epoch 39/100: Training Loss: 0.0003709876234980597
Epoch 40/100: Training Loss: 3.610404010340212e-05
Epoch 41/100: Training Loss: 2.4299564712865343e-05
Epoch 42/100: Training Loss: 7.753425214781566e-06
Epoch 43/100: Training Loss: 5.4335500595074346e-05
Epoch 44/100: Training Loss: 1.325000050491181e-05
Epoch 45/100: Training Loss: 7.504975796645678e-06
Epoch 46/100: Training Loss: 1.1940841036646262e-05
Epoch 47/100: Training Loss: 1.6359792311410396e-05
Epoch 48/100: Training Loss: 1.3567661362641675e-05
Epoch 49/100: Training Loss: 1.328958621347584e-05
Epoch 50/100: Training Loss: 2.1047003855163923e-05
Epoch 51/100: Training Loss: 4.63789688864192e-05
Epoch 52/100: Training Loss: 3.810890990754832e-05
Epoch 53/100: Training Loss: 1.833656609310332e-05
Epoch 54/100: Training Loss: 4.0447892810123555e-05
Epoch 55/100: Training Loss: 1.208429845670859e-05
Epoch 56/100: Training Loss: 2.0334080923438647e-05
Epoch 57/100: Training Loss: 6.646297507635925e-06
Epoch 58/100: Training Loss: 1.3159031210386235e-05
Epoch 59/100: Training Loss: 1.1946634807880374e-05
Epoch 60/100: Training Loss: 4.475550267143526e-05
Epoch 61/100: Training Loss: 5.707486672101965e-05
Epoch 62/100: Training Loss: 0.0002439699275194159
Epoch 63/100: Training Loss: 8.086749276029315e-06
Epoch 64/100: Training Loss: 1.9940624130520844e-05
Epoch 65/100: Training Loss: 5.5725870716974934e-05
Epoch 66/100: Training Loss: 1.3772421853913777e-05
Epoch 67/100: Training Loss: 6.215284119589605e-06
Epoch 68/100: Training Loss: 1.6954578999159992e-05
Epoch 69/100: Training Loss: 2.047122379647937e-05
Epoch 70/100: Training Loss: 3.6178887844229666e-05
Epoch 71/100: Training Loss: 8.496411981574003e-06
Epoch 72/100: Training Loss: 2.838497765470242e-05
Epoch 73/100: Training Loss: 8.428030633840008e-06
Epoch 74/100: Training Loss: 1.764015051657739e-05
Epoch 75/100: Training Loss: 1.769808035541848e-05
Epoch 76/100: Training Loss: 3.1807189942270085e-05
Epoch 77/100: Training Loss: 1.0922425179089901e-05
Epoch 78/100: Training Loss: 1.2512089333672455e-05
Epoch 79/100: Training Loss: 6.254159525972634e-05
Epoch 80/100: Training Loss: 3.2925206699952985e-05
Epoch 81/100: Training Loss: 2.7430099817577767e-05
Epoch 82/100: Training Loss: 6.319120850252068e-05
Epoch 83/100: Training Loss: 2.4960099624982778e-05
Epoch 84/100: Training Loss: 3.659105432278292e-05
Epoch 85/100: Training Loss: 2.7797372161370255e-06
Epoch 86/100: Training Loss: 5.111687677206049e-05
Epoch 87/100: Training Loss: 5.385334544999588e-05
Epoch 88/100: Training Loss: 2.407359526209209e-05
Epoch 89/100: Training Loss: 2.2351073668054912e-05
Epoch 90/100: Training Loss: 1.400108404169624e-05
Epoch 91/100: Training Loss: 1.3160473185676883e-05
Epoch 92/100: Training Loss: 5.552628424000625e-06
Epoch 93/100: Training Loss: 2.5247417138394526e-05
Epoch 94/100: Training Loss: 0.00011931115014541552
Epoch 95/100: Training Loss: 2.3297639338722552e-05
Epoch 96/100: Training Loss: 2.4301753545872832e-05
Epoch 97/100: Training Loss: 4.938743324671391e-05
Epoch 98/100: Training Loss: 4.8278480012347735e-05
Epoch 99/100: Training Loss: 2.644877826821977e-05
Epoch 0/100: Training Loss: 0.00019601666752029867
Epoch 1/100: Training Loss: 0.0001325244266612857
Epoch 2/100: Training Loss: 0.00035505869663229173
Epoch 3/100: Training Loss: 0.0005179136699321223
Epoch 4/100: Training Loss: 5.9257339065273605e-05
Epoch 5/100: Training Loss: 0.0002961354512794345
Epoch 6/100: Training Loss: 0.00037102342820635026
Epoch 7/100: Training Loss: 0.00025218904164491917
Epoch 8/100: Training Loss: 0.00016318730936915266
Epoch 9/100: Training Loss: 0.0003942187656374539
Epoch 10/100: Training Loss: 0.0001585059160110997
Epoch 11/100: Training Loss: 0.0001446556884284113
Epoch 12/100: Training Loss: 0.0001980602449061824
Epoch 13/100: Training Loss: 9.459935302170469e-06
Epoch 14/100: Training Loss: 3.7296654601745744e-05
Epoch 15/100: Training Loss: 0.00033088897665341693
Epoch 16/100: Training Loss: 6.0063088312745094e-05
Epoch 17/100: Training Loss: 0.00016381043721647824
Epoch 18/100: Training Loss: 8.148570344144223e-05
Epoch 19/100: Training Loss: 8.321639296470904e-05
Epoch 20/100: Training Loss: 4.7957643355224646e-05
Epoch 21/100: Training Loss: 4.6170248156961275e-05
Epoch 22/100: Training Loss: 0.0002016198145700436
Epoch 23/100: Training Loss: 9.625974823446835e-05
Epoch 24/100: Training Loss: 2.6145055159634237e-06
Epoch 25/100: Training Loss: 0.00024796315633198796
Epoch 26/100: Training Loss: 5.396354176542338e-05
Epoch 27/100: Training Loss: 0.00019410515532774085
Epoch 28/100: Training Loss: 0.00017434661733169182
Epoch 29/100: Training Loss: 0.0002354723492673799
Epoch 30/100: Training Loss: 3.3806078135967255e-05
Epoch 31/100: Training Loss: 2.2061092963935258e-06
Epoch 32/100: Training Loss: 7.657633806743166e-06
Epoch 33/100: Training Loss: 1.7031643758801854e-05
Epoch 34/100: Training Loss: 0.0001248175497440731
Epoch 35/100: Training Loss: 0.00013899649767314687
Epoch 36/100: Training Loss: 0.0001700530924341258
Epoch 37/100: Training Loss: 0.0001283387708313325
Epoch 38/100: Training Loss: 0.0002762031942313793
Epoch 39/100: Training Loss: 0.0004941848124943528
Epoch 40/100: Training Loss: 0.00016408738698445115
Epoch 41/100: Training Loss: 0.0001586481709690655
Epoch 42/100: Training Loss: 2.491604719384044e-05
Epoch 43/100: Training Loss: 0.0001736419165835661
Epoch 44/100: Training Loss: 2.38385092576637e-05
Epoch 45/100: Training Loss: 0.0005978738852575713
Epoch 46/100: Training Loss: 3.406508910177531e-06
Epoch 47/100: Training Loss: 4.549140701819138e-06
Epoch 48/100: Training Loss: 0.00020205574657987147
Epoch 49/100: Training Loss: 9.242377663944281e-05
Epoch 50/100: Training Loss: 2.9302870982563963e-05
Epoch 51/100: Training Loss: 0.0001935931287851988
Epoch 52/100: Training Loss: 0.0001537531577781135
Epoch 53/100: Training Loss: 0.00013787564182398367
Epoch 54/100: Training Loss: 4.584554984581237e-05
Epoch 55/100: Training Loss: 1.0324637496880456e-05
Epoch 56/100: Training Loss: 8.05943546926274e-05
Epoch 57/100: Training Loss: 0.0001485351400048125
Epoch 58/100: Training Loss: 0.0001681409410986246
Epoch 59/100: Training Loss: 0.00036984915826834884
Epoch 60/100: Training Loss: 0.000223055701045429
Epoch 61/100: Training Loss: 0.0002141716871775833
Epoch 62/100: Training Loss: 0.0005671938552575953
Epoch 63/100: Training Loss: 0.00017459325346292235
Epoch 64/100: Training Loss: 0.0001833959318259183
Epoch 65/100: Training Loss: 0.0001415192496542837
Epoch 66/100: Training Loss: 3.579002348523514e-05
Epoch 67/100: Training Loss: 3.149442365575655e-05
Epoch 68/100: Training Loss: 3.3659623095802233e-05
Epoch 69/100: Training Loss: 5.478180451866458e-05
Epoch 70/100: Training Loss: 0.00017486693447127063
Epoch 71/100: Training Loss: 0.00021892899245608087
Epoch 72/100: Training Loss: 1.913133332980614e-05
Epoch 73/100: Training Loss: 9.494028442228833e-06
Epoch 74/100: Training Loss: 1.356574962390404e-05
Epoch 75/100: Training Loss: 3.4598172550984456e-05
Epoch 76/100: Training Loss: 3.19301747653049e-06
Epoch 77/100: Training Loss: 8.688898136218388e-05
Epoch 78/100: Training Loss: 1.3507672074232615e-05
Epoch 79/100: Training Loss: 7.657543299537079e-05
Epoch 80/100: Training Loss: 4.133005973462965e-05
Epoch 81/100: Training Loss: 1.506007495610153e-05
Epoch 82/100: Training Loss: 2.5791303236402718e-05
Epoch 83/100: Training Loss: 3.578245192400965e-05
Epoch 84/100: Training Loss: 8.359190770516208e-07
Epoch 85/100: Training Loss: 3.763652285652272e-06
Epoch 86/100: Training Loss: 5.485039368710097e-05
Epoch 87/100: Training Loss: 0.0003829493972600675
Epoch 88/100: Training Loss: 0.00015802893276308097
Epoch 89/100: Training Loss: 0.00034420294504539637
Epoch 90/100: Training Loss: 3.3456882388860573e-05
Epoch 91/100: Training Loss: 8.669619758923848e-05
Epoch 92/100: Training Loss: 0.00016410022462699927
Epoch 93/100: Training Loss: 0.00011818645996790306
Epoch 94/100: Training Loss: 3.106217356581314e-05
Epoch 95/100: Training Loss: 0.0004973662104092392
Epoch 96/100: Training Loss: 2.4797123378626637e-07
Epoch 97/100: Training Loss: 2.495126397001977e-05
Epoch 98/100: Training Loss: 3.654777709230342e-06
Epoch 99/100: Training Loss: 9.358116407312599e-05
Epoch 0/100: Training Loss: 0.0001646892736890556
Epoch 1/100: Training Loss: 1.819836917994173e-05
Epoch 2/100: Training Loss: 7.652927547763048e-05
Epoch 3/100: Training Loss: 9.157355063413848e-05
Epoch 4/100: Training Loss: 6.253123545116228e-05
Epoch 5/100: Training Loss: 1.0716336271103987e-05
Epoch 6/100: Training Loss: 0.00010285701000718378
Epoch 7/100: Training Loss: 4.223636427863699e-05
Epoch 8/100: Training Loss: 4.854176271994723e-05
Epoch 9/100: Training Loss: 1.3916055285818961e-05
Epoch 10/100: Training Loss: 1.4291646556636488e-05
Epoch 11/100: Training Loss: 3.807812021822784e-05
Epoch 12/100: Training Loss: 3.879423966452445e-05
Epoch 13/100: Training Loss: 2.1162357800738316e-05
Epoch 14/100: Training Loss: 2.8619634323432798e-05
Epoch 15/100: Training Loss: 3.62131015050816e-06
Epoch 16/100: Training Loss: 1.0549805107454505e-05
Epoch 17/100: Training Loss: 3.3669965450741365e-05
Epoch 18/100: Training Loss: 2.9411566044603076e-05
Epoch 19/100: Training Loss: 3.469582926263854e-05
Epoch 20/100: Training Loss: 1.19763262901429e-05
Epoch 21/100: Training Loss: 2.0961440768119043e-05
